{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f888e8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from skopt import BayesSearchCV\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18db1b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'C:\\Users\\abact\\BC-Project\\data\\train_data.csv')\n",
    "valid = pd.read_csv(r'C:\\Users\\abact\\BC-Project\\data\\val_data.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\abact\\BC-Project\\data\\test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a42569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert variables to numeric in the train dataset\n",
    "variables_to_convert = train.columns.drop('Date')\n",
    "train[variables_to_convert] = train[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert variables to numeric in the valid dataset\n",
    "valid[variables_to_convert] = valid[variables_to_convert].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4414ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Direction'] = np.select([\n",
    "    train['Increase'] > 0,\n",
    "    train['Decrease'] > 0,\n",
    "    train['Difference'] == 0\n",
    "], [\n",
    "    1,  # Increase\n",
    "    -1,  # Decrease\n",
    "    0   # Hold\n",
    "], default=-1)\n",
    "\n",
    "valid['Direction'] = np.select([\n",
    "    valid['Increase'] > 0,\n",
    "    valid['Decrease'] > 0,\n",
    "    valid['Difference'] == 0\n",
    "], [\n",
    "    1,  # Increase\n",
    "    -1,  # Decrease\n",
    "    0   # Hold\n",
    "], default=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4693a22",
   "metadata": {},
   "source": [
    "###Multinomial Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00ab60a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9319371727748691\n"
     ]
    }
   ],
   "source": [
    "# Select the independent variables (excluding specified variables)\n",
    "exclude_vars = ['Increase', 'Decrease', 'Date', 'Difference']\n",
    "X_train = train.drop(['Direction'] + exclude_vars, axis=1)\n",
    "\n",
    "# Separate the target variable\n",
    "y_train = train['Direction']\n",
    "\n",
    "# Scale the independent variables\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit a multinomial logistic regression model\n",
    "logit_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "logit_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "accuracy = logit_model.score(X_train_scaled, y_train)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddba3ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "Cross-entropy loss: 1.1448712063854016\n",
      "AICc: -118.50311368114717\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the validation set\n",
    "X_valid = valid.drop(['Direction'] + exclude_vars, axis=1)\n",
    "y_valid = valid['Direction']\n",
    "\n",
    "# Scale the independent variables of the validation set using the same scaler object\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Predict class labels for the validation set\n",
    "y_valid_pred = logit_model.predict(X_valid_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = logit_model.score(X_valid_scaled, y_valid)\n",
    "\n",
    "# Calculate cross-entropy loss (log loss)\n",
    "loss = log_loss(y_valid, logit_model.predict_proba(X_valid_scaled))\n",
    "\n",
    "# Calculate AICc\n",
    "nll = -log_loss(y_valid, logit_model.predict_proba(X_valid_scaled), normalize=False)\n",
    "n = len(X_valid)\n",
    "p = logit_model.coef_.shape[1]\n",
    "aic = 2 * p + 2 * nll + (2 * p * (p + 1)) / (n - p - 1)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Cross-entropy loss:\", loss)\n",
    "print(\"AICc:\", aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f11795",
   "metadata": {},
   "source": [
    "###Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1de1a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9528795811518325\n",
      "Best Hyperparameters: OrderedDict([('max_depth', 5), ('n_estimators', 100)])\n",
      "Best Score: 0.5456140350877192\n"
     ]
    }
   ],
   "source": [
    "param_space = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    # Add other hyperparameters and their values here\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform Bayesian Search for hyperparameter tuning\n",
    "opt_model = BayesSearchCV(rf_model, param_space, n_iter=10, cv=5, random_state=42)\n",
    "opt_model.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model performance\n",
    "best_params = opt_model.best_params_\n",
    "best_score = opt_model.best_score_\n",
    "\n",
    "# Fit the Random Forest model with the best hyperparameters\n",
    "rf_model_best = RandomForestClassifier(**best_params)\n",
    "rf_model_best.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "accuracy = rf_model_best.score(X_train, y_train)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f66f07f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2916666666666667\n",
      "Cross-entropy loss: 1.0906148069620647\n",
      "AICc: -115.89880650882697\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the validation set\n",
    "X_valid = valid.drop(['Direction'] + exclude_vars, axis=1)\n",
    "y_valid = valid['Direction']\n",
    "\n",
    "# Predict class labels for the validation set using the best model\n",
    "y_valid_pred = rf_model_best.predict(X_valid)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = rf_model_best.score(X_valid, y_valid)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate cross-entropy loss (log loss)\n",
    "loss = log_loss(y_valid, rf_model_best.predict_proba(X_valid))\n",
    "print(\"Cross-entropy loss:\", loss)\n",
    "\n",
    "# Calculate AICc\n",
    "nll = -log_loss(y_valid, rf_model_best.predict_proba(X_valid), normalize=False)\n",
    "n = len(X_valid)\n",
    "p = X_valid.shape[1]  # Number of features in X_valid\n",
    "aic = 2 * p + 2 * nll + (2 * p * (p + 1)) / (n - p - 1)\n",
    "print(\"AICc:\", aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05048c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Select the independent variables (excluding specified variables)\n",
    "exclude_vars = ['Increase', 'Decrease', 'Date', 'Difference']\n",
    "X_train = train.drop(['Direction'] + exclude_vars, axis=1)\n",
    "\n",
    "# Separate the target variable\n",
    "y_train = train['Direction']\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the Random Forest model to the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "accuracy = rf_model.score(X_train, y_train)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ac12957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.625\n",
      "Cross-entropy loss: 0.9983295308113065\n",
      "AICc: -111.46911325359059\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the validation set\n",
    "X_valid = valid.drop(['Direction'] + exclude_vars, axis=1)\n",
    "y_valid = valid['Direction']\n",
    "\n",
    "# Predict class labels for the validation set\n",
    "y_valid_pred = rf_model.predict(X_valid)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = rf_model.score(X_valid, y_valid)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate cross-entropy loss (log loss)\n",
    "loss = log_loss(y_valid, rf_model.predict_proba(X_valid))\n",
    "print(\"Cross-entropy loss:\", loss)\n",
    "\n",
    "# Calculate AICc\n",
    "nll = -log_loss(y_valid, rf_model.predict_proba(X_valid), normalize=False)\n",
    "n = len(X_valid)\n",
    "p = X_valid.shape[1]  # Number of features in X_valid\n",
    "aic = 2 * p + 2 * nll + (2 * p * (p + 1)) / (n - p - 1)\n",
    "print(\"AICc:\", aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fd90ad",
   "metadata": {},
   "source": [
    "###Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3808703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9424083769633508\n"
     ]
    }
   ],
   "source": [
    "# Select the independent variables (excluding specified variables)\n",
    "exclude_vars = ['Increase', 'Decrease', 'Date', 'Difference']\n",
    "X_train = train.drop(['Direction'] + exclude_vars, axis=1)\n",
    "\n",
    "# Separate the target variable\n",
    "y_train = train['Direction']\n",
    "\n",
    "# Scale the independent variables\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit a SVM classifier\n",
    "svm_model = SVC(kernel='linear', probability=True)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "accuracy = svm_model.score(X_train_scaled, y_train)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "924c149d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "Cross-entropy loss: 1.0141586731506265\n",
      "AICc: -112.22891208587794\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the validation set\n",
    "X_valid = valid.drop(['Direction'] + exclude_vars, axis=1)\n",
    "y_valid = valid['Direction']\n",
    "\n",
    "# Scale the independent variables of the validation set using the same scaler object\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Predict class labels for the validation set\n",
    "y_valid_pred = svm_model.predict(X_valid_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = svm_model.score(X_valid_scaled, y_valid)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate cross-entropy loss (log loss)\n",
    "loss = log_loss(y_valid, svm_model.predict_proba(X_valid_scaled))\n",
    "print(\"Cross-entropy loss:\", loss)\n",
    "\n",
    "# Calculate AICc\n",
    "nll = -log_loss(y_valid, svm_model.predict_proba(X_valid_scaled), normalize=False)\n",
    "n = len(X_valid)\n",
    "p = X_valid.shape[1]  # Number of features in X_valid\n",
    "aic = 2 * p + 2 * nll + (2 * p * (p + 1)) / (n - p - 1)\n",
    "print(\"AICc:\", aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2890ae80",
   "metadata": {},
   "source": [
    "###Magnitude Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec769328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logit_increase subset\n",
    "train_logit_increase = train[train['Increase'] > 0].copy()\n",
    "train_logit_increase = train_logit_increase.drop(['Decrease', 'Date', 'Difference', 'Direction'], axis=1)\n",
    "\n",
    "# Create logit_decrease subset\n",
    "train_logit_decrease = train[train['Decrease'] > 0].copy()\n",
    "train_logit_decrease = train_logit_decrease.drop(['Increase', 'Date', 'Difference', 'Direction'], axis=1)\n",
    "\n",
    "# Convert 'Increase' and 'Decrease' variables to categorical\n",
    "train_logit_increase['Increase'] = train_logit_increase['Increase'].astype('category')\n",
    "train_logit_decrease['Decrease'] = train_logit_decrease['Decrease'].astype('category')\n",
    "\n",
    "# Create logit_increase subset\n",
    "valid_logit_increase = valid[valid['Increase'] > 0].copy()\n",
    "valid_logit_increase = valid_logit_increase.drop(['Decrease', 'Date', 'Difference', 'Direction'], axis=1)\n",
    "\n",
    "# Create logit_decrease subset\n",
    "valid_logit_decrease = valid[valid['Decrease'] > 0].copy()\n",
    "valid_logit_decrease = valid_logit_decrease.drop(['Increase', 'Date', 'Difference', 'Direction'], axis=1)\n",
    "\n",
    "# Convert 'Increase' and 'Decrease' variables to categorical\n",
    "valid_logit_increase['Increase'] = valid_logit_increase['Increase'].astype('category')\n",
    "valid_logit_decrease['Decrease'] = valid_logit_decrease['Decrease'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96c008a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25, 0.50, 0.75]\n",
      "Categories (3, float64): [0.25, 0.50, 0.75]\n",
      "[0.50, 0.25, 0.75]\n",
      "Categories (3, float64): [0.25, 0.50, 0.75]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abact\\AppData\\Local\\Temp\\ipykernel_24244\\3351578375.py:2: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  print(unique_values)\n",
      "C:\\Users\\abact\\AppData\\Local\\Temp\\ipykernel_24244\\3351578375.py:4: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  print(unique_values)\n"
     ]
    }
   ],
   "source": [
    "unique_values = train_logit_increase['Increase'].unique()\n",
    "print(unique_values)\n",
    "unique_values = train_logit_decrease['Decrease'].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197badd1",
   "metadata": {},
   "source": [
    "###Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b90038d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Select the independent variables (excluding specified variables)\n",
    "X_train = train_logit_increase.drop(['Increase'], axis=1)\n",
    "\n",
    "# Encode the target variable as ordinal\n",
    "ordinal_encoder = OrdinalEncoder(categories=[[0.25, 0.50, 0.75, 1.00]], handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "y_train_encoded = ordinal_encoder.fit_transform(train_logit_increase[['Increase']])\n",
    "\n",
    "# Reshape the target variable to a 1d array\n",
    "y_train_encoded = y_train_encoded.ravel()\n",
    "\n",
    "# Scale the independent variables\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit a multinomial logistic regression model\n",
    "logit_increase = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "logit_increase.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "accuracy = logit_increase.score(X_train_scaled, y_train_encoded)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a859ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Encode the target variable of the validation set as ordinal using the fitted ordinal_encoder from train_logit_increase\n",
    "ordinal_encoder = OrdinalEncoder(categories=[[0.25, 0.50, 0.75, 1.00]], handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "y_valid_encoded = ordinal_encoder.fit_transform(valid_logit_increase[['Increase']])\n",
    "\n",
    "# Scale the independent variables of the validation set using the same scaler object\n",
    "X_valid_scaled = scaler.transform(valid_logit_increase[X_train.columns])\n",
    "\n",
    "# Predict class labels for the validation set using the trained logit_increase model\n",
    "y_valid_pred = logit_increase.predict(X_valid_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_valid = accuracy_score(y_valid_encoded, y_valid_pred)\n",
    "print(\"Validation Accuracy:\", accuracy_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dd098da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "# Select the independent variables (excluding specified variables)\n",
    "X_train = train_logit_decrease.drop(['Decrease'], axis=1)\n",
    "\n",
    "# Encode the target variable as ordinal\n",
    "ordinal_encoder = OrdinalEncoder(categories=[[0.25, 0.50, 0.75, 1.00]], handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "y_train_encoded = ordinal_encoder.fit_transform(train_logit_decrease[['Decrease']])\n",
    "\n",
    "# Reshape the target variable to a 1d array\n",
    "y_train_encoded = y_train_encoded.ravel()\n",
    "\n",
    "# Scale the independent variables\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit a multinomial logistic regression model\n",
    "logit_decrease = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "logit_decrease.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "accuracy = logit_decrease.score(X_train_scaled, y_train_encoded)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d6f0a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Encode the target variable of the validation set as ordinal using the fitted ordinal_encoder from train_logit_decrease\n",
    "ordinal_encoder = OrdinalEncoder(categories=[[0.25, 0.50, 0.75, 1.00]], handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "y_valid_encoded = ordinal_encoder.fit_transform(valid_logit_decrease[['Decrease']])\n",
    "\n",
    "# Scale the independent variables of the validation set using the same scaler object\n",
    "X_valid_scaled = scaler.transform(valid_logit_decrease[X_train.columns])\n",
    "\n",
    "# Predict class labels for the validation set using the trained logit_decrease model\n",
    "y_valid_pred = logit_decrease.predict(X_valid_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_valid = accuracy_score(y_valid_encoded, y_valid_pred)\n",
    "print(\"Validation Accuracy:\", accuracy_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a466b548",
   "metadata": {},
   "source": [
    "###Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71fbf637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Select the independent variables (excluding specified variables)\n",
    "X_train = train_logit_increase.drop(['Increase'], axis=1)\n",
    "\n",
    "# Encode the target variable as ordinal\n",
    "ordinal_encoder = OrdinalEncoder(categories=[[0.25, 0.50, 0.75, 1.00]], handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "y_train_encoded = ordinal_encoder.fit_transform(train_logit_increase[['Increase']])\n",
    "\n",
    "# Reshape the target variable to a 1d array\n",
    "y_train_encoded = y_train_encoded.ravel()\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Scale the independent variables\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit the Random Forest model\n",
    "rf_model.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "accuracy = rf_model.score(X_train_scaled, y_train_encoded)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd17f8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the validation set\n",
    "X_valid = valid_logit_increase.drop(['Increase'], axis=1)\n",
    "y_valid_encoded = ordinal_encoder.transform(valid_logit_increase[['Increase']])\n",
    "y_valid_encoded = y_valid_encoded.ravel()\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Predict class labels for the validation set\n",
    "y_valid_pred = rf_model.predict(X_valid_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_valid = accuracy_score(y_valid_encoded, y_valid_pred)\n",
    "print(\"Validation Accuracy:\", accuracy_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da9538e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Select the independent variables (excluding specified variables)\n",
    "X_train = train_logit_decrease.drop(['Decrease'], axis=1)\n",
    "\n",
    "# Encode the target variable as ordinal\n",
    "ordinal_encoder = OrdinalEncoder(categories=[[0.25, 0.50, 0.75, 1.00]], handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "y_train_encoded = ordinal_encoder.fit_transform(train_logit_decrease[['Decrease']])\n",
    "\n",
    "# Reshape the target variable to a 1d array\n",
    "y_train_encoded = y_train_encoded.ravel()\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Scale the independent variables\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit the Random Forest model\n",
    "rf_model.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "accuracy = rf_model.score(X_train_scaled, y_train_encoded)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e24dd4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the validation set\n",
    "X_valid = valid_logit_decrease.drop(['Decrease'], axis=1)\n",
    "y_valid_encoded = ordinal_encoder.transform(valid_logit_decrease[['Decrease']])\n",
    "y_valid_encoded = y_valid_encoded.ravel()\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Predict class labels for the validation set\n",
    "y_valid_pred = rf_model.predict(X_valid_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_valid = accuracy_score(y_valid_encoded, y_valid_pred)\n",
    "print(\"Validation Accuracy:\", accuracy_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
