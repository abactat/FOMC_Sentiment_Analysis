{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "87439644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import random\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tabulate import tabulate\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "02d6f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and loaded into 'dataset_adjusted' successfully.\n",
      "Dataset downloaded and loaded into 'words' successfully.\n"
     ]
    }
   ],
   "source": [
    "# The URL of the raw dataset on GitHub\n",
    "url = \"https://raw.githubusercontent.com/abactat/BC-Project/main/data/dataset_adjusted.csv?token=GHSAT0AAAAAACC4ZCNKN5F6XR7HZA75QWTEZGQVYIA\"\n",
    "\n",
    "# Send an HTTP GET request to fetch the content of the raw dataset\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200 means success)\n",
    "if response.status_code == 200:\n",
    "    # Read the content as a pandas DataFrame and assign it to the 'train' variable\n",
    "    dataset_adjusted = pd.read_csv(StringIO(response.text))\n",
    "    print(\"Dataset downloaded and loaded into 'dataset_adjusted' successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download the dataset. Status code: {response.status_code}\")\n",
    "    \n",
    "# The URL of the raw dataset on GitHub\n",
    "url = \"https://raw.githubusercontent.com/abactat/BC-Project/main/data/raw/dataset_words.csv?token=GHSAT0AAAAAACC4ZCNKDB2KTQ7KVOHGVHMAZGQVX7A\"\n",
    "\n",
    "# Send an HTTP GET request to fetch the content of the raw dataset\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200 means success)\n",
    "if response.status_code == 200:\n",
    "    # Read the content as a pandas DataFrame and assign it to the 'valid' variable\n",
    "    words = pd.read_csv(StringIO(response.text))\n",
    "    print(\"Dataset downloaded and loaded into 'words' successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download the dataset. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "41539af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentiment word list from the CSV file into a dictionary\n",
    "sentiment_word_list = {}\n",
    "with open(r\"C:\\Users\\abact\\BC-Project\\data\\external\\Loughran-McDonald_MasterDictionary_1993-2021.csv\", 'r') as file:\n",
    "    # Skip the header line\n",
    "    next(file)\n",
    "\n",
    "    for line in file:\n",
    "        values = line.strip().split(',')\n",
    "\n",
    "        # Extract the necessary values\n",
    "        word = values[0].lower()\n",
    "        positive = float(values[8])  # Positive column index is 8\n",
    "        negative = float(values[7])  # Negative column index is 7\n",
    "\n",
    "        # Assign the word as positive or negative based on the positive or negative values\n",
    "        if positive == 2009:\n",
    "            sentiment_word_list[word] = 1\n",
    "        elif negative == 2009:\n",
    "            sentiment_word_list[word] = -1\n",
    "\n",
    "# Convert the sentiment word list keys to lowercase\n",
    "selected_words = set(sentiment_word_list.keys())\n",
    "\n",
    "# Filter the 'words' DataFrame to include only columns that are present in both 'selected_words' and 'words'\n",
    "common_columns = selected_words.intersection(words.columns)\n",
    "subset_words = words[list(common_columns)].copy()\n",
    "\n",
    "# Multiply sentiment values to the vectorized text columns in the 'subset_words' DataFrame\n",
    "for column in subset_words.columns:\n",
    "    sentiment_value = sentiment_word_list.get(column, 0)\n",
    "    if sentiment_value == 1:\n",
    "        subset_words.loc[:, column] = subset_words[column] * 1  # Multiply by 1 for positive sentiment\n",
    "    elif sentiment_value == -1:\n",
    "        subset_words.loc[:, column] = subset_words[column] * -1  # Multiply by -1 for negative sentiment\n",
    "    else:\n",
    "        subset_words.loc[:, column] = subset_words[column] * 0  # Multiply by 0 for unknown sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0293b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the vectorized DataFrame with the original dataset\n",
    "full_dataset = pd.concat([dataset_adjusted, subset_words], axis=1)\n",
    "\n",
    "full_dataset['Date'] = pd.to_datetime(full_dataset['Date'])\n",
    "\n",
    "# Calculate the time difference in days from the first date\n",
    "full_dataset['Date'] = (full_dataset['Date'] - full_dataset['Date'].min()).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "93442868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federal_Reserve_Mins    0\n",
      "Preprocessed Text       0\n",
      "Date                    0\n",
      "Difference              0\n",
      "Increase                0\n",
      "                       ..\n",
      "enhance                 0\n",
      "unanticipated           0\n",
      "burdensome              0\n",
      "impressive              0\n",
      "lose                    0\n",
      "Length: 837, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for the number of missing values in 'train' DataFrame\n",
    "missing_values_count = full_dataset.isna().sum()\n",
    "\n",
    "# Print the count of missing values for each column\n",
    "print(missing_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8ef1bdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with Missing Values:\n"
     ]
    }
   ],
   "source": [
    "# Filter columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0]\n",
    "\n",
    "# Print columns with missing values and their counts\n",
    "print(\"Columns with Missing Values:\")\n",
    "for column, count in columns_with_missing_values.items():\n",
    "    print(f\"{column}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "90d18c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 192\n",
      "Validation set size: 24\n",
      "Test set size: 24\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation, and test sets\n",
    "train, valid = train_test_split(full_dataset, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Split the combined set into validation and test sets\n",
    "valid, test = train_test_split(valid, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Verify the sizes of each set\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Validation set size:\", len(valid))\n",
    "print(\"Test set size:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "589999b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert variables to numeric in the train dataset\n",
    "variables_to_convert = train.columns.drop('Date')\n",
    "train[variables_to_convert] = train[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert variables to numeric in the valid dataset\n",
    "valid[variables_to_convert] = valid[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert variables to numeric in the test dataset\n",
    "test[variables_to_convert] = test[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_test = test.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "20128c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(y_true)\n",
    "    \n",
    "    for true_val, pred_val in zip(y_true, y_pred):\n",
    "        if true_val == pred_val:\n",
    "            correct_predictions += 1\n",
    "            \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "possible_values = [-1.00, -0.75, -0.50, -0.25, 0.00, 0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "def round_to_nearest(value, possible_values):\n",
    "    return min(possible_values, key=lambda x: abs(x - value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "cf085b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in 'train' DataFrame\n",
    "missing_values_count = train.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'train'\n",
    "train = train.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "# Check for missing values in 'valid' DataFrame\n",
    "missing_values_count = valid.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'valid'\n",
    "valid = valid.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "# Check for missing values in 'test' DataFrame\n",
    "missing_values_count = test.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'test'\n",
    "test = test.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_test = test.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "db9b4ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date             0\n",
      "Difference       0\n",
      "Increase         0\n",
      "Decrease         0\n",
      "Level            0\n",
      "                ..\n",
      "enhance          0\n",
      "unanticipated    0\n",
      "burdensome       0\n",
      "impressive       0\n",
      "lose             0\n",
      "Length: 833, dtype: int64\n",
      "Columns with Missing Values:\n"
     ]
    }
   ],
   "source": [
    "# Check for the number of missing values in 'train' DataFrame\n",
    "missing_values_count = train.isna().sum()\n",
    "\n",
    "# Print the count of missing values for each column\n",
    "print(missing_values_count)\n",
    "\n",
    "# Filter columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0]\n",
    "\n",
    "# Print columns with missing values and their counts\n",
    "print(\"Columns with Missing Values:\")\n",
    "for column, count in columns_with_missing_values.items():\n",
    "    print(f\"{column}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "55ba2a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Create GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data and find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with optimal hyperparameters\n",
    "best_rf_model_5 = grid_search.best_estimator_\n",
    "\n",
    "# Predict y_train_pred on the training set\n",
    "y_train_pred = best_rf_model_5.predict(X_train)\n",
    "\n",
    "# Predict y_valid_pred on the validation set\n",
    "y_valid_pred = best_rf_model_5.predict(X_valid)\n",
    "\n",
    "# Round the predicted values to the nearest possible value\n",
    "y_train_pred = [round_to_nearest(val, possible_values) for val in y_train_pred]\n",
    "y_valid_pred = [round_to_nearest(val, possible_values) for val in y_valid_pred]\n",
    "\n",
    "# Output the random seed\n",
    "print(\"Random seed:\", random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "bc4063c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the feature importances from the best model\n",
    "feature_importances = best_rf_model_5.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "num_features_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a4dfab75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------------+\n",
      "| Metric       |   Training Set |   Validation Set |\n",
      "+==============+================+==================+\n",
      "| MSE          |     0.00358073 |        0.0260417 |\n",
      "+--------------+----------------+------------------+\n",
      "| RMSE         |     0.0598392  |        0.161374  |\n",
      "+--------------+----------------+------------------+\n",
      "| R^2          |     0.910565   |        0.599332  |\n",
      "+--------------+----------------+------------------+\n",
      "| Adjusted R^2 |     1.02673    |        1.01142   |\n",
      "+--------------+----------------+------------------+\n",
      "| Accuracy     |     0.942708   |        0.708333  |\n",
      "+--------------+----------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]\n",
    "p_train = X_train.shape[1]\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]\n",
    "p_valid = X_valid.shape[1]\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "\n",
    "# Calculate accuracy for training and validation sets\n",
    "accuracy_train = calculate_accuracy(y_train, y_train_pred)\n",
    "accuracy_valid = calculate_accuracy(y_valid, y_valid_pred)\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = [\n",
    "    [\"MSE\", mse_train, mse_valid],\n",
    "    [\"RMSE\", rmse_train, rmse_valid],\n",
    "    [\"R^2\", r2_train, r2_valid],\n",
    "    [\"Adjusted R^2\", adj_r2_train, adj_r2_valid],\n",
    "    [\"Accuracy\", accuracy_train, accuracy_valid],\n",
    "]\n",
    "\n",
    "# Prepare the headers for the table\n",
    "headers = [\"Metric\", \"Training Set\", \"Validation Set\"]\n",
    "\n",
    "# Display the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24c8ce38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seeds: [2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# List to store random seeds used in each iteration\n",
    "random_seeds = []\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Function to calculate accuracy based on a threshold\n",
    "def calculate_accuracy(y_true, y_pred, threshold):\n",
    "    num_samples = len(y_true)\n",
    "    correct_predictions = sum(abs(y_true - y_pred) <= threshold)\n",
    "    return correct_predictions / num_samples\n",
    "\n",
    "best_accuracy = -1.0\n",
    "optimal_cv = None\n",
    "best_y_train_pred = None\n",
    "best_y_valid_pred = None\n",
    "threshold = 0.1  # Define your desired threshold here\n",
    "\n",
    "for cv in range(2, 11):  # Try cross-validation folds from 2 to 10\n",
    "    # Set the random seed for reproducibility\n",
    "    random_seed = cv  # Use cv as the random seed\n",
    "    random_seeds.append(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=cv, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model with optimal hyperparameters\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predict y_train_pred on the training set\n",
    "    y_train_pred = best_rf_model.predict(X_train)\n",
    "\n",
    "    # Predict y_valid_pred on the validation set\n",
    "    y_valid_pred = best_rf_model.predict(X_valid)\n",
    "\n",
    "    # Round the predicted values to the nearest possible value\n",
    "    y_train_pred = [round_to_nearest(val, possible_values) for val in y_train_pred]\n",
    "    y_valid_pred = [round_to_nearest(val, possible_values) for val in y_valid_pred]\n",
    "\n",
    "    # Calculate accuracy for training and validation sets after rounding\n",
    "    accuracy_train = calculate_accuracy(y_train, y_train_pred, threshold)\n",
    "    accuracy_valid = calculate_accuracy(y_valid, y_valid_pred, threshold)\n",
    "    \n",
    "    # Check if the accuracy after rounding is higher than the best accuracy so far\n",
    "    if accuracy_valid > best_accuracy:\n",
    "        best_accuracy = accuracy_valid\n",
    "        optimal_cv = cv\n",
    "        best_y_train_pred = y_train_pred\n",
    "        best_y_valid_pred = y_valid_pred\n",
    "\n",
    "# Use the optimal number of folds in GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=optimal_cv, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with optimal hyperparameters\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Get the feature importances from the best model\n",
    "feature_importances = best_rf_model.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "# Print the random seeds used in each iteration\n",
    "print(\"Random Seeds:\", random_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "86e60afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the feature importances from the best model\n",
    "feature_importances = best_rf_model.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "num_features_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fb99c236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------------+\n",
      "| Metric       |   Training Set |   Validation Set |\n",
      "+==============+================+==================+\n",
      "| MSE          |     0.00455729 |         0.03125  |\n",
      "+--------------+----------------+------------------+\n",
      "| RMSE         |     0.0675077  |         0.176777 |\n",
      "+--------------+----------------+------------------+\n",
      "| R^2          |     0.886174   |         0.519199 |\n",
      "+--------------+----------------+------------------+\n",
      "| Adjusted R^2 |     1.00323    |         1.0016   |\n",
      "+--------------+----------------+------------------+\n",
      "| Accuracy     |     0.927083   |         0.625    |\n",
      "+--------------+----------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]\n",
    "p_train = X_train.shape[1]\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]\n",
    "p_valid = X_valid.shape[1]\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "\n",
    "threshold = 0.1\n",
    "accuracy_train = calculate_accuracy(y_train, y_train_pred, threshold)\n",
    "accuracy_valid = calculate_accuracy(y_valid, y_valid_pred, threshold)\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = [\n",
    "    [\"MSE\", mse_train, mse_valid],\n",
    "    [\"RMSE\", rmse_train, rmse_valid],\n",
    "    [\"R^2\", r2_train, r2_valid],\n",
    "    [\"Adjusted R^2\", adj_r2_train, adj_r2_valid],\n",
    "    [\"Accuracy\", accuracy_train, accuracy_valid],\n",
    "]\n",
    "\n",
    "# Prepare the headers for the table\n",
    "headers = [\"Metric\", \"Training Set\", \"Validation Set\"]\n",
    "\n",
    "# Display the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2085f23",
   "metadata": {},
   "source": [
    "###Data Centric AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d7329b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>divestiture</th>\n",
       "      <th>unfounded</th>\n",
       "      <th>caution</th>\n",
       "      <th>hampering</th>\n",
       "      <th>unnecessarily</th>\n",
       "      <th>shutdown</th>\n",
       "      <th>ineffective</th>\n",
       "      <th>trouble</th>\n",
       "      <th>conspired</th>\n",
       "      <th>enable</th>\n",
       "      <th>...</th>\n",
       "      <th>disrupt</th>\n",
       "      <th>worsened</th>\n",
       "      <th>downturn</th>\n",
       "      <th>worsen</th>\n",
       "      <th>mistaken</th>\n",
       "      <th>enhance</th>\n",
       "      <th>unanticipated</th>\n",
       "      <th>burdensome</th>\n",
       "      <th>impressive</th>\n",
       "      <th>lose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.00000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.00000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.004167</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>-3.975000</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>-0.154167</td>\n",
       "      <td>-0.97500</td>\n",
       "      <td>-0.020833</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>-0.570833</td>\n",
       "      <td>-3.354167</td>\n",
       "      <td>-0.941667</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>1.90000</td>\n",
       "      <td>-2.700000</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>-0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.064550</td>\n",
       "      <td>0.064550</td>\n",
       "      <td>14.327878</td>\n",
       "      <td>0.064550</td>\n",
       "      <td>2.324416</td>\n",
       "      <td>9.95888</td>\n",
       "      <td>0.143125</td>\n",
       "      <td>0.156451</td>\n",
       "      <td>0.064550</td>\n",
       "      <td>2.726887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156451</td>\n",
       "      <td>3.247655</td>\n",
       "      <td>13.131918</td>\n",
       "      <td>5.056659</td>\n",
       "      <td>0.111335</td>\n",
       "      <td>7.92739</td>\n",
       "      <td>12.590493</td>\n",
       "      <td>0.064550</td>\n",
       "      <td>5.355533</td>\n",
       "      <td>0.213516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-141.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-36.000000</td>\n",
       "      <td>-152.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-37.000000</td>\n",
       "      <td>-141.000000</td>\n",
       "      <td>-53.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-147.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 799 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       divestiture   unfounded     caution   hampering  unnecessarily  \\\n",
       "count   240.000000  240.000000  240.000000  240.000000     240.000000   \n",
       "mean     -0.004167   -0.004167   -3.975000   -0.004167      -0.154167   \n",
       "std       0.064550    0.064550   14.327878    0.064550       2.324416   \n",
       "min      -1.000000   -1.000000 -141.000000   -1.000000     -36.000000   \n",
       "25%       0.000000    0.000000   -1.000000    0.000000       0.000000   \n",
       "50%       0.000000    0.000000    0.000000    0.000000       0.000000   \n",
       "75%       0.000000    0.000000    0.000000    0.000000       0.000000   \n",
       "max       0.000000    0.000000    0.000000    0.000000       0.000000   \n",
       "\n",
       "        shutdown  ineffective     trouble   conspired      enable  ...  \\\n",
       "count  240.00000   240.000000  240.000000  240.000000  240.000000  ...   \n",
       "mean    -0.97500    -0.020833   -0.025000   -0.004167    0.391667  ...   \n",
       "std      9.95888     0.143125    0.156451    0.064550    2.726887  ...   \n",
       "min   -152.00000    -1.000000   -1.000000   -1.000000    0.000000  ...   \n",
       "25%      0.00000     0.000000    0.000000    0.000000    0.000000  ...   \n",
       "50%      0.00000     0.000000    0.000000    0.000000    0.000000  ...   \n",
       "75%      0.00000     0.000000    0.000000    0.000000    0.000000  ...   \n",
       "max      0.00000     0.000000    0.000000    0.000000   27.000000  ...   \n",
       "\n",
       "          disrupt    worsened    downturn      worsen    mistaken    enhance  \\\n",
       "count  240.000000  240.000000  240.000000  240.000000  240.000000  240.00000   \n",
       "mean    -0.025000   -0.570833   -3.354167   -0.941667   -0.012500    1.90000   \n",
       "std      0.156451    3.247655   13.131918    5.056659    0.111335    7.92739   \n",
       "min     -1.000000  -37.000000 -141.000000  -53.000000   -1.000000    0.00000   \n",
       "25%      0.000000    0.000000   -1.000000    0.000000    0.000000    0.00000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.00000   \n",
       "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.00000   \n",
       "max      0.000000    0.000000    0.000000    0.000000    0.000000   66.00000   \n",
       "\n",
       "       unanticipated  burdensome  impressive        lose  \n",
       "count     240.000000  240.000000  240.000000  240.000000  \n",
       "mean       -2.700000   -0.004167    0.933333   -0.020833  \n",
       "std        12.590493    0.064550    5.355533    0.213516  \n",
       "min      -147.000000   -1.000000    0.000000   -3.000000  \n",
       "25%         0.000000    0.000000    0.000000    0.000000  \n",
       "50%         0.000000    0.000000    0.000000    0.000000  \n",
       "75%         0.000000    0.000000    0.000000    0.000000  \n",
       "max         0.000000    0.000000   49.000000    0.000000  \n",
       "\n",
       "[8 rows x 799 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_words.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8cc20d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desired</th>\n",
       "      <th>favored</th>\n",
       "      <th>improving</th>\n",
       "      <th>stable</th>\n",
       "      <th>attractive</th>\n",
       "      <th>good</th>\n",
       "      <th>desirable</th>\n",
       "      <th>opportunity</th>\n",
       "      <th>gain</th>\n",
       "      <th>boosted</th>\n",
       "      <th>...</th>\n",
       "      <th>improved</th>\n",
       "      <th>strong</th>\n",
       "      <th>better</th>\n",
       "      <th>strength</th>\n",
       "      <th>favorable</th>\n",
       "      <th>positive</th>\n",
       "      <th>improve</th>\n",
       "      <th>rebound</th>\n",
       "      <th>strengthening</th>\n",
       "      <th>rebounded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.00000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.816667</td>\n",
       "      <td>6.012500</td>\n",
       "      <td>7.108333</td>\n",
       "      <td>5.933333</td>\n",
       "      <td>3.487500</td>\n",
       "      <td>41.133333</td>\n",
       "      <td>6.370833</td>\n",
       "      <td>7.495833</td>\n",
       "      <td>39.962500</td>\n",
       "      <td>6.279167</td>\n",
       "      <td>...</td>\n",
       "      <td>7.087500</td>\n",
       "      <td>34.137500</td>\n",
       "      <td>5.741667</td>\n",
       "      <td>27.708333</td>\n",
       "      <td>24.02500</td>\n",
       "      <td>14.870833</td>\n",
       "      <td>3.387500</td>\n",
       "      <td>6.154167</td>\n",
       "      <td>13.808333</td>\n",
       "      <td>4.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.212447</td>\n",
       "      <td>22.626766</td>\n",
       "      <td>20.653060</td>\n",
       "      <td>14.244566</td>\n",
       "      <td>11.478917</td>\n",
       "      <td>79.312731</td>\n",
       "      <td>19.524901</td>\n",
       "      <td>21.268359</td>\n",
       "      <td>68.129972</td>\n",
       "      <td>14.805051</td>\n",
       "      <td>...</td>\n",
       "      <td>13.886745</td>\n",
       "      <td>64.572507</td>\n",
       "      <td>13.786474</td>\n",
       "      <td>61.956510</td>\n",
       "      <td>48.97595</td>\n",
       "      <td>35.800762</td>\n",
       "      <td>8.784487</td>\n",
       "      <td>16.316598</td>\n",
       "      <td>40.217358</td>\n",
       "      <td>11.234876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>25.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>116.000000</td>\n",
       "      <td>277.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>643.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>323.00000</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>119.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          desired     favored   improving      stable  attractive        good  \\\n",
       "count  240.000000  240.000000  240.000000  240.000000  240.000000  240.000000   \n",
       "mean     3.816667    6.012500    7.108333    5.933333    3.487500   41.133333   \n",
       "std     13.212447   22.626766   20.653060   14.244566   11.478917   79.312731   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    1.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    4.000000   \n",
       "50%      0.000000    0.000000    0.000000    2.000000    0.000000    7.000000   \n",
       "75%      0.250000    1.000000    2.000000    6.000000    0.000000   37.250000   \n",
       "max    116.000000  277.000000  143.000000  139.000000   89.000000  643.000000   \n",
       "\n",
       "        desirable  opportunity        gain     boosted  ...    improved  \\\n",
       "count  240.000000   240.000000  240.000000  240.000000  ...  240.000000   \n",
       "mean     6.370833     7.495833   39.962500    6.279167  ...    7.087500   \n",
       "std     19.524901    21.268359   68.129972   14.805051  ...   13.886745   \n",
       "min      0.000000     0.000000    0.000000    0.000000  ...    0.000000   \n",
       "25%      0.000000     0.000000    6.750000    0.000000  ...    0.000000   \n",
       "50%      0.000000     0.000000    9.000000    1.000000  ...    3.000000   \n",
       "75%      1.000000     1.000000   30.250000    3.000000  ...    6.000000   \n",
       "max    196.000000   175.000000  353.000000   96.000000  ...   84.000000   \n",
       "\n",
       "           strong      better    strength  favorable    positive     improve  \\\n",
       "count  240.000000  240.000000  240.000000  240.00000  240.000000  240.000000   \n",
       "mean    34.137500    5.741667   27.708333   24.02500   14.870833    3.387500   \n",
       "std     64.572507   13.786474   61.956510   48.97595   35.800762    8.784487   \n",
       "min      0.000000    0.000000    0.000000    0.00000    0.000000    0.000000   \n",
       "25%      4.000000    0.000000    1.000000    0.00000    0.000000    0.000000   \n",
       "50%      8.000000    1.000000    2.000000    2.00000    2.000000    1.000000   \n",
       "75%     25.250000    3.000000    8.000000   20.00000    5.250000    2.000000   \n",
       "max    527.000000   98.000000  396.000000  323.00000  239.000000   54.000000   \n",
       "\n",
       "          rebound  strengthening   rebounded  \n",
       "count  240.000000     240.000000  240.000000  \n",
       "mean     6.154167      13.808333    4.287500  \n",
       "std     16.316598      40.217358   11.234876  \n",
       "min      0.000000       0.000000    0.000000  \n",
       "25%      0.000000       0.000000    0.000000  \n",
       "50%      1.000000       1.000000    1.000000  \n",
       "75%      2.000000       3.000000    2.000000  \n",
       "max    142.000000     304.000000  119.000000  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean of each column in the 'subset_words' DataFrame\n",
    "column_means = subset_words.mean()\n",
    "\n",
    "# Create a subset of columns with mean >= 3\n",
    "selected_columns = column_means[column_means >= 3].index\n",
    "\n",
    "# Create a new DataFrame with only the selected columns\n",
    "subset_words_mean_3 = subset_words[selected_columns]\n",
    "\n",
    "subset_words_mean_3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2ee182f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the vectorized DataFrame with the original dataset\n",
    "second_dataset = pd.concat([dataset_adjusted, subset_words_mean_3], axis=1)\n",
    "\n",
    "second_dataset['Date'] = pd.to_datetime(second_dataset['Date'])\n",
    "\n",
    "# Calculate the time difference in days from the first date\n",
    "second_dataset['Date'] = (second_dataset['Date'] - second_dataset['Date'].min()).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9ef66de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 192\n",
      "Validation set size: 24\n",
      "Test set size: 24\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation, and test sets\n",
    "train, valid = train_test_split(second_dataset, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Split the combined set into validation and test sets\n",
    "valid, test = train_test_split(valid, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Verify the sizes of each set\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Validation set size:\", len(valid))\n",
    "print(\"Test set size:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "47eb4450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert variables to numeric in the train dataset\n",
    "variables_to_convert = train.columns.drop('Date')\n",
    "train[variables_to_convert] = train[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert variables to numeric in the valid dataset\n",
    "valid[variables_to_convert] = valid[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert variables to numeric in the test dataset\n",
    "test[variables_to_convert] = test[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_test = test.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "82fab057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in 'train' DataFrame\n",
    "missing_values_count = train.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'train'\n",
    "train = train.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "# Check for missing values in 'valid' DataFrame\n",
    "missing_values_count = valid.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'valid'\n",
    "valid = valid.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "# Check for missing values in 'test' DataFrame\n",
    "missing_values_count = test.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'test'\n",
    "test = test.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_test = test.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3b0004e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Create GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data and find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with optimal hyperparameters\n",
    "second_rf_model_5 = grid_search.best_estimator_\n",
    "\n",
    "# Predict y_train_pred on the training set\n",
    "y_train_pred = second_rf_model_5.predict(X_train)\n",
    "\n",
    "# Predict y_valid_pred on the validation set\n",
    "y_valid_pred = second_rf_model_5.predict(X_valid)\n",
    "\n",
    "# Round the predicted values to the nearest possible value\n",
    "y_train_pred = [round_to_nearest(val, possible_values) for val in y_train_pred]\n",
    "y_valid_pred = [round_to_nearest(val, possible_values) for val in y_valid_pred]\n",
    "\n",
    "# Output the random seed\n",
    "print(\"Random seed:\", random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cabc7dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the feature importances from the best model\n",
    "feature_importances = second_rf_model_5.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "num_features_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8d702e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------------+\n",
      "| Metric       |   Training Set |   Validation Set |\n",
      "+==============+================+==================+\n",
      "| MSE          |      0.0094401 |        0.0338542 |\n",
      "+--------------+----------------+------------------+\n",
      "| RMSE         |      0.0971602 |        0.183995  |\n",
      "+--------------+----------------+------------------+\n",
      "| R^2          |      0.764218  |        0.479132  |\n",
      "+--------------+----------------+------------------+\n",
      "| Adjusted R^2 |      0.653581  |        1.31526   |\n",
      "+--------------+----------------+------------------+\n",
      "| Accuracy     |      0.864583  |        0.708333  |\n",
      "+--------------+----------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(y_true)\n",
    "    \n",
    "    for true_val, pred_val in zip(y_true, y_pred):\n",
    "        if true_val == pred_val:\n",
    "            correct_predictions += 1\n",
    "            \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]\n",
    "p_train = X_train.shape[1]\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]\n",
    "p_valid = X_valid.shape[1]\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "\n",
    "threshold = 0.1\n",
    "accuracy_train = calculate_accuracy(y_train, y_train_pred)\n",
    "accuracy_valid = calculate_accuracy(y_valid, y_valid_pred)\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = [\n",
    "    [\"MSE\", mse_train, mse_valid],\n",
    "    [\"RMSE\", rmse_train, rmse_valid],\n",
    "    [\"R^2\", r2_train, r2_valid],\n",
    "    [\"Adjusted R^2\", adj_r2_train, adj_r2_valid],\n",
    "    [\"Accuracy\", accuracy_train, accuracy_valid],\n",
    "]\n",
    "\n",
    "# Prepare the headers for the table\n",
    "headers = [\"Metric\", \"Training Set\", \"Validation Set\"]\n",
    "\n",
    "# Display the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "de0af058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features and Absolute Standardized Relevance Scores (Descending Order):\n",
      "Short-Term Treasury Diff: 7.704887620577532\n",
      "LAG_RollingMean: 0.322017383010566\n",
      "LAG: 0.2851747920525605\n",
      "Consumer Sentiment: -0.2218933648952916\n",
      "improve: -0.21983855247856168\n",
      "boosted: -0.21941158295480928\n",
      "Sentiment Label: -0.2174058056107605\n",
      "Proportion Positive Words: -0.21300782176354285\n",
      "strengthen: -0.21111486549230846\n",
      "strong: -0.20346416882301052\n",
      "Proportion Negative Words: -0.20211728782651403\n",
      "improving: -0.19996993106595012\n",
      "stability: -0.19586754455493005\n",
      "Housing Sales: -0.19330671846947425\n",
      "better: -0.19134663781775868\n",
      "gain: -0.19122230508986315\n",
      "stable: -0.1906684690938487\n",
      "despite: -0.19055113984333438\n",
      "favored: -0.18896703183441366\n",
      "strength: -0.1886219593736572\n",
      "rebounded: -0.18830492285820247\n",
      "strengthened: -0.18693044827251917\n",
      "desirable: -0.18607437575948096\n",
      "Nonfarm Payroll: -0.1815852920709761\n",
      "Long-Term Treasury Bond Rate: -0.17899375689541397\n",
      "desired: -0.17825792908437976\n",
      "boost: -0.17712798400302385\n",
      "Level: -0.17316212408239776\n",
      "Date: -0.17275230051155585\n",
      "strengthening: -0.16722206827666575\n",
      "good: -0.1607649734383814\n",
      "LEI: -0.158524726883193\n",
      "Short-Term Treasury Bond Rate: -0.15764770597211042\n",
      "Treasury Deposits: -0.15693309582778328\n",
      "progress: -0.15386028078713973\n",
      "Unemployment Rate: -0.15294043919212436\n",
      "Bank Reserves: -0.15233110977270722\n",
      "improvement: -0.14336178384765455\n",
      "positive: -0.13933119218497125\n",
      "favorable: -0.13486681271803463\n",
      "Average Hourly Earnings: -0.1343359651854211\n",
      "CPI: -0.129797085585158\n",
      "upturn: -0.12969225192980563\n",
      "Positive Frequency: -0.12325839800034792\n",
      "Word Count: -0.11766096061684511\n",
      "opportunity: -0.11380611697824401\n",
      "Durable Goods Orders_RollingMean: -0.10939236360081313\n",
      "Negative Frequency: -0.10789566338481023\n",
      "Standardized Sentiment Score: -0.10653676452278475\n",
      "Net Sentiment Score: -0.10653676452278467\n",
      "stabilization: -0.10227678023900702\n",
      "attractive: -0.09416078437334478\n",
      "improved: -0.08843831364620064\n",
      "stronger: -0.08826641691125396\n",
      "CEI: 0.07953226533544056\n",
      "CEI_RollingMean: 0.0766988468026832\n",
      "Retail Sales_RollingMean: -0.06699895633303157\n",
      "Retail Sales: -0.055701634502943506\n",
      "Durable Goods Orders: -0.054720474010454656\n",
      "rebound: 0.04557192898080538\n",
      "LEI_RollingMean: -0.044658632989592946\n"
     ]
    }
   ],
   "source": [
    "# Subset numerical columns\n",
    "numerical_columns = second_dataset.select_dtypes(include='number')\n",
    "\n",
    "# Remove 'Difference', 'Increase', and 'Decrease' from numerical columns\n",
    "columns_to_exclude = ['Difference', 'Increase', 'Decrease']\n",
    "numerical_columns_subset = numerical_columns.drop(columns_to_exclude, axis=1)\n",
    "\n",
    "# Fill missing values with the mean in numerical columns\n",
    "numerical_columns_subset.fillna(numerical_columns_subset.mean(), inplace=True)\n",
    "\n",
    "# Fill missing values with the mean in 'Difference'\n",
    "difference_mean = second_dataset['Difference'].mean()\n",
    "second_dataset['Difference'].fillna(difference_mean, inplace=True)\n",
    "\n",
    "# Define the number of features to select\n",
    "k = 61\n",
    "\n",
    "# Perform univariate selection using f_regression\n",
    "selector = SelectKBest(score_func=f_regression, k=k)\n",
    "selected_features = selector.fit_transform(numerical_columns_subset, second_dataset['Difference'])\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_features_names = numerical_columns_subset.columns[selected_indices]\n",
    "\n",
    "# Get the feature scores\n",
    "feature_scores = selector.scores_[selected_indices]\n",
    "\n",
    "# Standardize the feature scores\n",
    "scaler = StandardScaler()\n",
    "standardized_scores = scaler.fit_transform(feature_scores.reshape(-1, 1))\n",
    "\n",
    "# Combine selected feature names and their standardized scores\n",
    "selected_features_with_scores = list(zip(selected_features_names, standardized_scores))\n",
    "\n",
    "# Sort the selected features by the absolute value of standardized scores in descending order\n",
    "selected_features_with_scores.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Print the selected features and their standardized relevance scores in descending order\n",
    "print(\"Selected Features and Absolute Standardized Relevance Scores (Descending Order):\")\n",
    "for feature, score in selected_features_with_scores:\n",
    "    print(f\"{feature}: {score[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
