{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "87439644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import random\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tabulate import tabulate\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "02d6f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and loaded into 'dataset_adjusted' successfully.\n",
      "Dataset downloaded and loaded into 'words' successfully.\n"
     ]
    }
   ],
   "source": [
    "# The URL of the raw dataset on GitHub\n",
    "url = \"https://raw.githubusercontent.com/abactat/BC-Project/main/data/dataset_adjusted.csv?token=GHSAT0AAAAAACC4ZCNKN5F6XR7HZA75QWTEZGQVYIA\"\n",
    "\n",
    "# Send an HTTP GET request to fetch the content of the raw dataset\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200 means success)\n",
    "if response.status_code == 200:\n",
    "    # Read the content as a pandas DataFrame and assign it to the 'train' variable\n",
    "    dataset_adjusted = pd.read_csv(StringIO(response.text))\n",
    "    print(\"Dataset downloaded and loaded into 'dataset_adjusted' successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download the dataset. Status code: {response.status_code}\")\n",
    "    \n",
    "# The URL of the raw dataset on GitHub\n",
    "url = \"https://raw.githubusercontent.com/abactat/BC-Project/main/data/raw/dataset_words.csv?token=GHSAT0AAAAAACC4ZCNKDB2KTQ7KVOHGVHMAZGQVX7A\"\n",
    "\n",
    "# Send an HTTP GET request to fetch the content of the raw dataset\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200 means success)\n",
    "if response.status_code == 200:\n",
    "    # Read the content as a pandas DataFrame and assign it to the 'valid' variable\n",
    "    words = pd.read_csv(StringIO(response.text))\n",
    "    print(\"Dataset downloaded and loaded into 'words' successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download the dataset. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "68bfa6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentiment word list from the CSV file into a dictionary\n",
    "sentiment_word_list = {}\n",
    "with open(r\"C:\\Users\\abact\\BC-Project\\data\\external\\Loughran-McDonald_MasterDictionary_1993-2021.csv\", 'r') as file:\n",
    "    # Skip the header line\n",
    "    next(file)\n",
    "\n",
    "    for line in file:\n",
    "        values = line.strip().split(',')\n",
    "\n",
    "        # Extract the necessary values\n",
    "        word = values[0].lower()\n",
    "        positive = float(values[8])  # Positive column index is 8\n",
    "        negative = float(values[7])  # Negative column index is 7\n",
    "\n",
    "        # Assign the word as positive or negative based on the positive or negative values\n",
    "        if positive == 2009:\n",
    "            sentiment_word_list[word] = 1\n",
    "        elif negative == 2009:\n",
    "            sentiment_word_list[word] = -1\n",
    "\n",
    "# Convert the sentiment word list keys to lowercase\n",
    "selected_words = set(sentiment_word_list.keys())\n",
    "\n",
    "# Filter the 'words' DataFrame to include only columns that are present in both 'selected_words' and 'words'\n",
    "common_columns = selected_words.intersection(words.columns)\n",
    "subset_words = words[list(common_columns)].copy()\n",
    "\n",
    "# Multiply sentiment values to the vectorized text columns in the 'subset_words' DataFrame\n",
    "for column in subset_words.columns:\n",
    "    sentiment_value = sentiment_word_list.get(column, 0)\n",
    "    if sentiment_value == 1:\n",
    "        subset_words.loc[:, column] = subset_words[column] * 1  # Multiply by 1 for positive sentiment\n",
    "    elif sentiment_value == -1:\n",
    "        subset_words.loc[:, column] = subset_words[column] * -1  # Multiply by -1 for negative sentiment\n",
    "    else:\n",
    "        subset_words.loc[:, column] = subset_words[column] * 0  # Multiply by 0 for unknown sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0293b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the vectorized DataFrame with the original dataset\n",
    "full_dataset = pd.concat([dataset_adjusted, subset_words], axis=1)\n",
    "\n",
    "full_dataset['Date'] = pd.to_datetime(full_dataset['Date'])\n",
    "\n",
    "# Calculate the time difference in days from the first date\n",
    "full_dataset['Date'] = (full_dataset['Date'] - full_dataset['Date'].min()).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9a9a7a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federal_Reserve_Mins    0\n",
      "Preprocessed Text       0\n",
      "Date                    0\n",
      "Difference              0\n",
      "Increase                0\n",
      "                       ..\n",
      "enhance                 0\n",
      "unanticipated           0\n",
      "burdensome              0\n",
      "impressive              0\n",
      "lose                    0\n",
      "Length: 837, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for the number of missing values in 'train' DataFrame\n",
    "missing_values_count = full_dataset.isna().sum()\n",
    "\n",
    "# Print the count of missing values for each column\n",
    "print(missing_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "95f33b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with Missing Values:\n"
     ]
    }
   ],
   "source": [
    "# Filter columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0]\n",
    "\n",
    "# Print columns with missing values and their counts\n",
    "print(\"Columns with Missing Values:\")\n",
    "for column, count in columns_with_missing_values.items():\n",
    "    print(f\"{column}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "90d18c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 192\n",
      "Validation set size: 24\n",
      "Test set size: 24\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation, and test sets\n",
    "train, valid = train_test_split(full_dataset, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Split the combined set into validation and test sets\n",
    "valid, test = train_test_split(valid, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Verify the sizes of each set\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Validation set size:\", len(valid))\n",
    "print(\"Test set size:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "589999b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert variables to numeric in the train dataset\n",
    "variables_to_convert = train.columns.drop('Date')\n",
    "train[variables_to_convert] = train[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert variables to numeric in the valid dataset\n",
    "valid[variables_to_convert] = valid[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert variables to numeric in the test dataset\n",
    "test[variables_to_convert] = test[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_test = test.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0fa5f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(y_true)\n",
    "    \n",
    "    for true_val, pred_val in zip(y_true, y_pred):\n",
    "        if true_val == pred_val:\n",
    "            correct_predictions += 1\n",
    "            \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "possible_values = [-1.00, -0.75, -0.50, -0.25, 0.00, 0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "def round_to_nearest(value, possible_values):\n",
    "    return min(possible_values, key=lambda x: abs(x - value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1b2cb31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in 'train' DataFrame\n",
    "missing_values_count = train.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'train'\n",
    "train = train.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "# Check for missing values in 'valid' DataFrame\n",
    "missing_values_count = valid.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'valid'\n",
    "valid = valid.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "# Check for missing values in 'test' DataFrame\n",
    "missing_values_count = test.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'test'\n",
    "test = test.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_test = test.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3009745c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date             0\n",
      "Difference       0\n",
      "Increase         0\n",
      "Decrease         0\n",
      "Level            0\n",
      "                ..\n",
      "enhance          0\n",
      "unanticipated    0\n",
      "burdensome       0\n",
      "impressive       0\n",
      "lose             0\n",
      "Length: 833, dtype: int64\n",
      "Columns with Missing Values:\n"
     ]
    }
   ],
   "source": [
    "# Check for the number of missing values in 'train' DataFrame\n",
    "missing_values_count = train.isna().sum()\n",
    "\n",
    "# Print the count of missing values for each column\n",
    "print(missing_values_count)\n",
    "\n",
    "# Filter columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0]\n",
    "\n",
    "# Print columns with missing values and their counts\n",
    "print(\"Columns with Missing Values:\")\n",
    "for column, count in columns_with_missing_values.items():\n",
    "    print(f\"{column}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6b0a28f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Create GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data and find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with optimal hyperparameters\n",
    "best_rf_model_5 = grid_search.best_estimator_\n",
    "\n",
    "# Predict y_train_pred on the training set\n",
    "y_train_pred = best_rf_model_5.predict(X_train)\n",
    "\n",
    "# Predict y_valid_pred on the validation set\n",
    "y_valid_pred = best_rf_model_5.predict(X_valid)\n",
    "\n",
    "# Round the predicted values to the nearest possible value\n",
    "y_train_pred = [round_to_nearest(val, possible_values) for val in y_train_pred]\n",
    "y_valid_pred = [round_to_nearest(val, possible_values) for val in y_valid_pred]\n",
    "\n",
    "# Output the random seed\n",
    "print(\"Random seed:\", random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6786c8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the feature importances from the best model\n",
    "feature_importances = best_rf_model_5.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "num_features_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e88ce0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------------+\n",
      "| Metric       |   Training Set |   Validation Set |\n",
      "+==============+================+==================+\n",
      "| MSE          |     0.00358073 |        0.0260417 |\n",
      "+--------------+----------------+------------------+\n",
      "| RMSE         |     0.0598392  |        0.161374  |\n",
      "+--------------+----------------+------------------+\n",
      "| R^2          |     0.910565   |        0.599332  |\n",
      "+--------------+----------------+------------------+\n",
      "| Adjusted R^2 |     1.02673    |        1.01142   |\n",
      "+--------------+----------------+------------------+\n",
      "| Accuracy     |     0.942708   |        0.708333  |\n",
      "+--------------+----------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]\n",
    "p_train = X_train.shape[1]\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]\n",
    "p_valid = X_valid.shape[1]\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "\n",
    "# Calculate accuracy for training and validation sets\n",
    "accuracy_train = calculate_accuracy(y_train, y_train_pred)\n",
    "accuracy_valid = calculate_accuracy(y_valid, y_valid_pred)\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = [\n",
    "    [\"MSE\", mse_train, mse_valid],\n",
    "    [\"RMSE\", rmse_train, rmse_valid],\n",
    "    [\"R^2\", r2_train, r2_valid],\n",
    "    [\"Adjusted R^2\", adj_r2_train, adj_r2_valid],\n",
    "    [\"Accuracy\", accuracy_train, accuracy_valid],\n",
    "]\n",
    "\n",
    "# Prepare the headers for the table\n",
    "headers = [\"Metric\", \"Training Set\", \"Validation Set\"]\n",
    "\n",
    "# Display the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bcfc89fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seeds: [2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# List to store random seeds used in each iteration\n",
    "random_seeds = []\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Function to calculate accuracy based on a threshold\n",
    "def calculate_accuracy(y_true, y_pred, threshold):\n",
    "    num_samples = len(y_true)\n",
    "    correct_predictions = sum(abs(y_true - y_pred) <= threshold)\n",
    "    return correct_predictions / num_samples\n",
    "\n",
    "best_accuracy = -1.0\n",
    "optimal_cv = None\n",
    "best_y_train_pred = None\n",
    "best_y_valid_pred = None\n",
    "threshold = 0.1  # Define your desired threshold here\n",
    "\n",
    "for cv in range(2, 11):  # Try cross-validation folds from 2 to 10\n",
    "    # Set the random seed for reproducibility\n",
    "    random_seed = cv  # Use cv as the random seed\n",
    "    random_seeds.append(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=cv, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model with optimal hyperparameters\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predict y_train_pred on the training set\n",
    "    y_train_pred = best_rf_model.predict(X_train)\n",
    "\n",
    "    # Predict y_valid_pred on the validation set\n",
    "    y_valid_pred = best_rf_model.predict(X_valid)\n",
    "\n",
    "    # Round the predicted values to the nearest possible value\n",
    "    y_train_pred = [round_to_nearest(val, possible_values) for val in y_train_pred]\n",
    "    y_valid_pred = [round_to_nearest(val, possible_values) for val in y_valid_pred]\n",
    "\n",
    "    # Calculate accuracy for training and validation sets after rounding\n",
    "    accuracy_train = calculate_accuracy(y_train, y_train_pred, threshold)\n",
    "    accuracy_valid = calculate_accuracy(y_valid, y_valid_pred, threshold)\n",
    "    \n",
    "    # Check if the accuracy after rounding is higher than the best accuracy so far\n",
    "    if accuracy_valid > best_accuracy:\n",
    "        best_accuracy = accuracy_valid\n",
    "        optimal_cv = cv\n",
    "        best_y_train_pred = y_train_pred\n",
    "        best_y_valid_pred = y_valid_pred\n",
    "\n",
    "# Use the optimal number of folds in GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=optimal_cv, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with optimal hyperparameters\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Get the feature importances from the best model\n",
    "feature_importances = best_rf_model.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "# Print the random seeds used in each iteration\n",
    "print(\"Random Seeds:\", random_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a1ec3e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the feature importances from the best model\n",
    "feature_importances = best_rf_model.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "num_features_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2cf567d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------------+\n",
      "| Metric       |   Training Set |   Validation Set |\n",
      "+==============+================+==================+\n",
      "| MSE          |     0.00455729 |         0.03125  |\n",
      "+--------------+----------------+------------------+\n",
      "| RMSE         |     0.0675077  |         0.176777 |\n",
      "+--------------+----------------+------------------+\n",
      "| R^2          |     0.886174   |         0.519199 |\n",
      "+--------------+----------------+------------------+\n",
      "| Adjusted R^2 |     1.00323    |         1.0016   |\n",
      "+--------------+----------------+------------------+\n",
      "| Accuracy     |     0.927083   |         0.625    |\n",
      "+--------------+----------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]\n",
    "p_train = X_train.shape[1]\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]\n",
    "p_valid = X_valid.shape[1]\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "\n",
    "threshold = 0.1\n",
    "accuracy_train = calculate_accuracy(y_train, y_train_pred, threshold)\n",
    "accuracy_valid = calculate_accuracy(y_valid, y_valid_pred, threshold)\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = [\n",
    "    [\"MSE\", mse_train, mse_valid],\n",
    "    [\"RMSE\", rmse_train, rmse_valid],\n",
    "    [\"R^2\", r2_train, r2_valid],\n",
    "    [\"Adjusted R^2\", adj_r2_train, adj_r2_valid],\n",
    "    [\"Accuracy\", accuracy_train, accuracy_valid],\n",
    "]\n",
    "\n",
    "# Prepare the headers for the table\n",
    "headers = [\"Metric\", \"Training Set\", \"Validation Set\"]\n",
    "\n",
    "# Display the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e784bdbf",
   "metadata": {},
   "source": [
    "###Data Centric AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a9094080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>divestiture</th>\n",
       "      <th>unfounded</th>\n",
       "      <th>caution</th>\n",
       "      <th>hampering</th>\n",
       "      <th>unnecessarily</th>\n",
       "      <th>shutdown</th>\n",
       "      <th>ineffective</th>\n",
       "      <th>trouble</th>\n",
       "      <th>conspired</th>\n",
       "      <th>enable</th>\n",
       "      <th>...</th>\n",
       "      <th>disrupt</th>\n",
       "      <th>worsened</th>\n",
       "      <th>downturn</th>\n",
       "      <th>worsen</th>\n",
       "      <th>mistaken</th>\n",
       "      <th>enhance</th>\n",
       "      <th>unanticipated</th>\n",
       "      <th>burdensome</th>\n",
       "      <th>impressive</th>\n",
       "      <th>lose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.00000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.00000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.004167</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>-3.975000</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>-0.154167</td>\n",
       "      <td>-0.97500</td>\n",
       "      <td>-0.020833</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>-0.570833</td>\n",
       "      <td>-3.354167</td>\n",
       "      <td>-0.941667</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>1.90000</td>\n",
       "      <td>-2.700000</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>-0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.064550</td>\n",
       "      <td>0.064550</td>\n",
       "      <td>14.327878</td>\n",
       "      <td>0.064550</td>\n",
       "      <td>2.324416</td>\n",
       "      <td>9.95888</td>\n",
       "      <td>0.143125</td>\n",
       "      <td>0.156451</td>\n",
       "      <td>0.064550</td>\n",
       "      <td>2.726887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156451</td>\n",
       "      <td>3.247655</td>\n",
       "      <td>13.131918</td>\n",
       "      <td>5.056659</td>\n",
       "      <td>0.111335</td>\n",
       "      <td>7.92739</td>\n",
       "      <td>12.590493</td>\n",
       "      <td>0.064550</td>\n",
       "      <td>5.355533</td>\n",
       "      <td>0.213516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-141.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-36.000000</td>\n",
       "      <td>-152.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-37.000000</td>\n",
       "      <td>-141.000000</td>\n",
       "      <td>-53.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-147.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 799 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       divestiture   unfounded     caution   hampering  unnecessarily  \\\n",
       "count   240.000000  240.000000  240.000000  240.000000     240.000000   \n",
       "mean     -0.004167   -0.004167   -3.975000   -0.004167      -0.154167   \n",
       "std       0.064550    0.064550   14.327878    0.064550       2.324416   \n",
       "min      -1.000000   -1.000000 -141.000000   -1.000000     -36.000000   \n",
       "25%       0.000000    0.000000   -1.000000    0.000000       0.000000   \n",
       "50%       0.000000    0.000000    0.000000    0.000000       0.000000   \n",
       "75%       0.000000    0.000000    0.000000    0.000000       0.000000   \n",
       "max       0.000000    0.000000    0.000000    0.000000       0.000000   \n",
       "\n",
       "        shutdown  ineffective     trouble   conspired      enable  ...  \\\n",
       "count  240.00000   240.000000  240.000000  240.000000  240.000000  ...   \n",
       "mean    -0.97500    -0.020833   -0.025000   -0.004167    0.391667  ...   \n",
       "std      9.95888     0.143125    0.156451    0.064550    2.726887  ...   \n",
       "min   -152.00000    -1.000000   -1.000000   -1.000000    0.000000  ...   \n",
       "25%      0.00000     0.000000    0.000000    0.000000    0.000000  ...   \n",
       "50%      0.00000     0.000000    0.000000    0.000000    0.000000  ...   \n",
       "75%      0.00000     0.000000    0.000000    0.000000    0.000000  ...   \n",
       "max      0.00000     0.000000    0.000000    0.000000   27.000000  ...   \n",
       "\n",
       "          disrupt    worsened    downturn      worsen    mistaken    enhance  \\\n",
       "count  240.000000  240.000000  240.000000  240.000000  240.000000  240.00000   \n",
       "mean    -0.025000   -0.570833   -3.354167   -0.941667   -0.012500    1.90000   \n",
       "std      0.156451    3.247655   13.131918    5.056659    0.111335    7.92739   \n",
       "min     -1.000000  -37.000000 -141.000000  -53.000000   -1.000000    0.00000   \n",
       "25%      0.000000    0.000000   -1.000000    0.000000    0.000000    0.00000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.00000   \n",
       "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.00000   \n",
       "max      0.000000    0.000000    0.000000    0.000000    0.000000   66.00000   \n",
       "\n",
       "       unanticipated  burdensome  impressive        lose  \n",
       "count     240.000000  240.000000  240.000000  240.000000  \n",
       "mean       -2.700000   -0.004167    0.933333   -0.020833  \n",
       "std        12.590493    0.064550    5.355533    0.213516  \n",
       "min      -147.000000   -1.000000    0.000000   -3.000000  \n",
       "25%         0.000000    0.000000    0.000000    0.000000  \n",
       "50%         0.000000    0.000000    0.000000    0.000000  \n",
       "75%         0.000000    0.000000    0.000000    0.000000  \n",
       "max         0.000000    0.000000   49.000000    0.000000  \n",
       "\n",
       "[8 rows x 799 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_words.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "74b5b843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caution</th>\n",
       "      <th>severe</th>\n",
       "      <th>desired</th>\n",
       "      <th>liquidation</th>\n",
       "      <th>favored</th>\n",
       "      <th>improving</th>\n",
       "      <th>stable</th>\n",
       "      <th>disappointing</th>\n",
       "      <th>tightening</th>\n",
       "      <th>unemployment</th>\n",
       "      <th>...</th>\n",
       "      <th>weakness</th>\n",
       "      <th>adverse</th>\n",
       "      <th>slow</th>\n",
       "      <th>diminished</th>\n",
       "      <th>question</th>\n",
       "      <th>imbalance</th>\n",
       "      <th>strengthening</th>\n",
       "      <th>slowing</th>\n",
       "      <th>rebounded</th>\n",
       "      <th>downturn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.975000</td>\n",
       "      <td>-4.083333</td>\n",
       "      <td>3.816667</td>\n",
       "      <td>-4.791667</td>\n",
       "      <td>6.012500</td>\n",
       "      <td>7.108333</td>\n",
       "      <td>5.933333</td>\n",
       "      <td>-3.854167</td>\n",
       "      <td>-22.875000</td>\n",
       "      <td>-23.754167</td>\n",
       "      <td>...</td>\n",
       "      <td>-28.266667</td>\n",
       "      <td>-11.283333</td>\n",
       "      <td>-8.441667</td>\n",
       "      <td>-5.262500</td>\n",
       "      <td>-8.616667</td>\n",
       "      <td>-3.875000</td>\n",
       "      <td>13.808333</td>\n",
       "      <td>-15.433333</td>\n",
       "      <td>4.287500</td>\n",
       "      <td>-3.354167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.327878</td>\n",
       "      <td>19.696052</td>\n",
       "      <td>13.212447</td>\n",
       "      <td>19.041859</td>\n",
       "      <td>22.626766</td>\n",
       "      <td>20.653060</td>\n",
       "      <td>14.244566</td>\n",
       "      <td>16.120609</td>\n",
       "      <td>56.657757</td>\n",
       "      <td>39.604240</td>\n",
       "      <td>...</td>\n",
       "      <td>77.153624</td>\n",
       "      <td>27.362340</td>\n",
       "      <td>18.384683</td>\n",
       "      <td>13.457272</td>\n",
       "      <td>26.702822</td>\n",
       "      <td>12.461195</td>\n",
       "      <td>40.217358</td>\n",
       "      <td>34.193330</td>\n",
       "      <td>11.234876</td>\n",
       "      <td>13.131918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-141.000000</td>\n",
       "      <td>-254.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-173.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-130.000000</td>\n",
       "      <td>-358.000000</td>\n",
       "      <td>-344.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-634.000000</td>\n",
       "      <td>-209.000000</td>\n",
       "      <td>-148.000000</td>\n",
       "      <td>-106.000000</td>\n",
       "      <td>-243.000000</td>\n",
       "      <td>-105.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-243.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-141.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.250000</td>\n",
       "      <td>-21.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-12.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>277.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          caution      severe     desired  liquidation     favored  \\\n",
       "count  240.000000  240.000000  240.000000   240.000000  240.000000   \n",
       "mean    -3.975000   -4.083333    3.816667    -4.791667    6.012500   \n",
       "std     14.327878   19.696052   13.212447    19.041859   22.626766   \n",
       "min   -141.000000 -254.000000    0.000000  -173.000000    0.000000   \n",
       "25%     -1.000000   -1.000000    0.000000     0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000     0.000000    0.000000   \n",
       "75%      0.000000    0.000000    0.250000     0.000000    1.000000   \n",
       "max      0.000000    0.000000  116.000000     0.000000  277.000000   \n",
       "\n",
       "        improving      stable  disappointing  tightening  unemployment  ...  \\\n",
       "count  240.000000  240.000000     240.000000  240.000000    240.000000  ...   \n",
       "mean     7.108333    5.933333      -3.854167  -22.875000    -23.754167  ...   \n",
       "std     20.653060   14.244566      16.120609   56.657757     39.604240  ...   \n",
       "min      0.000000    0.000000    -130.000000 -358.000000   -344.000000  ...   \n",
       "25%      0.000000    0.000000       0.000000   -8.250000    -21.000000  ...   \n",
       "50%      0.000000    2.000000       0.000000   -2.000000    -12.500000  ...   \n",
       "75%      2.000000    6.000000       0.000000   -1.000000     -6.000000  ...   \n",
       "max    143.000000  139.000000       0.000000    0.000000     -1.000000  ...   \n",
       "\n",
       "         weakness     adverse        slow  diminished    question   imbalance  \\\n",
       "count  240.000000  240.000000  240.000000  240.000000  240.000000  240.000000   \n",
       "mean   -28.266667  -11.283333   -8.441667   -5.262500   -8.616667   -3.875000   \n",
       "std     77.153624   27.362340   18.384683   13.457272   26.702822   12.461195   \n",
       "min   -634.000000 -209.000000 -148.000000 -106.000000 -243.000000 -105.000000   \n",
       "25%     -8.000000   -4.000000   -5.000000   -3.000000   -1.000000   -2.000000   \n",
       "50%     -2.000000   -2.000000   -3.000000    0.000000    0.000000    0.000000   \n",
       "75%     -0.750000    0.000000   -1.000000    0.000000    0.000000    0.000000   \n",
       "max      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "       strengthening     slowing   rebounded    downturn  \n",
       "count     240.000000  240.000000  240.000000  240.000000  \n",
       "mean       13.808333  -15.433333    4.287500   -3.354167  \n",
       "std        40.217358   34.193330   11.234876   13.131918  \n",
       "min         0.000000 -243.000000    0.000000 -141.000000  \n",
       "25%         0.000000   -6.000000    0.000000   -1.000000  \n",
       "50%         1.000000   -2.000000    1.000000    0.000000  \n",
       "75%         3.000000   -1.000000    2.000000    0.000000  \n",
       "max       304.000000    0.000000  119.000000    0.000000  \n",
       "\n",
       "[8 rows x 83 columns]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean of each column in the 'subset_words' DataFrame\n",
    "column_means = subset_words.mean()\n",
    "\n",
    "# Create a subset of columns with mean >= |3|\n",
    "selected_columns = column_means[column_means.abs() >= 3].index\n",
    "\n",
    "# Create a new DataFrame with only the selected columns\n",
    "subset_words_mean_3 = subset_words[selected_columns]\n",
    "\n",
    "subset_words_mean_3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "babf7f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the vectorized DataFrame with the original dataset\n",
    "second_dataset = pd.concat([dataset_adjusted, subset_words_mean_3], axis=1)\n",
    "\n",
    "second_dataset['Date'] = pd.to_datetime(second_dataset['Date'])\n",
    "\n",
    "# Calculate the time difference in days from the first date\n",
    "second_dataset['Date'] = (second_dataset['Date'] - second_dataset['Date'].min()).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "852f92a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 192\n",
      "Validation set size: 24\n",
      "Test set size: 24\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation, and test sets\n",
    "train, valid = train_test_split(second_dataset, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Split the combined set into validation and test sets\n",
    "valid, test = train_test_split(valid, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Verify the sizes of each set\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Validation set size:\", len(valid))\n",
    "print(\"Test set size:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b84dd39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert variables to numeric in the train dataset\n",
    "variables_to_convert = train.columns.drop('Date')\n",
    "train[variables_to_convert] = train[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert variables to numeric in the valid dataset\n",
    "valid[variables_to_convert] = valid[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert variables to numeric in the test dataset\n",
    "test[variables_to_convert] = test[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_test = test.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "55201092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in 'train' DataFrame\n",
    "missing_values_count = train.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'train'\n",
    "train = train.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "# Check for missing values in 'valid' DataFrame\n",
    "missing_values_count = valid.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'valid'\n",
    "valid = valid.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "# Check for missing values in 'test' DataFrame\n",
    "missing_values_count = test.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'test'\n",
    "test = test.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_test = test.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "d4fca831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Create GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data and find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with optimal hyperparameters\n",
    "second_rf_model_5 = grid_search.best_estimator_\n",
    "\n",
    "# Predict y_train_pred on the training set\n",
    "y_train_pred = second_rf_model_5.predict(X_train)\n",
    "\n",
    "# Predict y_valid_pred on the validation set\n",
    "y_valid_pred = second_rf_model_5.predict(X_valid)\n",
    "\n",
    "# Round the predicted values to the nearest possible value\n",
    "y_train_pred = [round_to_nearest(val, possible_values) for val in y_train_pred]\n",
    "y_valid_pred = [round_to_nearest(val, possible_values) for val in y_valid_pred]\n",
    "\n",
    "# Output the random seed\n",
    "print(\"Random seed:\", random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "d0e79e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the feature importances from the best model\n",
    "feature_importances = second_rf_model_5.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "num_features_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "8c45088d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------------+\n",
      "| Metric       |   Training Set |   Validation Set |\n",
      "+==============+================+==================+\n",
      "| MSE          |     0.00292969 |        0.0364583 |\n",
      "+--------------+----------------+------------------+\n",
      "| RMSE         |     0.0541266  |        0.190941  |\n",
      "+--------------+----------------+------------------+\n",
      "| R^2          |     0.926826   |        0.439065  |\n",
      "+--------------+----------------+------------------+\n",
      "| Adjusted R^2 |     0.818491   |        1.14177   |\n",
      "+--------------+----------------+------------------+\n",
      "| Accuracy     |     0.953125   |        0.666667  |\n",
      "+--------------+----------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(y_true)\n",
    "    \n",
    "    for true_val, pred_val in zip(y_true, y_pred):\n",
    "        if true_val == pred_val:\n",
    "            correct_predictions += 1\n",
    "            \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]\n",
    "p_train = X_train.shape[1]\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]\n",
    "p_valid = X_valid.shape[1]\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "\n",
    "threshold = 0.1\n",
    "accuracy_train = calculate_accuracy(y_train, y_train_pred)\n",
    "accuracy_valid = calculate_accuracy(y_valid, y_valid_pred)\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = [\n",
    "    [\"MSE\", mse_train, mse_valid],\n",
    "    [\"RMSE\", rmse_train, rmse_valid],\n",
    "    [\"R^2\", r2_train, r2_valid],\n",
    "    [\"Adjusted R^2\", adj_r2_train, adj_r2_valid],\n",
    "    [\"Accuracy\", accuracy_train, accuracy_valid],\n",
    "]\n",
    "\n",
    "# Prepare the headers for the table\n",
    "headers = [\"Metric\", \"Training Set\", \"Validation Set\"]\n",
    "\n",
    "# Display the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "ac351eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features and Absolute Standardized Relevance Scores (Descending Order):\n",
      "Short-Term Treasury Diff: 8.91927189306211\n",
      "LAG_RollingMean: 0.35773651450958127\n",
      "easing: 0.33990792576034856\n",
      "LAG: 0.3150120542692436\n",
      "weaker: 0.30616666956121863\n",
      "weakness: 0.23855274810032798\n",
      "Nonfarm Payroll: -0.22626563921988915\n",
      "Long-Term Treasury Bond Rate: -0.2232603689961976\n",
      "desired: -0.2224070672810142\n",
      "boost: -0.22109672801206368\n",
      "slowly: -0.2210073315188488\n",
      "problem: -0.21877354224871173\n",
      "Level: -0.2164977239764715\n",
      "deficit: -0.21628520676205676\n",
      "Date: -0.21602247263277677\n",
      "volatility: -0.21239410077078177\n",
      "weakened: -0.20991963417539647\n",
      "strengthening: -0.20960934638145778\n",
      "caution: -0.2030745216928687\n",
      "good: -0.2021213851791409\n",
      "severe: -0.20189352532336366\n",
      "lagged: -0.20121125470590306\n",
      "LEI: -0.19952348632385147\n",
      "Short-Term Treasury Bond Rate: -0.19850645022320904\n",
      "Treasury Deposits: -0.19767775354621783\n",
      "progress: -0.194114367784898\n",
      "Unemployment Rate: -0.19304767473666912\n",
      "absence: -0.19285151437827744\n",
      "Bank Reserves: -0.1923410667084002\n",
      "difficulty: -0.19197522567367034\n",
      "slowdown: -0.19143975143516545\n",
      "tightening: -0.18773306918951965\n",
      "improvement: -0.1819398001605523\n",
      "depressed: -0.18155836661576955\n",
      "disappointing: -0.17752856377719886\n",
      "positive: -0.17726573004877236\n",
      "declining: -0.17538310418967212\n",
      "cut: -0.17210358315993446\n",
      "favorable: -0.17208861850278132\n",
      "Average Hourly Earnings: -0.17147302188725885\n",
      "contraction: 0.16889384253712583\n",
      "volatile: -0.1677108914181054\n",
      "persistence: -0.16700769660996423\n",
      "CPI: -0.16620951636349807\n",
      "upturn: -0.16608794615837713\n",
      "declined: -0.16431278597564888\n",
      "loss: -0.1625677859276815\n",
      "Positive Frequency: -0.15862693624421065\n",
      "downturn: -0.15276014016793882\n",
      "Word Count: -0.1521358756341728\n",
      "opportunity: -0.14766561147827076\n",
      "diminished: -0.14758104526248308\n",
      "Durable Goods Orders_RollingMean: -0.14254720840776883\n",
      "Negative Frequency: -0.1408115620482821\n",
      "Standardized Sentiment Score: -0.13923571684055974\n",
      "Net Sentiment Score: -0.13923571684055966\n",
      "stabilization: -0.13429563188930918\n",
      "dropped: -0.13209792085869296\n",
      "attractive: -0.12488392837832442\n",
      "weak: -0.1235805195518795\n",
      "improved: -0.11824787302225447\n",
      "stronger: -0.11804853320855771\n",
      "sluggish: -0.10763760034456571\n",
      "concern: -0.10661501374409063\n",
      "dissented: -0.10493345905298644\n",
      "sharply: -0.10409936354178348\n",
      "Retail Sales_RollingMean: -0.09338575162180898\n",
      "liquidation: -0.08512100750803456\n",
      "negative: -0.08225341050751975\n",
      "Retail Sales: -0.08028482780507562\n",
      "decline: 0.07981769725469089\n",
      "Durable Goods Orders: -0.07914702638121508\n",
      "CEI: 0.07653897927669584\n",
      "CEI_RollingMean: 0.0732532093484893\n",
      "LEI_RollingMean: -0.06747882619598905\n",
      "downward: -0.059222003375854634\n",
      "deterioration: -0.05716085001708827\n",
      "persist: -0.051988701242872914\n",
      "rebound: 0.03715692164709187\n",
      "correction: -0.021745016172559382\n",
      "adverse: 0.02036402590006283\n",
      "weakening: 0.00444221578976373\n"
     ]
    }
   ],
   "source": [
    "# Subset numerical columns\n",
    "numerical_columns = second_dataset.select_dtypes(include='number')\n",
    "\n",
    "# Remove 'Difference', 'Increase', and 'Decrease' from numerical columns\n",
    "columns_to_exclude = ['Difference', 'Increase', 'Decrease']\n",
    "numerical_columns_subset = numerical_columns.drop(columns_to_exclude, axis=1)\n",
    "\n",
    "# Fill missing values with the mean in numerical columns\n",
    "numerical_columns_subset.fillna(numerical_columns_subset.mean(), inplace=True)\n",
    "\n",
    "# Fill missing values with the mean in 'Difference'\n",
    "difference_mean = second_dataset['Difference'].mean()\n",
    "second_dataset['Difference'].fillna(difference_mean, inplace=True)\n",
    "\n",
    "# Define the number of features to select\n",
    "k = 82\n",
    "\n",
    "# Perform univariate selection using f_regression\n",
    "selector = SelectKBest(score_func=f_regression, k=k)\n",
    "selected_features = selector.fit_transform(numerical_columns_subset, second_dataset['Difference'])\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_features_names = numerical_columns_subset.columns[selected_indices]\n",
    "\n",
    "# Get the feature scores\n",
    "feature_scores = selector.scores_[selected_indices]\n",
    "\n",
    "# Standardize the feature scores\n",
    "scaler = StandardScaler()\n",
    "standardized_scores = scaler.fit_transform(feature_scores.reshape(-1, 1))\n",
    "\n",
    "# Combine selected feature names and their standardized scores\n",
    "selected_features_with_scores = list(zip(selected_features_names, standardized_scores))\n",
    "\n",
    "# Sort the selected features by the absolute value of standardized scores in descending order\n",
    "selected_features_with_scores.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Print the selected features and their standardized relevance scores in descending order\n",
    "print(\"Selected Features and Absolute Standardized Relevance Scores (Descending Order):\")\n",
    "for feature, score in selected_features_with_scores:\n",
    "    print(f\"{feature}: {score[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "711daf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features with importance greater than the absolute value of 0.2\n",
    "selected_features_greater_than_0_2 = [feature for feature, score in selected_features_with_scores if abs(score[0]) > 0.2]\n",
    "\n",
    "# Include the 'Difference' variable in the selected features\n",
    "selected_features_greater_than_0_2.append('Difference')\n",
    "\n",
    "# Create a subset of 'second_dataset' with the specified variables\n",
    "subset_second_dataset = second_dataset[selected_features_greater_than_0_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "322ca083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Short-Term Treasury Diff</th>\n",
       "      <th>LAG_RollingMean</th>\n",
       "      <th>easing</th>\n",
       "      <th>LAG</th>\n",
       "      <th>weaker</th>\n",
       "      <th>weakness</th>\n",
       "      <th>Nonfarm Payroll</th>\n",
       "      <th>Long-Term Treasury Bond Rate</th>\n",
       "      <th>desired</th>\n",
       "      <th>boost</th>\n",
       "      <th>...</th>\n",
       "      <th>deficit</th>\n",
       "      <th>Date</th>\n",
       "      <th>volatility</th>\n",
       "      <th>weakened</th>\n",
       "      <th>strengthening</th>\n",
       "      <th>caution</th>\n",
       "      <th>good</th>\n",
       "      <th>severe</th>\n",
       "      <th>lagged</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.384181</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469678</td>\n",
       "      <td>0</td>\n",
       "      <td>-5</td>\n",
       "      <td>110570.0</td>\n",
       "      <td>6.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.094047</td>\n",
       "      <td>-3</td>\n",
       "      <td>-0.028876</td>\n",
       "      <td>-2</td>\n",
       "      <td>-4</td>\n",
       "      <td>111060.0</td>\n",
       "      <td>5.81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-5</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.180436</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100506</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>111209.0</td>\n",
       "      <td>5.68</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-5</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.581703</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.673479</td>\n",
       "      <td>0</td>\n",
       "      <td>-4</td>\n",
       "      <td>111455.0</td>\n",
       "      <td>5.36</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.171385</td>\n",
       "      <td>0</td>\n",
       "      <td>1.740168</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>111989.0</td>\n",
       "      <td>5.72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-3</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.89</td>\n",
       "      <td>-1.181303</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2.092038</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>154006.0</td>\n",
       "      <td>3.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>10760</td>\n",
       "      <td>-12</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.20</td>\n",
       "      <td>-2.196739</td>\n",
       "      <td>-6</td>\n",
       "      <td>-3.592363</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>154535.0</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>10802</td>\n",
       "      <td>-5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.38</td>\n",
       "      <td>-2.272438</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1.132914</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1</td>\n",
       "      <td>155007.0</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10851</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>-0.21</td>\n",
       "      <td>-2.666227</td>\n",
       "      <td>-5</td>\n",
       "      <td>-3.273404</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>155472.0</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>10900</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.59</td>\n",
       "      <td>-2.171107</td>\n",
       "      <td>-7</td>\n",
       "      <td>-2.107003</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>155689.0</td>\n",
       "      <td>3.46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10942</td>\n",
       "      <td>-7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Short-Term Treasury Diff  LAG_RollingMean  easing       LAG  weaker  \\\n",
       "0                       -0.02         0.384181       0  0.469678       0   \n",
       "1                        0.02        -0.094047      -3 -0.028876      -2   \n",
       "2                        0.06         0.180436       0  0.100506       0   \n",
       "3                       -0.06         0.581703      -1  1.673479       0   \n",
       "4                        0.00         1.171385       0  1.740168      -1   \n",
       "..                        ...              ...     ...       ...     ...   \n",
       "235                      0.89        -1.181303      -2 -2.092038      -4   \n",
       "236                      0.20        -2.196739      -6 -3.592363      -2   \n",
       "237                      0.38        -2.272438      -3 -1.132914      -3   \n",
       "238                     -0.21        -2.666227      -5 -3.273404       0   \n",
       "239                      0.59        -2.171107      -7 -2.107003       0   \n",
       "\n",
       "     weakness  Nonfarm Payroll  Long-Term Treasury Bond Rate  desired  boost  \\\n",
       "0          -5         110570.0                          6.04        0      0   \n",
       "1          -4         111060.0                          5.81        0      0   \n",
       "2          -1         111209.0                          5.68        0      1   \n",
       "3          -4         111455.0                          5.36        0      1   \n",
       "4          -1         111989.0                          5.72        0      0   \n",
       "..        ...              ...                           ...      ...    ...   \n",
       "235         0         154006.0                          3.98        0      0   \n",
       "236         0         154535.0                          3.62        0      0   \n",
       "237        -1         155007.0                          3.53        0      2   \n",
       "238         0         155472.0                          3.66        0      0   \n",
       "239        -1         155689.0                          3.46        0      0   \n",
       "\n",
       "     ...  deficit   Date  volatility  weakened  strengthening  caution  good  \\\n",
       "0    ...       -9      0           0        -1              2       -2    11   \n",
       "1    ...       -5     50           0         0              0        0     9   \n",
       "2    ...       -5     91           0         0              3       -1     5   \n",
       "3    ...       -2    126           0         0              2       -1    10   \n",
       "4    ...       -3    182           0         0              6        0     7   \n",
       "..   ...      ...    ...         ...       ...            ...      ...   ...   \n",
       "235  ...       -1  10760         -12        -1              0        0     3   \n",
       "236  ...       -1  10802          -5         0              0       -1     9   \n",
       "237  ...        0  10851          -2         0              0        0     5   \n",
       "238  ...       -2  10900          -4         0              0       -2     7   \n",
       "239  ...        0  10942          -7         0              0       -1     7   \n",
       "\n",
       "     severe  lagged  Difference  \n",
       "0         0       0        0.00  \n",
       "1         0       0       -0.50  \n",
       "2        -1       0        0.00  \n",
       "3         0       0        0.00  \n",
       "4         0       0        0.00  \n",
       "..      ...     ...         ...  \n",
       "235       0       0        0.75  \n",
       "236      -1      -1        0.50  \n",
       "237      -1      -1        0.25  \n",
       "238      -1       0        0.25  \n",
       "239       0      -1        0.25  \n",
       "\n",
       "[240 rows x 23 columns]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_second_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "322f9108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 192\n",
      "Validation set size: 24\n",
      "Test set size: 24\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation, and test sets\n",
    "train, valid = train_test_split(subset_second_dataset, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Split the combined set into validation and test sets\n",
    "valid, test = train_test_split(valid, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Verify the sizes of each set\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Validation set size:\", len(valid))\n",
    "print(\"Test set size:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "9b13b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_valid = valid.drop(columns=['Difference'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_test = test.drop(columns=['Difference'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "b1d74378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Featurees Used: 20\n",
      "Random Seed Used: 2823020463\n"
     ]
    }
   ],
   "source": [
    "# List to store random seeds used in each iteration\n",
    "random_seeds = []\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Function to calculate accuracy based on a threshold\n",
    "def calculate_accuracy(y_true, y_pred, threshold):\n",
    "    num_samples = len(y_true)\n",
    "    correct_predictions = sum(abs(y_true - y_pred) <= threshold)\n",
    "    return correct_predictions / num_samples\n",
    "\n",
    "best_accuracy = -1.0\n",
    "optimal_cv = None\n",
    "best_y_train_pred = None\n",
    "best_y_valid_pred = None\n",
    "threshold = 0.1  # Define your desired threshold here\n",
    "\n",
    "for cv in range(2, 11):  # Try cross-validation folds from 2 to 10\n",
    "    grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=cv, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model with optimal hyperparameters\n",
    "    subset_second_rf_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predict y_train_pred on the training set\n",
    "    y_train_pred = subset_second_rf_model.predict(X_train)\n",
    "\n",
    "    # Predict y_valid_pred on the validation set\n",
    "    y_valid_pred = subset_second_rf_model.predict(X_valid)\n",
    "\n",
    "    # Round the predicted values to the nearest possible value\n",
    "    y_train_pred = [round_to_nearest(val, possible_values) for val in y_train_pred]\n",
    "    y_valid_pred = [round_to_nearest(val, possible_values) for val in y_valid_pred]\n",
    "\n",
    "    # Calculate accuracy for training and validation sets after rounding\n",
    "    accuracy_train = calculate_accuracy(y_train, y_train_pred, threshold)\n",
    "    accuracy_valid = calculate_accuracy(y_valid, y_valid_pred, threshold)\n",
    "    \n",
    "    # Check if the accuracy after rounding is higher than the best accuracy so far\n",
    "    if accuracy_valid > best_accuracy:\n",
    "        best_accuracy = accuracy_valid\n",
    "        optimal_cv = cv\n",
    "        best_y_train_pred = y_train_pred\n",
    "        best_y_valid_pred = y_valid_pred\n",
    "\n",
    "    # Store the random seed used in this iteration\n",
    "    random_seeds.append(np.random.get_state()[1][0])\n",
    "\n",
    "# Use the optimal number of folds in GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=optimal_cv, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with optimal hyperparameters\n",
    "subset_second_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Get the feature importances from the best model\n",
    "feature_importances = subset_second_rf_model.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "# Print the number of features used in the last iteration\n",
    "print(\"Number of Featurees Used:\", num_features_used)\n",
    "\n",
    "# Extract the random seed from the last element of the random_seeds list\n",
    "random_seed_used = random_seeds[-1]\n",
    "\n",
    "# Print the random seed used in the last iteration\n",
    "print(\"Random Seed Used:\", random_seed_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "57a0541e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------------+\n",
      "| Metric       |   Training Set |   Validation Set |\n",
      "+==============+================+==================+\n",
      "| MSE          |     0.00260417 |        0.0260417 |\n",
      "+--------------+----------------+------------------+\n",
      "| RMSE         |     0.051031   |        0.161374  |\n",
      "+--------------+----------------+------------------+\n",
      "| R^2          |     0.934957   |        0.599332  |\n",
      "+--------------+----------------+------------------+\n",
      "| Adjusted R^2 |     0.926489   |       -8.21536   |\n",
      "+--------------+----------------+------------------+\n",
      "| Accuracy     |     0.958333   |        0.708333  |\n",
      "+--------------+----------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(y_true, y_pred, threshold=None):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(y_true)\n",
    "    \n",
    "    if threshold is not None:\n",
    "        for true_val, pred_val in zip(y_true, y_pred):\n",
    "            if abs(true_val - pred_val) <= threshold:\n",
    "                correct_predictions += 1\n",
    "    else:\n",
    "        for true_val, pred_val in zip(y_true, y_pred):\n",
    "            if true_val == pred_val:\n",
    "                correct_predictions += 1\n",
    "            \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, best_y_train_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, best_y_valid_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, best_y_train_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, best_y_valid_pred)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]\n",
    "p_train = X_train.shape[1]\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]\n",
    "p_valid = X_valid.shape[1]\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "\n",
    "# Calculate accuracy for training and validation sets with the threshold\n",
    "accuracy_train = calculate_accuracy(y_train, best_y_train_pred, threshold)\n",
    "accuracy_valid = calculate_accuracy(y_valid, best_y_valid_pred, threshold)\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = [\n",
    "    [\"MSE\", mse_train, mse_valid],\n",
    "    [\"RMSE\", rmse_train, rmse_valid],\n",
    "    [\"R^2\", r2_train, r2_valid],\n",
    "    [\"Adjusted R^2\", adj_r2_train, adj_r2_valid],\n",
    "    [\"Accuracy\", accuracy_train, accuracy_valid],\n",
    "]\n",
    "\n",
    "# Prepare the headers for the table\n",
    "headers = [\"Metric\", \"Training Set\", \"Validation Set\"]\n",
    "\n",
    "# Display the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "f8122987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Short-Term Treasury Diff  LAG_RollingMean       LAG  Nonfarm Payroll  \\\n",
      "0                     -0.02         0.384181  0.469678         110570.0   \n",
      "1                      0.02        -0.094047 -0.028876         111060.0   \n",
      "2                      0.06         0.180436  0.100506         111209.0   \n",
      "3                     -0.06         0.581703  1.673479         111455.0   \n",
      "4                      0.00         1.171385  1.740168         111989.0   \n",
      "\n",
      "   Long-Term Treasury Bond Rate  Level  Date  Difference  \\\n",
      "0                          6.04   3.75     0         0.0   \n",
      "1                          5.81   3.25    50        -0.5   \n",
      "2                          5.68   3.25    91         0.0   \n",
      "3                          5.36   3.25   126         0.0   \n",
      "4                          5.72   3.25   182         0.0   \n",
      "\n",
      "   easing_x_Short-Term_Treasury_Diff  easing_x_LAG_RollingMean  ...  \\\n",
      "0                              -0.00                  0.000000  ...   \n",
      "1                              -0.06                  0.282142  ...   \n",
      "2                               0.00                  0.000000  ...   \n",
      "3                               0.06                 -0.581703  ...   \n",
      "4                               0.00                  0.000000  ...   \n",
      "\n",
      "   severe_x_Short-Term_Treasury_Diff  severe_x_LAG_RollingMean  severe_x_LAG  \\\n",
      "0                              -0.00                  0.000000      0.000000   \n",
      "1                               0.00                 -0.000000     -0.000000   \n",
      "2                              -0.06                 -0.180436     -0.100506   \n",
      "3                              -0.00                  0.000000      0.000000   \n",
      "4                               0.00                  0.000000      0.000000   \n",
      "\n",
      "   severe_x_Nonfarm_Payroll  severe_x_Level  \\\n",
      "0                       0.0            0.00   \n",
      "1                       0.0            0.00   \n",
      "2                 -111209.0           -3.25   \n",
      "3                       0.0            0.00   \n",
      "4                       0.0            0.00   \n",
      "\n",
      "   lagged_x_Short-Term_Treasury_Diff  lagged_x_LAG_RollingMean  lagged_x_LAG  \\\n",
      "0                               -0.0                       0.0           0.0   \n",
      "1                                0.0                      -0.0          -0.0   \n",
      "2                                0.0                       0.0           0.0   \n",
      "3                               -0.0                       0.0           0.0   \n",
      "4                                0.0                       0.0           0.0   \n",
      "\n",
      "   lagged_x_Nonfarm_Payroll  lagged_x_Level  \n",
      "0                       0.0             0.0  \n",
      "1                       0.0             0.0  \n",
      "2                       0.0             0.0  \n",
      "3                       0.0             0.0  \n",
      "4                       0.0             0.0  \n",
      "\n",
      "[5 rows x 83 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a third subset incorporating designated variables from subset_second_dataset\n",
    "third_subset = subset_second_dataset[['Short-Term Treasury Diff', 'LAG_RollingMean', 'LAG', 'Nonfarm Payroll', 'Long-Term Treasury Bond Rate', 'Level', 'Date', 'Difference']].copy()\n",
    "\n",
    "# List of vectorized terms (previously sentiment columns)\n",
    "vectorized_terms = [col for col in subset_second_dataset.columns if col not in ['Short-Term Treasury Diff', 'LAG_RollingMean', 'LAG', 'Nonfarm Payroll', 'Long-Term Treasury Bond Rate', 'Level', 'Date', 'Difference']]\n",
    "\n",
    "# Introduce new variables by computing the product of vectorized terms with specified columns\n",
    "for term in vectorized_terms:\n",
    "    for column in ['Short-Term Treasury Diff', 'LAG_RollingMean', 'LAG', 'Nonfarm Payroll', 'Level']:\n",
    "        new_column_name = f'{term}_x_{column.replace(\" \", \"_\")}'\n",
    "        if term in subset_second_dataset.columns:\n",
    "            third_subset[new_column_name] = subset_second_dataset[term] * subset_second_dataset[column]\n",
    "\n",
    "# Append the 'Date' variable from subset_second_dataset at this stage\n",
    "third_subset['Date'] = subset_second_dataset['Date']\n",
    "\n",
    "# Display initial records of the third subset\n",
    "print(third_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "a56e9248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 192\n",
      "Validation set size: 24\n",
      "Test set size: 24\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation, and test sets\n",
    "train, valid = train_test_split(third_subset, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Split the combined set into validation and test sets\n",
    "valid, test = train_test_split(valid, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Verify the sizes of each set\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Validation set size:\", len(valid))\n",
    "print(\"Test set size:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "da9cc90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_valid = valid.drop(columns=['Difference'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_test = test.drop(columns=['Difference'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "9d52a163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features Used: 33\n",
      "Random Seed Used: 2823020463\n"
     ]
    }
   ],
   "source": [
    "# List to store random seeds used in each iteration\n",
    "random_seeds = []\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Function to calculate accuracy based on a threshold\n",
    "def calculate_accuracy(y_true, y_pred, threshold):\n",
    "    num_samples = len(y_true)\n",
    "    correct_predictions = sum(abs(y_true - y_pred) <= threshold)\n",
    "    return correct_predictions / num_samples\n",
    "\n",
    "best_accuracy = -1.0\n",
    "optimal_cv = None\n",
    "best_y_train_pred = None\n",
    "best_y_valid_pred = None\n",
    "threshold = 0.1  # Define your desired threshold here\n",
    "\n",
    "for cv in range(2, 11):  # Try cross-validation folds from 2 to 10\n",
    "    grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=cv, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model with optimal hyperparameters\n",
    "    subset_third_rf_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predict y_train_pred on the training set\n",
    "    y_train_pred = subset_third_rf_model.predict(X_train)\n",
    "\n",
    "    # Predict y_valid_pred on the validation set\n",
    "    y_valid_pred = subset_third_rf_model.predict(X_valid)\n",
    "\n",
    "    # Round the predicted values to the nearest possible value\n",
    "    y_train_pred = [round_to_nearest(val, possible_values) for val in y_train_pred]\n",
    "    y_valid_pred = [round_to_nearest(val, possible_values) for val in y_valid_pred]\n",
    "\n",
    "    # Calculate accuracy for training and validation sets after rounding\n",
    "    accuracy_train = calculate_accuracy(y_train, y_train_pred, threshold)\n",
    "    accuracy_valid = calculate_accuracy(y_valid, y_valid_pred, threshold)\n",
    "    \n",
    "    # Check if the accuracy after rounding is higher than the best accuracy so far\n",
    "    if accuracy_valid > best_accuracy:\n",
    "        best_accuracy = accuracy_valid\n",
    "        optimal_cv = cv\n",
    "        best_y_train_pred = y_train_pred\n",
    "        best_y_valid_pred = y_valid_pred\n",
    "\n",
    "    # Store the random seed used in this iteration\n",
    "    random_seeds.append(np.random.get_state()[1][0])\n",
    "\n",
    "# Use the optimal number of folds in GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=optimal_cv, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with optimal hyperparameters\n",
    "subset_third_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Get the feature importances from the best model\n",
    "feature_importances = subset_third_rf_model.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "# Print the number of features used in the last iteration\n",
    "print(\"Number of Features Used:\", num_features_used)\n",
    "\n",
    "# Extract the random seed from the last element of the random_seeds list\n",
    "random_seed_used = random_seeds[-1]\n",
    "\n",
    "# Print the random seed used in the last iteration\n",
    "print(\"Random Seed Used:\", random_seed_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "99ab4f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------------+\n",
      "| Metric       |   Training Set |   Validation Set |\n",
      "+==============+================+==================+\n",
      "| MSE          |     0.00846354 |        0.0390625 |\n",
      "+--------------+----------------+------------------+\n",
      "| RMSE         |     0.0919975  |        0.197642  |\n",
      "+--------------+----------------+------------------+\n",
      "| R^2          |     0.788609   |        0.398998  |\n",
      "+--------------+----------------+------------------+\n",
      "| Adjusted R^2 |     0.629581   |        1.23429   |\n",
      "+--------------+----------------+------------------+\n",
      "| Accuracy     |     0.880208   |        0.708333  |\n",
      "+--------------+----------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(y_true, y_pred, threshold=None):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(y_true)\n",
    "    \n",
    "    if threshold is not None:\n",
    "        for true_val, pred_val in zip(y_true, y_pred):\n",
    "            if abs(true_val - pred_val) <= threshold:\n",
    "                correct_predictions += 1\n",
    "    else:\n",
    "        for true_val, pred_val in zip(y_true, y_pred):\n",
    "            if true_val == pred_val:\n",
    "                correct_predictions += 1\n",
    "            \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, best_y_train_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, best_y_valid_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, best_y_train_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, best_y_valid_pred)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]\n",
    "p_train = X_train.shape[1]\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]\n",
    "p_valid = X_valid.shape[1]\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "\n",
    "# Calculate accuracy for training and validation sets with the threshold\n",
    "accuracy_train = calculate_accuracy(y_train, best_y_train_pred, threshold)\n",
    "accuracy_valid = calculate_accuracy(y_valid, best_y_valid_pred, threshold)\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = [\n",
    "    [\"MSE\", mse_train, mse_valid],\n",
    "    [\"RMSE\", rmse_train, rmse_valid],\n",
    "    [\"R^2\", r2_train, r2_valid],\n",
    "    [\"Adjusted R^2\", adj_r2_train, adj_r2_valid],\n",
    "    [\"Accuracy\", accuracy_train, accuracy_valid],\n",
    "]\n",
    "\n",
    "# Prepare the headers for the table\n",
    "headers = [\"Metric\", \"Training Set\", \"Validation Set\"]\n",
    "\n",
    "# Display the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "0dffcd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "Short-Term Treasury Diff\n",
      "LAG_RollingMean\n",
      "LAG\n",
      "Nonfarm Payroll\n",
      "Long-Term Treasury Bond Rate\n",
      "Level\n",
      "Date\n",
      "easing_x_Short-Term_Treasury_Diff\n",
      "easing_x_LAG_RollingMean\n",
      "easing_x_LAG\n",
      "easing_x_Nonfarm_Payroll\n",
      "weaker_x_Short-Term_Treasury_Diff\n",
      "weaker_x_LAG\n",
      "weakness_x_Level\n",
      "desired_x_Level\n",
      "boost_x_Short-Term_Treasury_Diff\n",
      "boost_x_Level\n",
      "slowly_x_Short-Term_Treasury_Diff\n",
      "problem_x_Nonfarm_Payroll\n",
      "deficit_x_LAG_RollingMean\n",
      "deficit_x_LAG\n",
      "deficit_x_Nonfarm_Payroll\n",
      "volatility_x_Level\n",
      "weakened_x_LAG\n",
      "strengthening_x_LAG_RollingMean\n",
      "good_x_LAG\n",
      "good_x_Nonfarm_Payroll\n",
      "severe_x_LAG\n",
      "lagged_x_Short-Term_Treasury_Diff\n",
      "lagged_x_LAG_RollingMean\n",
      "lagged_x_LAG\n",
      "lagged_x_Nonfarm_Payroll\n",
      "lagged_x_Level\n"
     ]
    }
   ],
   "source": [
    "# Get the feature importances from the best model\n",
    "feature_importances = subset_third_rf_model.feature_importances_\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_indices = np.where(feature_importances > 0)[0]\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_features_names = X_train.columns[selected_indices]\n",
    "\n",
    "# Print the selected feature names\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features_names:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "2f7d0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features with importance greater than the absolute value of 0.2\n",
    "selected_features_greater_than_0_2 = [feature for feature, score in selected_features_with_scores if abs(score[0]) > 0.2]\n",
    "\n",
    "# Include the 'Difference' variable in the selected features\n",
    "selected_features_greater_than_0_2.append('Difference')\n",
    "\n",
    "# Create a subset of 'second_dataset' with the specified variables\n",
    "subset_second_dataset = second_dataset[selected_features_greater_than_0_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "9899e88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 192\n",
      "Validation set size: 24\n",
      "Test set size: 24\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation, and test sets\n",
    "train, valid = train_test_split(subset_second_dataset, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Split the combined set into validation and test sets\n",
    "valid, test = train_test_split(valid, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Verify the sizes of each set\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Validation set size:\", len(valid))\n",
    "print(\"Test set size:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "0a894035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_valid = valid.drop(columns=['Difference'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_test = test.drop(columns=['Difference'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "53fc25c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features Used: 21\n",
      "Random Seed Used: 2823020463\n"
     ]
    }
   ],
   "source": [
    "# List to store random seeds used in each iteration\n",
    "random_seeds = []\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Function to calculate accuracy based on a threshold\n",
    "def calculate_accuracy(y_true, y_pred, threshold):\n",
    "    num_samples = len(y_true)\n",
    "    correct_predictions = sum(abs(y_true - y_pred) <= threshold)\n",
    "    return correct_predictions / num_samples\n",
    "\n",
    "best_accuracy = -1.0\n",
    "optimal_cv = None\n",
    "best_y_train_pred = None\n",
    "best_y_test_pred = None  # Changed variable name from best_y_valid_pred\n",
    "threshold = 0.1  # Define your desired threshold here\n",
    "\n",
    "for cv in range(2, 11):  # Try cross-validation folds from 2 to 10\n",
    "    grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=cv, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model with optimal hyperparameters\n",
    "    subset_third_rf_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predict y_train_pred on the training set\n",
    "    y_train_pred = subset_third_rf_model.predict(X_train)\n",
    "\n",
    "    # Predict y_test_pred on the test set  # Changed variable name from y_valid_pred\n",
    "    y_test_pred = subset_third_rf_model.predict(X_test)  # Use 'test' set instead of 'valid'\n",
    "\n",
    "    # Round the predicted values to the nearest possible value\n",
    "    y_train_pred = [round_to_nearest(val, possible_values) for val in y_train_pred]\n",
    "    y_test_pred = [round_to_nearest(val, possible_values) for val in y_test_pred]\n",
    "\n",
    "    # Calculate accuracy for training and test sets after rounding\n",
    "    accuracy_train = calculate_accuracy(y_train, y_train_pred, threshold)\n",
    "    accuracy_test = calculate_accuracy(y_test, y_test_pred, threshold)  # Use 'test' set instead of 'valid'\n",
    "    \n",
    "    # Check if the accuracy after rounding is higher than the best accuracy so far\n",
    "    if accuracy_test > best_accuracy:  # Change from accuracy_valid to accuracy_test\n",
    "        best_accuracy = accuracy_test  # Change from accuracy_valid to accuracy_test\n",
    "        optimal_cv = cv\n",
    "        best_y_train_pred = y_train_pred\n",
    "        best_y_test_pred = y_test_pred  # Changed variable name from best_y_valid_pred\n",
    "\n",
    "    # Store the random seed used in this iteration\n",
    "    random_seeds.append(np.random.get_state()[1][0])\n",
    "\n",
    "# Use the optimal number of folds in GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=optimal_cv, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with optimal hyperparameters\n",
    "subset_third_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Get the feature importances from the best model\n",
    "feature_importances = subset_third_rf_model.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "# Print the number of features used in the last iteration\n",
    "print(\"Number of Features Used:\", num_features_used)\n",
    "\n",
    "# Extract the random seed from the last element of the random_seeds list\n",
    "random_seed_used = random_seeds[-1]\n",
    "\n",
    "# Print the random seed used in the last iteration\n",
    "print(\"Random Seed Used:\", random_seed_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "688f4867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------------+-------------+\n",
      "| Metric       |   Training Set |   Validation Set |    Test Set |\n",
      "+==============+================+==================+=============+\n",
      "| MSE          |     0.00716146 |        0.0390625 |   0.0572917 |\n",
      "+--------------+----------------+------------------+-------------+\n",
      "| RMSE         |     0.0846254  |        0.197642  |   0.239357  |\n",
      "+--------------+----------------+------------------+-------------+\n",
      "| R^2          |     0.821131   |        0.398998  |   0.297872  |\n",
      "+--------------+----------------+------------------+-------------+\n",
      "| Adjusted R^2 |     0.797846   |      -12.823     | -15.1489    |\n",
      "+--------------+----------------+------------------+-------------+\n",
      "| Accuracy     |     0.885417   |        0.708333  |   0.708333  |\n",
      "+--------------+----------------+------------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(y_true, y_pred, threshold=None):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(y_true)\n",
    "    \n",
    "    if threshold is not None:\n",
    "        for true_val, pred_val in zip(y_true, y_pred):\n",
    "            if abs(true_val - pred_val) <= threshold:\n",
    "                correct_predictions += 1\n",
    "    else:\n",
    "        for true_val, pred_val in zip(y_true, y_pred):\n",
    "            if true_val == pred_val:\n",
    "                correct_predictions += 1\n",
    "            \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, best_y_train_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, best_y_valid_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for test set\n",
    "mse_test = mean_squared_error(y_test, best_y_test_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for test set\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, best_y_train_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, best_y_valid_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for test set\n",
    "r2_test = r2_score(y_test, best_y_test_pred)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]\n",
    "p_train = X_train.shape[1]\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]\n",
    "p_valid = X_valid.shape[1]\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for test set\n",
    "n_test = X_test.shape[0]\n",
    "p_test = X_test.shape[1]\n",
    "adj_r2_test = 1 - ((1 - r2_test) * (n_test - 1) / (n_test - p_test - 1))\n",
    "\n",
    "# Calculate accuracy for training, validation, and test sets with the threshold\n",
    "accuracy_train = calculate_accuracy(y_train, best_y_train_pred, threshold)\n",
    "accuracy_valid = calculate_accuracy(y_valid, best_y_valid_pred, threshold)\n",
    "accuracy_test = calculate_accuracy(y_test, best_y_test_pred, threshold)\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = [\n",
    "    [\"MSE\", mse_train, mse_valid, mse_test],\n",
    "    [\"RMSE\", rmse_train, rmse_valid, rmse_test],\n",
    "    [\"R^2\", r2_train, r2_valid, r2_test],\n",
    "    [\"Adjusted R^2\", adj_r2_train, adj_r2_valid, adj_r2_test],\n",
    "    [\"Accuracy\", accuracy_train, accuracy_valid, accuracy_test],\n",
    "]\n",
    "\n",
    "# Prepare the headers for the table\n",
    "headers = [\"Metric\", \"Training Set\", \"Validation Set\", \"Test Set\"]\n",
    "\n",
    "# Display the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "12802399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\abact\\\\BC-Project\\\\models\\\\best_random_forest_model_data.joblib']"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the best_rf_model to the specified location\n",
    "model_filename = r\"C:\\Users\\abact\\BC-Project\\models\\best_random_forest_model_data.joblib\"\n",
    "joblib.dump(subset_third_rf_model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "4fe91bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "Short-Term Treasury Diff\n",
      "LAG_RollingMean\n",
      "easing\n",
      "LAG\n",
      "weaker\n",
      "weakness\n",
      "Nonfarm Payroll\n",
      "Long-Term Treasury Bond Rate\n",
      "desired\n",
      "boost\n",
      "slowly\n",
      "problem\n",
      "Level\n",
      "deficit\n",
      "Date\n",
      "volatility\n",
      "weakened\n",
      "strengthening\n",
      "good\n",
      "severe\n",
      "lagged\n"
     ]
    }
   ],
   "source": [
    "# Get the feature importances from the best model\n",
    "feature_importances = subset_third_rf_model.feature_importances_\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_indices = np.where(feature_importances > 0)[0]\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_features_names = X_train.columns[selected_indices]\n",
    "\n",
    "# Print the selected feature names\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features_names:\n",
    "    print(feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
