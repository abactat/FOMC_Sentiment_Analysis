{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "87439644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import random\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "02d6f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and loaded into 'dataset_adjusted' successfully.\n",
      "Dataset downloaded and loaded into 'words' successfully.\n"
     ]
    }
   ],
   "source": [
    "# The URL of the raw dataset on GitHub\n",
    "url = \"https://raw.githubusercontent.com/abactat/BC-Project/main/data/dataset_adjusted.csv?token=GHSAT0AAAAAACC4ZCNKN5F6XR7HZA75QWTEZGQVYIA\"\n",
    "\n",
    "# Send an HTTP GET request to fetch the content of the raw dataset\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200 means success)\n",
    "if response.status_code == 200:\n",
    "    # Read the content as a pandas DataFrame and assign it to the 'train' variable\n",
    "    dataset_adjusted = pd.read_csv(StringIO(response.text))\n",
    "    print(\"Dataset downloaded and loaded into 'dataset_adjusted' successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download the dataset. Status code: {response.status_code}\")\n",
    "    \n",
    "# The URL of the raw dataset on GitHub\n",
    "url = \"https://raw.githubusercontent.com/abactat/BC-Project/main/data/raw/dataset_words.csv?token=GHSAT0AAAAAACC4ZCNKDB2KTQ7KVOHGVHMAZGQVX7A\"\n",
    "\n",
    "# Send an HTTP GET request to fetch the content of the raw dataset\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200 means success)\n",
    "if response.status_code == 200:\n",
    "    # Read the content as a pandas DataFrame and assign it to the 'valid' variable\n",
    "    words = pd.read_csv(StringIO(response.text))\n",
    "    print(\"Dataset downloaded and loaded into 'words' successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download the dataset. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ddb23e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentiment word list from the CSV file into a dictionary\n",
    "sentiment_word_list = {}\n",
    "with open(r\"C:\\Users\\abact\\BC-Project\\data\\external\\Loughran-McDonald_MasterDictionary_1993-2021.csv\", 'r') as file:\n",
    "    # Skip the header line\n",
    "    next(file)\n",
    "\n",
    "    for line in file:\n",
    "        values = line.strip().split(',')\n",
    "\n",
    "        # Extract the necessary values\n",
    "        word = values[0].lower()\n",
    "        positive = float(values[8])  # Positive column index is 8\n",
    "        negative = float(values[7])  # Negative column index is 7\n",
    "\n",
    "        # Assign the word as positive or negative based on the positive or negative values\n",
    "        if positive == 2009:\n",
    "            sentiment_word_list[word] = 1\n",
    "        elif negative == 2009:\n",
    "            sentiment_word_list[word] = -1\n",
    "\n",
    "# Convert the sentiment word list keys to lowercase\n",
    "selected_words = set(sentiment_word_list.keys())\n",
    "\n",
    "# Filter the 'words' DataFrame to include only columns that are present in both 'selected_words' and 'words'\n",
    "common_columns = selected_words.intersection(words.columns)\n",
    "subset_words = words[list(common_columns)].copy()\n",
    "\n",
    "# Multiply sentiment values to the vectorized text columns in the 'subset_words' DataFrame\n",
    "for column in subset_words.columns:\n",
    "    sentiment_value = sentiment_word_list.get(column, 0)\n",
    "    if sentiment_value == 1:\n",
    "        subset_words.loc[:, column] = subset_words[column] * 1  # Multiply by 1 for positive sentiment\n",
    "    elif sentiment_value == -1:\n",
    "        subset_words.loc[:, column] = subset_words[column] * -1  # Multiply by -1 for negative sentiment\n",
    "    else:\n",
    "        subset_words.loc[:, column] = subset_words[column] * 0  # Multiply by 0 for unknown sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0293b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the vectorized DataFrame with the original dataset\n",
    "full_dataset = pd.concat([dataset_adjusted, subset_words], axis=1)\n",
    "\n",
    "full_dataset['Date'] = pd.to_datetime(full_dataset['Date'])\n",
    "\n",
    "# Calculate the time difference in days from the first date\n",
    "full_dataset['Date'] = (full_dataset['Date'] - full_dataset['Date'].min()).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "47ce3363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federal_Reserve_Mins    0\n",
      "Preprocessed Text       0\n",
      "Date                    0\n",
      "Difference              0\n",
      "Increase                0\n",
      "                       ..\n",
      "enhance                 0\n",
      "unanticipated           0\n",
      "burdensome              0\n",
      "impressive              0\n",
      "lose                    0\n",
      "Length: 837, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for the number of missing values in 'train' DataFrame\n",
    "missing_values_count = full_dataset.isna().sum()\n",
    "\n",
    "# Print the count of missing values for each column\n",
    "print(missing_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8b441281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with Missing Values:\n"
     ]
    }
   ],
   "source": [
    "# Filter columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0]\n",
    "\n",
    "# Print columns with missing values and their counts\n",
    "print(\"Columns with Missing Values:\")\n",
    "for column, count in columns_with_missing_values.items():\n",
    "    print(f\"{column}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "90d18c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 192\n",
      "Validation set size: 24\n",
      "Test set size: 24\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation, and test sets\n",
    "train, valid = train_test_split(full_dataset, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Split the combined set into validation and test sets\n",
    "valid, test = train_test_split(valid, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Verify the sizes of each set\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Validation set size:\", len(valid))\n",
    "print(\"Test set size:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "589999b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert variables to numeric in the train dataset\n",
    "variables_to_convert = train.columns.drop('Date')\n",
    "train[variables_to_convert] = train[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert variables to numeric in the valid dataset\n",
    "valid[variables_to_convert] = valid[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert variables to numeric in the test dataset\n",
    "test[variables_to_convert] = test[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_test = test.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d8ec9e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(y_true)\n",
    "    \n",
    "    for true_val, pred_val in zip(y_true, y_pred):\n",
    "        if true_val == pred_val:\n",
    "            correct_predictions += 1\n",
    "            \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "possible_values = [-1.00, -0.75, -0.50, -0.25, 0.00, 0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "def round_to_nearest(value, possible_values):\n",
    "    return min(possible_values, key=lambda x: abs(x - value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "82a63db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in 'train' DataFrame\n",
    "missing_values_count = train.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'train'\n",
    "train = train.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "# Check for missing values in 'valid' DataFrame\n",
    "missing_values_count = valid.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'valid'\n",
    "valid = valid.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "# Check for missing values in 'test' DataFrame\n",
    "missing_values_count = test.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'test'\n",
    "test = test.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_test = test.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dce7e6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date             0\n",
      "Difference       0\n",
      "Increase         0\n",
      "Decrease         0\n",
      "Level            0\n",
      "                ..\n",
      "enhance          0\n",
      "unanticipated    0\n",
      "burdensome       0\n",
      "impressive       0\n",
      "lose             0\n",
      "Length: 833, dtype: int64\n",
      "Columns with Missing Values:\n"
     ]
    }
   ],
   "source": [
    "# Check for the number of missing values in 'train' DataFrame\n",
    "missing_values_count = train.isna().sum()\n",
    "\n",
    "# Print the count of missing values for each column\n",
    "print(missing_values_count)\n",
    "\n",
    "# Filter columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0]\n",
    "\n",
    "# Print columns with missing values and their counts\n",
    "print(\"Columns with Missing Values:\")\n",
    "for column, count in columns_with_missing_values.items():\n",
    "    print(f\"{column}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9652fc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Create GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data and find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with optimal hyperparameters\n",
    "best_rf_model_5 = grid_search.best_estimator_\n",
    "\n",
    "# Predict y_train_pred on the training set\n",
    "y_train_pred = best_rf_model_5.predict(X_train)\n",
    "\n",
    "# Predict y_valid_pred on the validation set\n",
    "y_valid_pred = best_rf_model_5.predict(X_valid)\n",
    "\n",
    "# Round the predicted values to the nearest possible value\n",
    "y_train_pred = [round_to_nearest(val, possible_values) for val in y_train_pred]\n",
    "y_valid_pred = [round_to_nearest(val, possible_values) for val in y_valid_pred]\n",
    "\n",
    "# Output the random seed\n",
    "print(\"Random seed:\", random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e085e54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the feature importances from the best model\n",
    "feature_importances = best_rf_model_5.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "num_features_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0e1ca52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------------+\n",
      "| Metric       |   Training Set |   Validation Set |\n",
      "+==============+================+==================+\n",
      "| MSE          |     0.00358073 |        0.0260417 |\n",
      "+--------------+----------------+------------------+\n",
      "| RMSE         |     0.0598392  |        0.161374  |\n",
      "+--------------+----------------+------------------+\n",
      "| R^2          |     0.910565   |        0.599332  |\n",
      "+--------------+----------------+------------------+\n",
      "| Adjusted R^2 |     1.02673    |        1.01142   |\n",
      "+--------------+----------------+------------------+\n",
      "| Accuracy     |     0.942708   |        0.708333  |\n",
      "+--------------+----------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]\n",
    "p_train = X_train.shape[1]\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]\n",
    "p_valid = X_valid.shape[1]\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "\n",
    "# Calculate accuracy for training and validation sets\n",
    "accuracy_train = calculate_accuracy(y_train, y_train_pred)\n",
    "accuracy_valid = calculate_accuracy(y_valid, y_valid_pred)\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = [\n",
    "    [\"MSE\", mse_train, mse_valid],\n",
    "    [\"RMSE\", rmse_train, rmse_valid],\n",
    "    [\"R^2\", r2_train, r2_valid],\n",
    "    [\"Adjusted R^2\", adj_r2_train, adj_r2_valid],\n",
    "    [\"Accuracy\", accuracy_train, accuracy_valid],\n",
    "]\n",
    "\n",
    "# Prepare the headers for the table\n",
    "headers = [\"Metric\", \"Training Set\", \"Validation Set\"]\n",
    "\n",
    "# Display the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dc088b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seeds: [2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# List to store random seeds used in each iteration\n",
    "random_seeds = []\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Function to calculate accuracy based on a threshold\n",
    "def calculate_accuracy(y_true, y_pred, threshold):\n",
    "    num_samples = len(y_true)\n",
    "    correct_predictions = sum(abs(y_true - y_pred) <= threshold)\n",
    "    return correct_predictions / num_samples\n",
    "\n",
    "best_accuracy = -1.0\n",
    "optimal_cv = None\n",
    "best_y_train_pred = None\n",
    "best_y_valid_pred = None\n",
    "threshold = 0.1  # Define your desired threshold here\n",
    "\n",
    "for cv in range(2, 11):  # Try cross-validation folds from 2 to 10\n",
    "    # Set the random seed for reproducibility\n",
    "    random_seed = cv  # Use cv as the random seed\n",
    "    random_seeds.append(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=cv, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model with optimal hyperparameters\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predict y_train_pred on the training set\n",
    "    y_train_pred = best_rf_model.predict(X_train)\n",
    "\n",
    "    # Predict y_valid_pred on the validation set\n",
    "    y_valid_pred = best_rf_model.predict(X_valid)\n",
    "\n",
    "    # Round the predicted values to the nearest possible value\n",
    "    y_train_pred = [round_to_nearest(val, possible_values) for val in y_train_pred]\n",
    "    y_valid_pred = [round_to_nearest(val, possible_values) for val in y_valid_pred]\n",
    "\n",
    "    # Calculate accuracy for training and validation sets after rounding\n",
    "    accuracy_train = calculate_accuracy(y_train, y_train_pred, threshold)\n",
    "    accuracy_valid = calculate_accuracy(y_valid, y_valid_pred, threshold)\n",
    "    \n",
    "    # Check if the accuracy after rounding is higher than the best accuracy so far\n",
    "    if accuracy_valid > best_accuracy:\n",
    "        best_accuracy = accuracy_valid\n",
    "        optimal_cv = cv\n",
    "        best_y_train_pred = y_train_pred\n",
    "        best_y_valid_pred = y_valid_pred\n",
    "\n",
    "# Use the optimal number of folds in GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=optimal_cv, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with optimal hyperparameters\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Get the feature importances from the best model\n",
    "feature_importances = best_rf_model.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "# Print the random seeds used in each iteration\n",
    "print(\"Random Seeds:\", random_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a1acfc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the feature importances from the best model\n",
    "feature_importances = best_rf_model.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "num_features_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7a520b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------------+\n",
      "| Metric       |   Training Set |   Validation Set |\n",
      "+==============+================+==================+\n",
      "| MSE          |     0.00455729 |         0.03125  |\n",
      "+--------------+----------------+------------------+\n",
      "| RMSE         |     0.0675077  |         0.176777 |\n",
      "+--------------+----------------+------------------+\n",
      "| R^2          |     0.886174   |         0.519199 |\n",
      "+--------------+----------------+------------------+\n",
      "| Adjusted R^2 |     1.00323    |         1.0016   |\n",
      "+--------------+----------------+------------------+\n",
      "| Accuracy     |     0.927083   |         0.625    |\n",
      "+--------------+----------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]\n",
    "p_train = X_train.shape[1]\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]\n",
    "p_valid = X_valid.shape[1]\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "\n",
    "threshold = 0.1\n",
    "accuracy_train = calculate_accuracy(y_train, y_train_pred, threshold)\n",
    "accuracy_valid = calculate_accuracy(y_valid, y_valid_pred, threshold)\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = [\n",
    "    [\"MSE\", mse_train, mse_valid],\n",
    "    [\"RMSE\", rmse_train, rmse_valid],\n",
    "    [\"R^2\", r2_train, r2_valid],\n",
    "    [\"Adjusted R^2\", adj_r2_train, adj_r2_valid],\n",
    "    [\"Accuracy\", accuracy_train, accuracy_valid],\n",
    "]\n",
    "\n",
    "# Prepare the headers for the table\n",
    "headers = [\"Metric\", \"Training Set\", \"Validation Set\"]\n",
    "\n",
    "# Display the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24980e4",
   "metadata": {},
   "source": [
    "###Data Centric AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3c565965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>divestiture</th>\n",
       "      <th>unfounded</th>\n",
       "      <th>caution</th>\n",
       "      <th>hampering</th>\n",
       "      <th>unnecessarily</th>\n",
       "      <th>shutdown</th>\n",
       "      <th>ineffective</th>\n",
       "      <th>trouble</th>\n",
       "      <th>conspired</th>\n",
       "      <th>enable</th>\n",
       "      <th>...</th>\n",
       "      <th>disrupt</th>\n",
       "      <th>worsened</th>\n",
       "      <th>downturn</th>\n",
       "      <th>worsen</th>\n",
       "      <th>mistaken</th>\n",
       "      <th>enhance</th>\n",
       "      <th>unanticipated</th>\n",
       "      <th>burdensome</th>\n",
       "      <th>impressive</th>\n",
       "      <th>lose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.00000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.00000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.004167</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>-3.975000</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>-0.154167</td>\n",
       "      <td>-0.97500</td>\n",
       "      <td>-0.020833</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>-0.570833</td>\n",
       "      <td>-3.354167</td>\n",
       "      <td>-0.941667</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>1.90000</td>\n",
       "      <td>-2.700000</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>-0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.064550</td>\n",
       "      <td>0.064550</td>\n",
       "      <td>14.327878</td>\n",
       "      <td>0.064550</td>\n",
       "      <td>2.324416</td>\n",
       "      <td>9.95888</td>\n",
       "      <td>0.143125</td>\n",
       "      <td>0.156451</td>\n",
       "      <td>0.064550</td>\n",
       "      <td>2.726887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156451</td>\n",
       "      <td>3.247655</td>\n",
       "      <td>13.131918</td>\n",
       "      <td>5.056659</td>\n",
       "      <td>0.111335</td>\n",
       "      <td>7.92739</td>\n",
       "      <td>12.590493</td>\n",
       "      <td>0.064550</td>\n",
       "      <td>5.355533</td>\n",
       "      <td>0.213516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-141.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-36.000000</td>\n",
       "      <td>-152.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-37.000000</td>\n",
       "      <td>-141.000000</td>\n",
       "      <td>-53.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-147.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 799 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       divestiture   unfounded     caution   hampering  unnecessarily  \\\n",
       "count   240.000000  240.000000  240.000000  240.000000     240.000000   \n",
       "mean     -0.004167   -0.004167   -3.975000   -0.004167      -0.154167   \n",
       "std       0.064550    0.064550   14.327878    0.064550       2.324416   \n",
       "min      -1.000000   -1.000000 -141.000000   -1.000000     -36.000000   \n",
       "25%       0.000000    0.000000   -1.000000    0.000000       0.000000   \n",
       "50%       0.000000    0.000000    0.000000    0.000000       0.000000   \n",
       "75%       0.000000    0.000000    0.000000    0.000000       0.000000   \n",
       "max       0.000000    0.000000    0.000000    0.000000       0.000000   \n",
       "\n",
       "        shutdown  ineffective     trouble   conspired      enable  ...  \\\n",
       "count  240.00000   240.000000  240.000000  240.000000  240.000000  ...   \n",
       "mean    -0.97500    -0.020833   -0.025000   -0.004167    0.391667  ...   \n",
       "std      9.95888     0.143125    0.156451    0.064550    2.726887  ...   \n",
       "min   -152.00000    -1.000000   -1.000000   -1.000000    0.000000  ...   \n",
       "25%      0.00000     0.000000    0.000000    0.000000    0.000000  ...   \n",
       "50%      0.00000     0.000000    0.000000    0.000000    0.000000  ...   \n",
       "75%      0.00000     0.000000    0.000000    0.000000    0.000000  ...   \n",
       "max      0.00000     0.000000    0.000000    0.000000   27.000000  ...   \n",
       "\n",
       "          disrupt    worsened    downturn      worsen    mistaken    enhance  \\\n",
       "count  240.000000  240.000000  240.000000  240.000000  240.000000  240.00000   \n",
       "mean    -0.025000   -0.570833   -3.354167   -0.941667   -0.012500    1.90000   \n",
       "std      0.156451    3.247655   13.131918    5.056659    0.111335    7.92739   \n",
       "min     -1.000000  -37.000000 -141.000000  -53.000000   -1.000000    0.00000   \n",
       "25%      0.000000    0.000000   -1.000000    0.000000    0.000000    0.00000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.00000   \n",
       "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.00000   \n",
       "max      0.000000    0.000000    0.000000    0.000000    0.000000   66.00000   \n",
       "\n",
       "       unanticipated  burdensome  impressive        lose  \n",
       "count     240.000000  240.000000  240.000000  240.000000  \n",
       "mean       -2.700000   -0.004167    0.933333   -0.020833  \n",
       "std        12.590493    0.064550    5.355533    0.213516  \n",
       "min      -147.000000   -1.000000    0.000000   -3.000000  \n",
       "25%         0.000000    0.000000    0.000000    0.000000  \n",
       "50%         0.000000    0.000000    0.000000    0.000000  \n",
       "75%         0.000000    0.000000    0.000000    0.000000  \n",
       "max         0.000000    0.000000   49.000000    0.000000  \n",
       "\n",
       "[8 rows x 799 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_words.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c2da955f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desired</th>\n",
       "      <th>favored</th>\n",
       "      <th>improving</th>\n",
       "      <th>stable</th>\n",
       "      <th>achieving</th>\n",
       "      <th>attractive</th>\n",
       "      <th>good</th>\n",
       "      <th>desirable</th>\n",
       "      <th>confident</th>\n",
       "      <th>opportunity</th>\n",
       "      <th>...</th>\n",
       "      <th>strength</th>\n",
       "      <th>exceptional</th>\n",
       "      <th>favorable</th>\n",
       "      <th>positive</th>\n",
       "      <th>improve</th>\n",
       "      <th>rebound</th>\n",
       "      <th>exceptionally</th>\n",
       "      <th>satisfactory</th>\n",
       "      <th>strengthening</th>\n",
       "      <th>rebounded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.00000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.816667</td>\n",
       "      <td>6.012500</td>\n",
       "      <td>7.108333</td>\n",
       "      <td>5.933333</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>3.487500</td>\n",
       "      <td>41.133333</td>\n",
       "      <td>6.370833</td>\n",
       "      <td>2.495833</td>\n",
       "      <td>7.495833</td>\n",
       "      <td>...</td>\n",
       "      <td>27.708333</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>24.02500</td>\n",
       "      <td>14.870833</td>\n",
       "      <td>3.387500</td>\n",
       "      <td>6.154167</td>\n",
       "      <td>2.529167</td>\n",
       "      <td>2.362500</td>\n",
       "      <td>13.808333</td>\n",
       "      <td>4.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.212447</td>\n",
       "      <td>22.626766</td>\n",
       "      <td>20.653060</td>\n",
       "      <td>14.244566</td>\n",
       "      <td>10.691189</td>\n",
       "      <td>11.478917</td>\n",
       "      <td>79.312731</td>\n",
       "      <td>19.524901</td>\n",
       "      <td>11.876279</td>\n",
       "      <td>21.268359</td>\n",
       "      <td>...</td>\n",
       "      <td>61.956510</td>\n",
       "      <td>9.082513</td>\n",
       "      <td>48.97595</td>\n",
       "      <td>35.800762</td>\n",
       "      <td>8.784487</td>\n",
       "      <td>16.316598</td>\n",
       "      <td>10.930322</td>\n",
       "      <td>12.791777</td>\n",
       "      <td>40.217358</td>\n",
       "      <td>11.234876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>116.000000</td>\n",
       "      <td>277.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>643.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>323.00000</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>119.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          desired     favored   improving      stable   achieving  attractive  \\\n",
       "count  240.000000  240.000000  240.000000  240.000000  240.000000  240.000000   \n",
       "mean     3.816667    6.012500    7.108333    5.933333    2.062500    3.487500   \n",
       "std     13.212447   22.626766   20.653060   14.244566   10.691189   11.478917   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    2.000000    0.000000    0.000000   \n",
       "75%      0.250000    1.000000    2.000000    6.000000    1.000000    0.000000   \n",
       "max    116.000000  277.000000  143.000000  139.000000  136.000000   89.000000   \n",
       "\n",
       "             good   desirable   confident  opportunity  ...    strength  \\\n",
       "count  240.000000  240.000000  240.000000   240.000000  ...  240.000000   \n",
       "mean    41.133333    6.370833    2.495833     7.495833  ...   27.708333   \n",
       "std     79.312731   19.524901   11.876279    21.268359  ...   61.956510   \n",
       "min      1.000000    0.000000    0.000000     0.000000  ...    0.000000   \n",
       "25%      4.000000    0.000000    0.000000     0.000000  ...    1.000000   \n",
       "50%      7.000000    0.000000    0.000000     0.000000  ...    2.000000   \n",
       "75%     37.250000    1.000000    0.000000     1.000000  ...    8.000000   \n",
       "max    643.000000  196.000000  144.000000   175.000000  ...  396.000000   \n",
       "\n",
       "       exceptional  favorable    positive     improve     rebound  \\\n",
       "count   240.000000  240.00000  240.000000  240.000000  240.000000   \n",
       "mean      2.600000   24.02500   14.870833    3.387500    6.154167   \n",
       "std       9.082513   48.97595   35.800762    8.784487   16.316598   \n",
       "min       0.000000    0.00000    0.000000    0.000000    0.000000   \n",
       "25%       0.000000    0.00000    0.000000    0.000000    0.000000   \n",
       "50%       0.000000    2.00000    2.000000    1.000000    1.000000   \n",
       "75%       0.000000   20.00000    5.250000    2.000000    2.000000   \n",
       "max      62.000000  323.00000  239.000000   54.000000  142.000000   \n",
       "\n",
       "       exceptionally  satisfactory  strengthening   rebounded  \n",
       "count     240.000000    240.000000     240.000000  240.000000  \n",
       "mean        2.529167      2.362500      13.808333    4.287500  \n",
       "std        10.930322     12.791777      40.217358   11.234876  \n",
       "min         0.000000      0.000000       0.000000    0.000000  \n",
       "25%         0.000000      0.000000       0.000000    0.000000  \n",
       "50%         0.000000      0.000000       1.000000    1.000000  \n",
       "75%         1.000000      0.000000       3.000000    2.000000  \n",
       "max       145.000000    155.000000     304.000000  119.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean of each column in the 'subset_words' DataFrame\n",
    "column_means = subset_words.mean()\n",
    "\n",
    "# Create a subset of columns with mean >= 2\n",
    "selected_columns = column_means[column_means >= 2].index\n",
    "\n",
    "# Create a new DataFrame with only the selected columns\n",
    "subset_words_mean_2 = subset_words[selected_columns]\n",
    "\n",
    "subset_words_mean_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3cdee798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 192\n",
      "Validation set size: 24\n",
      "Test set size: 24\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation, and test sets\n",
    "train, valid = train_test_split(dataset_adjusted, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Split the combined set into validation and test sets\n",
    "valid, test = train_test_split(valid, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Verify the sizes of each set\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Validation set size:\", len(valid))\n",
    "print(\"Test set size:\", len(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
