{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87439644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import random\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tabulate import tabulate\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "02d6f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download the dataset. Status code: 404\n",
      "Failed to download the dataset. Status code: 404\n"
     ]
    }
   ],
   "source": [
    "# The URL of the raw dataset on GitHub\n",
    "url = \"https://raw.githubusercontent.com/abactat/BC-Project/main/data/dataset_adjusted.csv?token=GHSAT0AAAAAACC4ZCNLXSOPVJD4ZPTZE3OUZGYJQ3A\"\n",
    "\n",
    "# Send an HTTP GET request to fetch the content of the raw dataset\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200 means success)\n",
    "if response.status_code == 200:\n",
    "    # Read the content as a pandas DataFrame and assign it to the 'train' variable\n",
    "    dataset_adjusted = pd.read_csv(StringIO(response.text))\n",
    "    print(\"Dataset downloaded and loaded into 'dataset_adjusted' successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download the dataset. Status code: {response.status_code}\")\n",
    "    \n",
    "# The URL of the raw dataset on GitHub\n",
    "url = \"https://raw.githubusercontent.com/abactat/BC-Project/main/data/raw/dataset_words.csv?token=GHSAT0AAAAAACC4ZCNL775NH4NITFXZSX7AZGYJRFQ\"\n",
    "\n",
    "# Send an HTTP GET request to fetch the content of the raw dataset\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200 means success)\n",
    "if response.status_code == 200:\n",
    "    # Read the content as a pandas DataFrame and assign it to the 'valid' variable\n",
    "    words = pd.read_csv(StringIO(response.text))\n",
    "    print(\"Dataset downloaded and loaded into 'words' successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download the dataset. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "696bc41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentiment word list from the CSV file into a dictionary\n",
    "sentiment_word_list = {}\n",
    "with open(r\"C:\\Users\\abact\\BC-Project\\data\\external\\Loughran-McDonald_MasterDictionary_1993-2021.csv\", 'r') as file:\n",
    "    # Skip the header line\n",
    "    next(file)\n",
    "\n",
    "    for line in file:\n",
    "        values = line.strip().split(',')\n",
    "\n",
    "        # Extract the necessary values\n",
    "        word = values[0].lower()\n",
    "        positive = float(values[8])  # Positive column index is 8\n",
    "        negative = float(values[7])  # Negative column index is 7\n",
    "\n",
    "        # Assign the word as positive or negative based on the positive or negative values\n",
    "        if positive == 2009:\n",
    "            sentiment_word_list[word] = 1\n",
    "        elif negative == 2009:\n",
    "            sentiment_word_list[word] = -1\n",
    "\n",
    "# Convert the sentiment word list keys to lowercase\n",
    "selected_words = set(sentiment_word_list.keys())\n",
    "\n",
    "# Filter the 'words' DataFrame to include only columns that are present in both 'selected_words' and 'words'\n",
    "common_columns = selected_words.intersection(words.columns)\n",
    "subset_words = words[list(common_columns)].copy()\n",
    "\n",
    "# Multiply sentiment values to the vectorized text columns in the 'subset_words' DataFrame\n",
    "for column in subset_words.columns:\n",
    "    sentiment_value = sentiment_word_list.get(column, 0)\n",
    "    if sentiment_value == 1:\n",
    "        subset_words.loc[:, column] = subset_words[column] * 1  # Multiply by 1 for positive sentiment\n",
    "    elif sentiment_value == -1:\n",
    "        subset_words.loc[:, column] = subset_words[column] * -1  # Multiply by -1 for negative sentiment\n",
    "    else:\n",
    "        subset_words.loc[:, column] = subset_words[column] * 0  # Multiply by 0 for unknown sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0293b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the vectorized DataFrame with the original dataset\n",
    "full_dataset = pd.concat([dataset_adjusted, subset_words], axis=1)\n",
    "\n",
    "full_dataset['Date'] = pd.to_datetime(full_dataset['Date'])\n",
    "\n",
    "# Calculate the time difference in days from the first date\n",
    "full_dataset['Date'] = (full_dataset['Date'] - full_dataset['Date'].min()).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "07fdd087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federal_Reserve_Mins    0\n",
      "Preprocessed Text       0\n",
      "Date                    0\n",
      "Difference              0\n",
      "Increase                0\n",
      "                       ..\n",
      "enhancing               0\n",
      "dissent                 0\n",
      "delay                   0\n",
      "recessionary            0\n",
      "enjoy                   0\n",
      "Length: 837, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for the number of missing values in 'train' DataFrame\n",
    "missing_values_count = full_dataset.isna().sum()\n",
    "\n",
    "# Print the count of missing values for each column\n",
    "print(missing_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "32beeb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with Missing Values:\n"
     ]
    }
   ],
   "source": [
    "# Filter columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0]\n",
    "\n",
    "# Print columns with missing values and their counts\n",
    "print(\"Columns with Missing Values:\")\n",
    "for column, count in columns_with_missing_values.items():\n",
    "    print(f\"{column}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "90d18c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 192\n",
      "Validation set size: 24\n",
      "Test set size: 24\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation, and test sets\n",
    "train, valid = train_test_split(full_dataset, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Split the combined set into validation and test sets\n",
    "valid, test = train_test_split(valid, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Verify the sizes of each set\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Validation set size:\", len(valid))\n",
    "print(\"Test set size:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "589999b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert variables to numeric in the train dataset\n",
    "variables_to_convert = train.columns.drop('Date')\n",
    "train[variables_to_convert] = train[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert variables to numeric in the valid dataset\n",
    "valid[variables_to_convert] = valid[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert variables to numeric in the test dataset\n",
    "test[variables_to_convert] = test[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_test = test.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29b81eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(y_true)\n",
    "    \n",
    "    for true_val, pred_val in zip(y_true, y_pred):\n",
    "        if true_val == pred_val:\n",
    "            correct_predictions += 1\n",
    "            \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "possible_values = [-1.00, -0.75, -0.50, -0.25, 0.00, 0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "def round_to_nearest(value, possible_values):\n",
    "    return min(possible_values, key=lambda x: abs(x - value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "10450340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in 'train' DataFrame\n",
    "missing_values_count = train.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'train'\n",
    "train = train.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "# Check for missing values in 'valid' DataFrame\n",
    "missing_values_count = valid.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'valid'\n",
    "valid = valid.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "# Check for missing values in 'test' DataFrame\n",
    "missing_values_count = test.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'test'\n",
    "test = test.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_test = test.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "55bc828c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date            0\n",
      "Difference      0\n",
      "Increase        0\n",
      "Decrease        0\n",
      "Level           0\n",
      "               ..\n",
      "enhancing       0\n",
      "dissent         0\n",
      "delay           0\n",
      "recessionary    0\n",
      "enjoy           0\n",
      "Length: 833, dtype: int64\n",
      "Columns with Missing Values:\n"
     ]
    }
   ],
   "source": [
    "# Check for the number of missing values in 'train' DataFrame\n",
    "missing_values_count = train.isna().sum()\n",
    "\n",
    "# Print the count of missing values for each column\n",
    "print(missing_values_count)\n",
    "\n",
    "# Filter columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0]\n",
    "\n",
    "# Print columns with missing values and their counts\n",
    "print(\"Columns with Missing Values:\")\n",
    "for column, count in columns_with_missing_values.items():\n",
    "    print(f\"{column}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "969738df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Create a RandomForestRegressor instance (you need to adjust hyperparameters)\n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Create GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data and find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with optimal hyperparameters\n",
    "best_rf_model_5 = grid_search.best_estimator_\n",
    "\n",
    "# Predict y_train_pred on the training set\n",
    "y_train_pred = best_rf_model_5.predict(X_train)\n",
    "\n",
    "# Predict y_valid_pred on the validation set\n",
    "y_valid_pred = best_rf_model_5.predict(X_valid)\n",
    "\n",
    "# Round the predicted values to the nearest possible value\n",
    "y_train_pred = [round_to_nearest(val, possible_values) for val in y_train_pred]\n",
    "y_valid_pred = [round_to_nearest(val, possible_values) for val in y_valid_pred]\n",
    "\n",
    "# Output the random seed\n",
    "print(\"Random seed:\", random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3aed26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the feature importances from the best model\n",
    "feature_importances = best_rf_model_5.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "num_features_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1801af51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------------+\n",
      "| Metric       |   Training Set |   Validation Set |\n",
      "+==============+================+==================+\n",
      "| MSE          |     0.00325521 |        0.0364583 |\n",
      "+--------------+----------------+------------------+\n",
      "| RMSE         |     0.0570544  |        0.190941  |\n",
      "+--------------+----------------+------------------+\n",
      "| R^2          |     0.918696   |        0.439065  |\n",
      "+--------------+----------------+------------------+\n",
      "| Adjusted R^2 |     1.0243     |        1.01599   |\n",
      "+--------------+----------------+------------------+\n",
      "| Accuracy     |     0.947917   |        0.666667  |\n",
      "+--------------+----------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]\n",
    "p_train = X_train.shape[1]\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]\n",
    "p_valid = X_valid.shape[1]\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "\n",
    "# Calculate accuracy for training and validation sets\n",
    "accuracy_train = calculate_accuracy(y_train, y_train_pred)\n",
    "accuracy_valid = calculate_accuracy(y_valid, y_valid_pred)\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = [\n",
    "    [\"MSE\", mse_train, mse_valid],\n",
    "    [\"RMSE\", rmse_train, rmse_valid],\n",
    "    [\"R^2\", r2_train, r2_valid],\n",
    "    [\"Adjusted R^2\", adj_r2_train, adj_r2_valid],\n",
    "    [\"Accuracy\", accuracy_train, accuracy_valid],\n",
    "]\n",
    "\n",
    "# Prepare the headers for the table\n",
    "headers = [\"Metric\", \"Training Set\", \"Validation Set\"]\n",
    "\n",
    "# Display the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9462a7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seeds: [2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# List to store random seeds used in each iteration\n",
    "random_seeds = []\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Function to calculate accuracy based on a threshold\n",
    "def calculate_accuracy(y_true, y_pred, threshold):\n",
    "    num_samples = len(y_true)\n",
    "    correct_predictions = sum(abs(y_true - y_pred) <= threshold)\n",
    "    return correct_predictions / num_samples\n",
    "\n",
    "best_accuracy = -1.0\n",
    "optimal_cv = None\n",
    "best_y_train_pred = None\n",
    "best_y_valid_pred = None\n",
    "threshold = 0.1  # Define your desired threshold here\n",
    "\n",
    "for cv in range(2, 11):  # Try cross-validation folds from 2 to 10\n",
    "    # Set the random seed for reproducibility\n",
    "    random_seed = cv  # Use cv as the random seed\n",
    "    random_seeds.append(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=cv, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model with optimal hyperparameters\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predict y_train_pred on the training set\n",
    "    y_train_pred = best_rf_model.predict(X_train)\n",
    "\n",
    "    # Predict y_valid_pred on the validation set\n",
    "    y_valid_pred = best_rf_model.predict(X_valid)\n",
    "\n",
    "    # Round the predicted values to the nearest possible value\n",
    "    y_train_pred = [round_to_nearest(val, possible_values) for val in y_train_pred]\n",
    "    y_valid_pred = [round_to_nearest(val, possible_values) for val in y_valid_pred]\n",
    "\n",
    "    # Calculate accuracy for training and validation sets after rounding\n",
    "    accuracy_train = calculate_accuracy(y_train, y_train_pred, threshold)\n",
    "    accuracy_valid = calculate_accuracy(y_valid, y_valid_pred, threshold)\n",
    "    \n",
    "    # Check if the accuracy after rounding is higher than the best accuracy so far\n",
    "    if accuracy_valid > best_accuracy:\n",
    "        best_accuracy = accuracy_valid\n",
    "        optimal_cv = cv\n",
    "        best_y_train_pred = y_train_pred\n",
    "        best_y_valid_pred = y_valid_pred\n",
    "\n",
    "# Use the optimal number of folds in GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=optimal_cv, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with optimal hyperparameters\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Get the feature importances from the best model\n",
    "feature_importances = best_rf_model.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "# Print the random seeds used in each iteration\n",
    "print(\"Random Seeds:\", random_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c285395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the feature importances from the best model\n",
    "feature_importances = best_rf_model.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "num_features_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5d7a310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------------+\n",
      "| Metric       |   Training Set |   Validation Set |\n",
      "+==============+================+==================+\n",
      "| MSE          |     0.00520833 |        0.0286458 |\n",
      "+--------------+----------------+------------------+\n",
      "| RMSE         |     0.0721688  |        0.169251  |\n",
      "+--------------+----------------+------------------+\n",
      "| R^2          |     0.869913   |        0.559265  |\n",
      "+--------------+----------------+------------------+\n",
      "| Adjusted R^2 |     1.03888    |        1.01256   |\n",
      "+--------------+----------------+------------------+\n",
      "| Accuracy     |     0.932292   |        0.666667  |\n",
      "+--------------+----------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]\n",
    "p_train = X_train.shape[1]\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]\n",
    "p_valid = X_valid.shape[1]\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "\n",
    "threshold = 0.1\n",
    "accuracy_train = calculate_accuracy(y_train, y_train_pred, threshold)\n",
    "accuracy_valid = calculate_accuracy(y_valid, y_valid_pred, threshold)\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = [\n",
    "    [\"MSE\", mse_train, mse_valid],\n",
    "    [\"RMSE\", rmse_train, rmse_valid],\n",
    "    [\"R^2\", r2_train, r2_valid],\n",
    "    [\"Adjusted R^2\", adj_r2_train, adj_r2_valid],\n",
    "    [\"Accuracy\", accuracy_train, accuracy_valid],\n",
    "]\n",
    "\n",
    "# Prepare the headers for the table\n",
    "headers = [\"Metric\", \"Training Set\", \"Validation Set\"]\n",
    "\n",
    "# Display the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e0ff39",
   "metadata": {},
   "source": [
    "###Data Centric AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7c8d7ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>optimistic</th>\n",
       "      <th>encouraged</th>\n",
       "      <th>persists</th>\n",
       "      <th>overstate</th>\n",
       "      <th>hazard</th>\n",
       "      <th>discontinuing</th>\n",
       "      <th>breaching</th>\n",
       "      <th>cutback</th>\n",
       "      <th>purported</th>\n",
       "      <th>forestall</th>\n",
       "      <th>...</th>\n",
       "      <th>depress</th>\n",
       "      <th>idle</th>\n",
       "      <th>omit</th>\n",
       "      <th>questioning</th>\n",
       "      <th>unprofitable</th>\n",
       "      <th>enhancing</th>\n",
       "      <th>dissent</th>\n",
       "      <th>delay</th>\n",
       "      <th>recessionary</th>\n",
       "      <th>enjoy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.379167</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>-0.029167</td>\n",
       "      <td>-0.016667</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>-2.050000</td>\n",
       "      <td>-0.008333</td>\n",
       "      <td>-1.158333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.741667</td>\n",
       "      <td>-0.291667</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>-0.141667</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>0.545833</td>\n",
       "      <td>-0.016667</td>\n",
       "      <td>-2.054167</td>\n",
       "      <td>-0.008333</td>\n",
       "      <td>0.104167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.054113</td>\n",
       "      <td>3.193261</td>\n",
       "      <td>0.064550</td>\n",
       "      <td>0.111335</td>\n",
       "      <td>0.280435</td>\n",
       "      <td>0.203868</td>\n",
       "      <td>0.064550</td>\n",
       "      <td>11.035773</td>\n",
       "      <td>0.091096</td>\n",
       "      <td>5.888072</td>\n",
       "      <td>...</td>\n",
       "      <td>4.839682</td>\n",
       "      <td>3.252024</td>\n",
       "      <td>1.686438</td>\n",
       "      <td>2.130849</td>\n",
       "      <td>0.064550</td>\n",
       "      <td>3.783633</td>\n",
       "      <td>0.128287</td>\n",
       "      <td>7.109513</td>\n",
       "      <td>0.091096</td>\n",
       "      <td>1.550268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-111.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-53.000000</td>\n",
       "      <td>-42.000000</td>\n",
       "      <td>-26.000000</td>\n",
       "      <td>-33.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>66.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 799 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       optimistic  encouraged    persists   overstate      hazard  \\\n",
       "count  240.000000  240.000000  240.000000  240.000000  240.000000   \n",
       "mean     2.379167    0.437500   -0.004167   -0.012500   -0.029167   \n",
       "std      9.054113    3.193261    0.064550    0.111335    0.280435   \n",
       "min      0.000000    0.000000   -1.000000   -1.000000   -3.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      1.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "max     66.000000   40.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "       discontinuing   breaching     cutback   purported   forestall  ...  \\\n",
       "count     240.000000  240.000000  240.000000  240.000000  240.000000  ...   \n",
       "mean       -0.016667   -0.004167   -2.050000   -0.008333   -1.158333  ...   \n",
       "std         0.203868    0.064550   11.035773    0.091096    5.888072  ...   \n",
       "min        -3.000000   -1.000000 -111.000000   -1.000000  -60.000000  ...   \n",
       "25%         0.000000    0.000000    0.000000    0.000000    0.000000  ...   \n",
       "50%         0.000000    0.000000    0.000000    0.000000    0.000000  ...   \n",
       "75%         0.000000    0.000000    0.000000    0.000000    0.000000  ...   \n",
       "max         0.000000    0.000000    0.000000    0.000000    0.000000  ...   \n",
       "\n",
       "          depress        idle        omit  questioning  unprofitable  \\\n",
       "count  240.000000  240.000000  240.000000   240.000000    240.000000   \n",
       "mean    -0.741667   -0.291667   -0.133333    -0.141667     -0.004167   \n",
       "std      4.839682    3.252024    1.686438     2.130849      0.064550   \n",
       "min    -53.000000  -42.000000  -26.000000   -33.000000     -1.000000   \n",
       "25%      0.000000    0.000000    0.000000     0.000000      0.000000   \n",
       "50%      0.000000    0.000000    0.000000     0.000000      0.000000   \n",
       "75%      0.000000    0.000000    0.000000     0.000000      0.000000   \n",
       "max      0.000000    0.000000    0.000000     0.000000      0.000000   \n",
       "\n",
       "        enhancing     dissent       delay  recessionary       enjoy  \n",
       "count  240.000000  240.000000  240.000000    240.000000  240.000000  \n",
       "mean     0.545833   -0.016667   -2.054167     -0.008333    0.104167  \n",
       "std      3.783633    0.128287    7.109513      0.091096    1.550268  \n",
       "min      0.000000   -1.000000  -50.000000     -1.000000    0.000000  \n",
       "25%      0.000000    0.000000   -1.000000      0.000000    0.000000  \n",
       "50%      0.000000    0.000000    0.000000      0.000000    0.000000  \n",
       "75%      0.000000    0.000000    0.000000      0.000000    0.000000  \n",
       "max     41.000000    0.000000    0.000000      0.000000   24.000000  \n",
       "\n",
       "[8 rows x 799 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_words.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c65d390c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volatile</th>\n",
       "      <th>rebounded</th>\n",
       "      <th>lagged</th>\n",
       "      <th>stronger</th>\n",
       "      <th>downward</th>\n",
       "      <th>cut</th>\n",
       "      <th>difficult</th>\n",
       "      <th>desirable</th>\n",
       "      <th>imbalance</th>\n",
       "      <th>declining</th>\n",
       "      <th>...</th>\n",
       "      <th>volatility</th>\n",
       "      <th>positive</th>\n",
       "      <th>declined</th>\n",
       "      <th>boost</th>\n",
       "      <th>weakening</th>\n",
       "      <th>contraction</th>\n",
       "      <th>favorable</th>\n",
       "      <th>decline</th>\n",
       "      <th>negative</th>\n",
       "      <th>gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.00000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-4.633333</td>\n",
       "      <td>4.287500</td>\n",
       "      <td>-4.508333</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>-8.958333</td>\n",
       "      <td>-4.858333</td>\n",
       "      <td>-6.954167</td>\n",
       "      <td>6.370833</td>\n",
       "      <td>-3.875000</td>\n",
       "      <td>-10.595833</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.808333</td>\n",
       "      <td>14.870833</td>\n",
       "      <td>-24.683333</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>-5.979167</td>\n",
       "      <td>-3.687500</td>\n",
       "      <td>24.02500</td>\n",
       "      <td>-54.737500</td>\n",
       "      <td>-9.008333</td>\n",
       "      <td>39.962500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.597584</td>\n",
       "      <td>11.234876</td>\n",
       "      <td>14.828445</td>\n",
       "      <td>11.784561</td>\n",
       "      <td>22.059598</td>\n",
       "      <td>13.525303</td>\n",
       "      <td>17.105199</td>\n",
       "      <td>19.524901</td>\n",
       "      <td>12.461195</td>\n",
       "      <td>26.719040</td>\n",
       "      <td>...</td>\n",
       "      <td>9.109557</td>\n",
       "      <td>35.800762</td>\n",
       "      <td>46.589363</td>\n",
       "      <td>13.691663</td>\n",
       "      <td>18.116418</td>\n",
       "      <td>12.240198</td>\n",
       "      <td>48.97595</td>\n",
       "      <td>108.182676</td>\n",
       "      <td>22.721157</td>\n",
       "      <td>68.129972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-94.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-143.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-185.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-110.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-105.000000</td>\n",
       "      <td>-248.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-73.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-470.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-140.000000</td>\n",
       "      <td>-88.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-891.000000</td>\n",
       "      <td>-149.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-38.250000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>6.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>-14.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>323.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>353.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         volatile   rebounded      lagged    stronger    downward         cut  \\\n",
       "count  240.000000  240.000000  240.000000  240.000000  240.000000  240.000000   \n",
       "mean    -4.633333    4.287500   -4.508333    5.333333   -8.958333   -4.858333   \n",
       "std     13.597584   11.234876   14.828445   11.784561   22.059598   13.525303   \n",
       "min    -94.000000    0.000000 -143.000000    0.000000 -185.000000 -100.000000   \n",
       "25%     -2.000000    0.000000   -1.000000    0.000000   -4.000000   -2.000000   \n",
       "50%      0.000000    1.000000    0.000000    1.000000   -2.000000    0.000000   \n",
       "75%      0.000000    2.000000    0.000000    4.000000    0.000000    0.000000   \n",
       "max      0.000000  119.000000    0.000000   75.000000    0.000000    0.000000   \n",
       "\n",
       "        difficult   desirable   imbalance   declining  ...  volatility  \\\n",
       "count  240.000000  240.000000  240.000000  240.000000  ...  240.000000   \n",
       "mean    -6.954167    6.370833   -3.875000  -10.595833  ...   -3.808333   \n",
       "std     17.105199   19.524901   12.461195   26.719040  ...    9.109557   \n",
       "min   -110.000000    0.000000 -105.000000 -248.000000  ...  -73.000000   \n",
       "25%     -2.000000    0.000000   -2.000000   -4.000000  ...   -3.000000   \n",
       "50%     -1.000000    0.000000    0.000000   -1.000000  ...   -1.000000   \n",
       "75%      0.000000    1.000000    0.000000    0.000000  ...    0.000000   \n",
       "max      0.000000  196.000000    0.000000    0.000000  ...    0.000000   \n",
       "\n",
       "         positive    declined       boost   weakening  contraction  favorable  \\\n",
       "count  240.000000  240.000000  240.000000  240.000000   240.000000  240.00000   \n",
       "mean    14.870833  -24.683333    5.666667   -5.979167    -3.687500   24.02500   \n",
       "std     35.800762   46.589363   13.691663   18.116418    12.240198   48.97595   \n",
       "min      0.000000 -470.000000    0.000000 -140.000000   -88.000000    0.00000   \n",
       "25%      0.000000  -19.000000    0.000000   -1.000000    -1.000000    0.00000   \n",
       "50%      2.000000  -10.000000    1.000000    0.000000     0.000000    2.00000   \n",
       "75%      5.250000   -6.000000    2.000000    0.000000     0.000000   20.00000   \n",
       "max    239.000000    0.000000   89.000000    0.000000     0.000000  323.00000   \n",
       "\n",
       "          decline    negative        gain  \n",
       "count  240.000000  240.000000  240.000000  \n",
       "mean   -54.737500   -9.008333   39.962500  \n",
       "std    108.182676   22.721157   68.129972  \n",
       "min   -891.000000 -149.000000    0.000000  \n",
       "25%    -38.250000   -3.000000    6.750000  \n",
       "50%    -14.000000   -1.000000    9.000000  \n",
       "75%     -8.000000    0.000000   30.250000  \n",
       "max      0.000000    0.000000  353.000000  \n",
       "\n",
       "[8 rows x 83 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean of each column in the 'subset_words' DataFrame\n",
    "column_means = subset_words.mean()\n",
    "\n",
    "# Create a subset of columns with mean >= |3|\n",
    "selected_columns = column_means[column_means.abs() >= 3].index\n",
    "\n",
    "# Create a new DataFrame with only the selected columns\n",
    "subset_words_mean_3 = subset_words[selected_columns]\n",
    "\n",
    "subset_words_mean_3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "267fd149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the vectorized DataFrame with the original dataset\n",
    "second_dataset = pd.concat([dataset_adjusted, subset_words_mean_3], axis=1)\n",
    "\n",
    "second_dataset['Date'] = pd.to_datetime(second_dataset['Date'])\n",
    "\n",
    "# Calculate the time difference in days from the first date\n",
    "second_dataset['Date'] = (second_dataset['Date'] - second_dataset['Date'].min()).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a44e8c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 192\n",
      "Validation set size: 24\n",
      "Test set size: 24\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation, and test sets\n",
    "train, valid = train_test_split(second_dataset, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Split the combined set into validation and test sets\n",
    "valid, test = train_test_split(valid, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Verify the sizes of each set\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Validation set size:\", len(valid))\n",
    "print(\"Test set size:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f5b9b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert variables to numeric in the train dataset\n",
    "variables_to_convert = train.columns.drop('Date')\n",
    "train[variables_to_convert] = train[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert variables to numeric in the valid dataset\n",
    "valid[variables_to_convert] = valid[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert variables to numeric in the test dataset\n",
    "test[variables_to_convert] = test[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_test = test.drop(columns=['Difference', 'Increase', 'Decrease', 'Date'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aa449ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in 'train' DataFrame\n",
    "missing_values_count = train.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'train'\n",
    "train = train.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "# Check for missing values in 'valid' DataFrame\n",
    "missing_values_count = valid.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'valid'\n",
    "valid = valid.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "# Check for missing values in 'test' DataFrame\n",
    "missing_values_count = test.isna().sum()\n",
    "\n",
    "# Get the list of columns with missing values\n",
    "columns_with_missing_values = missing_values_count[missing_values_count > 0].index.tolist()\n",
    "\n",
    "# Drop columns with missing values from 'test'\n",
    "test = test.drop(columns=columns_with_missing_values)\n",
    "\n",
    "# Prepare the data for the model\n",
    "X_test = test.drop(columns=['Difference', 'Increase', 'Decrease'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df7320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Create GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the training data and find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with optimal hyperparameters\n",
    "second_rf_model_5 = grid_search.best_estimator_\n",
    "\n",
    "# Predict y_train_pred on the training set\n",
    "y_train_pred = second_rf_model_5.predict(X_train)\n",
    "\n",
    "# Predict y_valid_pred on the validation set\n",
    "y_valid_pred = second_rf_model_5.predict(X_valid)\n",
    "\n",
    "# Round the predicted values to the nearest possible value\n",
    "y_train_pred = [round_to_nearest(val, possible_values) for val in y_train_pred]\n",
    "y_valid_pred = [round_to_nearest(val, possible_values) for val in y_valid_pred]\n",
    "\n",
    "# Output the random seed\n",
    "print(\"Random seed:\", random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances from the best model\n",
    "feature_importances = second_rf_model_5.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "num_features_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6031ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(y_true)\n",
    "    \n",
    "    for true_val, pred_val in zip(y_true, y_pred):\n",
    "        if true_val == pred_val:\n",
    "            correct_predictions += 1\n",
    "            \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]\n",
    "p_train = X_train.shape[1]\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]\n",
    "p_valid = X_valid.shape[1]\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "\n",
    "threshold = 0.1\n",
    "accuracy_train = calculate_accuracy(y_train, y_train_pred)\n",
    "accuracy_valid = calculate_accuracy(y_valid, y_valid_pred)\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = [\n",
    "    [\"MSE\", mse_train, mse_valid],\n",
    "    [\"RMSE\", rmse_train, rmse_valid],\n",
    "    [\"R^2\", r2_train, r2_valid],\n",
    "    [\"Adjusted R^2\", adj_r2_train, adj_r2_valid],\n",
    "    [\"Accuracy\", accuracy_train, accuracy_valid],\n",
    "]\n",
    "\n",
    "# Prepare the headers for the table\n",
    "headers = [\"Metric\", \"Training Set\", \"Validation Set\"]\n",
    "\n",
    "# Display the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5332a524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Federal_Reserve_Mins</th>\n",
       "      <th>Preprocessed Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>Difference</th>\n",
       "      <th>Increase</th>\n",
       "      <th>Decrease</th>\n",
       "      <th>Level</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment Rate</th>\n",
       "      <th>Consumer Sentiment</th>\n",
       "      <th>...</th>\n",
       "      <th>volatility</th>\n",
       "      <th>positive</th>\n",
       "      <th>declined</th>\n",
       "      <th>boost</th>\n",
       "      <th>weakening</th>\n",
       "      <th>contraction</th>\n",
       "      <th>favorable</th>\n",
       "      <th>decline</th>\n",
       "      <th>negative</th>\n",
       "      <th>gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A meeting of the Federal Open Market Committee...</td>\n",
       "      <td>meeting federal open market committee held off...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>144.200</td>\n",
       "      <td>7.1</td>\n",
       "      <td>80.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>-11</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A meeting of the Federal Open Market Committee...</td>\n",
       "      <td>meeting federal open market committee held off...</td>\n",
       "      <td>50</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>144.500</td>\n",
       "      <td>6.9</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-8</td>\n",
       "      <td>-2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A meeting of the Federal Open Market Committee...</td>\n",
       "      <td>meeting federal open market committee held off...</td>\n",
       "      <td>91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>144.800</td>\n",
       "      <td>6.8</td>\n",
       "      <td>77.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-6</td>\n",
       "      <td>-3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A meeting of the Federal Open Market Committee...</td>\n",
       "      <td>meeting federal open market committee held off...</td>\n",
       "      <td>126</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>145.000</td>\n",
       "      <td>6.7</td>\n",
       "      <td>77.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-13</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A meeting of the Federal Open Market Committee...</td>\n",
       "      <td>meeting federal open market committee held off...</td>\n",
       "      <td>182</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>146.000</td>\n",
       "      <td>6.6</td>\n",
       "      <td>81.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-5</td>\n",
       "      <td>-2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "      <td>federal reserve central bank united state prov...</td>\n",
       "      <td>10760</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>297.987</td>\n",
       "      <td>3.7</td>\n",
       "      <td>59.9</td>\n",
       "      <td>...</td>\n",
       "      <td>-12</td>\n",
       "      <td>0</td>\n",
       "      <td>-7</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7</td>\n",
       "      <td>-2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "      <td>federal reserve central bank united state prov...</td>\n",
       "      <td>10802</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.50</td>\n",
       "      <td>298.990</td>\n",
       "      <td>3.5</td>\n",
       "      <td>59.7</td>\n",
       "      <td>...</td>\n",
       "      <td>-5</td>\n",
       "      <td>1</td>\n",
       "      <td>-13</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-14</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "      <td>federal reserve central bank united state prov...</td>\n",
       "      <td>10851</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>300.536</td>\n",
       "      <td>3.4</td>\n",
       "      <td>64.9</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-13</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "      <td>federal reserve central bank united state prov...</td>\n",
       "      <td>10900</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>301.808</td>\n",
       "      <td>3.5</td>\n",
       "      <td>62.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>-8</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>The Federal Reserve, the central bank of the U...</td>\n",
       "      <td>federal reserve central bank united state prov...</td>\n",
       "      <td>10942</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.25</td>\n",
       "      <td>302.918</td>\n",
       "      <td>3.4</td>\n",
       "      <td>63.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-7</td>\n",
       "      <td>1</td>\n",
       "      <td>-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-12</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Federal_Reserve_Mins  \\\n",
       "0    A meeting of the Federal Open Market Committee...   \n",
       "1    A meeting of the Federal Open Market Committee...   \n",
       "2    A meeting of the Federal Open Market Committee...   \n",
       "3    A meeting of the Federal Open Market Committee...   \n",
       "4    A meeting of the Federal Open Market Committee...   \n",
       "..                                                 ...   \n",
       "235  The Federal Reserve, the central bank of the U...   \n",
       "236  The Federal Reserve, the central bank of the U...   \n",
       "237  The Federal Reserve, the central bank of the U...   \n",
       "238  The Federal Reserve, the central bank of the U...   \n",
       "239  The Federal Reserve, the central bank of the U...   \n",
       "\n",
       "                                     Preprocessed Text   Date  Difference  \\\n",
       "0    meeting federal open market committee held off...      0        0.00   \n",
       "1    meeting federal open market committee held off...     50       -0.50   \n",
       "2    meeting federal open market committee held off...     91        0.00   \n",
       "3    meeting federal open market committee held off...    126        0.00   \n",
       "4    meeting federal open market committee held off...    182        0.00   \n",
       "..                                                 ...    ...         ...   \n",
       "235  federal reserve central bank united state prov...  10760        0.75   \n",
       "236  federal reserve central bank united state prov...  10802        0.50   \n",
       "237  federal reserve central bank united state prov...  10851        0.25   \n",
       "238  federal reserve central bank united state prov...  10900        0.25   \n",
       "239  federal reserve central bank united state prov...  10942        0.25   \n",
       "\n",
       "     Increase  Decrease  Level      CPI  Unemployment Rate  \\\n",
       "0        0.00       0.5   3.75  144.200                7.1   \n",
       "1        0.00       0.0   3.25  144.500                6.9   \n",
       "2        0.00       0.0   3.25  144.800                6.8   \n",
       "3        0.00       0.0   3.25  145.000                6.7   \n",
       "4        0.00       0.0   3.25  146.000                6.6   \n",
       "..        ...       ...    ...      ...                ...   \n",
       "235      0.50       0.0   4.00  297.987                3.7   \n",
       "236      0.25       0.0   4.50  298.990                3.5   \n",
       "237      0.25       0.0   4.75  300.536                3.4   \n",
       "238      0.25       0.0   5.00  301.808                3.5   \n",
       "239      0.00       0.0   5.25  302.918                3.4   \n",
       "\n",
       "     Consumer Sentiment  ...  volatility  positive  declined  boost  \\\n",
       "0                  80.3  ...           0         4        -3      0   \n",
       "1                  77.0  ...           0         2        -5      0   \n",
       "2                  77.3  ...           0         1        -8      1   \n",
       "3                  77.9  ...           0         1        -5      1   \n",
       "4                  81.2  ...           0         0        -1      0   \n",
       "..                  ...  ...         ...       ...       ...    ...   \n",
       "235                59.9  ...         -12         0        -7      0   \n",
       "236                59.7  ...          -5         1       -13      0   \n",
       "237                64.9  ...          -2         0       -10      2   \n",
       "238                62.0  ...          -4         0        -8      0   \n",
       "239                63.5  ...          -7         1       -14      0   \n",
       "\n",
       "     weakening  contraction  favorable  decline  negative  gain  \n",
       "0           -3           -1          6      -11        -1    10  \n",
       "1            0            0          6       -8        -2     6  \n",
       "2            0            0          6       -6        -3     4  \n",
       "3            0            0          6      -13        -1     9  \n",
       "4            0            0          4       -5        -2     8  \n",
       "..         ...          ...        ...      ...       ...   ...  \n",
       "235         -1            0          0       -7        -2     5  \n",
       "236         -2            0          0      -14        -1     7  \n",
       "237          0            0          0      -13        -1     5  \n",
       "238         -2            0          0       -9         0     7  \n",
       "239          0            0          1      -12         0     8  \n",
       "\n",
       "[240 rows x 121 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1899c025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features and Absolute Standardized Relevance Scores (Descending Order):\n",
      "Short-Term Treasury Diff: 10.46605321524853\n",
      "Proportion Negative Words: 0.7607458083403843\n",
      "LAG_RollingMean: 0.48261304866862675\n",
      "easing: 0.46182347750381014\n",
      "LAG: 0.43279288515831177\n",
      "weaker: 0.42247845380010396\n",
      "weakness: 0.34363517283631523\n",
      "contraction: 0.2624072756180427\n",
      "Consumer Sentiment: -0.2528868630117049\n",
      "improve: -0.2501082557551958\n",
      "boosted: -0.2495308888834817\n",
      "persisting: -0.2494660135029889\n",
      "Proportion Positive Words: -0.24087144280941072\n",
      "strengthen: -0.23831170451472158\n",
      "concerned: -0.23184765784744207\n",
      "turmoil: -0.23171464634280825\n",
      "strong: -0.22796609772863247\n",
      "slowing: -0.22351326589942916\n",
      "improving: -0.2232410365460709\n",
      "slower: -0.2230806666261028\n",
      "question: -0.22161881586384774\n",
      "imbalance: -0.22104161629379762\n",
      "diminishing: -0.2188573162603204\n",
      "stability: -0.21769361000936252\n",
      "slow: -0.21530161731721756\n",
      "Housing Sales: -0.21423074892979543\n",
      "slowed: -0.21235192291064128\n",
      "better: -0.21158024211084825\n",
      "gain: -0.21141211395611298\n",
      "stable: -0.21066319268966918\n",
      "despite: -0.21050453494347995\n",
      "unemployment: -0.20893493044515624\n",
      "favored: -0.20836243478249014\n",
      "strength: -0.20789581270269689\n",
      "rebounded: -0.20746710205483015\n",
      "strengthened: -0.20560847738107793\n",
      "difficult: -0.20520201820357958\n",
      "desirable: -0.20445085867220733\n",
      "Nonfarm Payroll: -0.19838052329249092\n",
      "Standardized Sentiment Score: 0.19780763958900766\n",
      "Net Sentiment Score: 0.19780763958900596\n",
      "Long-Term Treasury Bond Rate: -0.19487613603763132\n",
      "desired: -0.1938811174760175\n",
      "Sentiment Label: -0.1925577315858196\n",
      "boost: -0.19235315629720154\n",
      "slowly: -0.19224891278226516\n",
      "problem: -0.18964413448758824\n",
      "Level: -0.18699034698963127\n",
      "deficit: -0.1867425347923245\n",
      "Date: -0.1864361656256496\n",
      "volatility: -0.18220519164905735\n",
      "weakened: -0.17931976419861248\n",
      "strengthening: -0.1789579436269232\n",
      "caution: -0.17133781144350074\n",
      "good: -0.1702263774617851\n",
      "severe: -0.16996067450829191\n",
      "lagged: -0.16916509198884142\n",
      "LEI: -0.1671970180497361\n",
      "Short-Term Treasury Bond Rate: -0.16601107200045026\n",
      "Treasury Deposits: -0.1650447448961191\n",
      "progress: -0.1608895499496965\n",
      "Unemployment Rate: -0.1596456998980134\n",
      "absence: -0.15941696111291603\n",
      "Bank Reserves: -0.158821737996376\n",
      "decline: 0.15853731218548878\n",
      "difficulty: -0.1583951378692706\n",
      "slowdown: -0.1577707317569922\n",
      "CEI: 0.15471406281383956\n",
      "tightening: -0.15344844156256854\n",
      "CEI_RollingMean: 0.15088259029997567\n",
      "improvement: -0.14669302303512985\n",
      "depressed: -0.14624824078481005\n",
      "disappointing: -0.14154916594097214\n",
      "positive: -0.14124268063366632\n",
      "declining: -0.1390473871740999\n",
      "cut: -0.1352232013794065\n",
      "favorable: -0.13520575138326713\n",
      "Average Hourly Earnings: -0.13448791612287522\n",
      "volatile: -0.13010096883165112\n",
      "persistence: -0.12928098701973068\n",
      "CPI: -0.12835024453250066\n",
      "upturn: -0.12820848387682385\n",
      "declined: -0.12613850405582608\n",
      "loss: -0.12410369338235705\n",
      "Positive Frequency: -0.11950834508548787\n",
      "downturn: -0.11266718809585355\n",
      "Word Count: -0.11193924534430187\n",
      "rebound: 0.10879141008885315\n",
      "opportunity: -0.10672655711071077\n",
      "diminished: -0.10662794608849326\n",
      "Durable Goods Orders_RollingMean: -0.10075808667364367\n",
      "stabilization: -0.09113608351538235\n",
      "adverse: 0.08920954042397271\n",
      "dropped: -0.0885733753552838\n",
      "attractive: -0.08016127879719062\n",
      "weak: -0.07864139907281263\n",
      "improved: -0.07242310349800266\n",
      "stronger: -0.07219065721180924\n",
      "weakening: 0.07064342680631316\n",
      "sluggish: -0.06005067054699717\n",
      "concern: -0.058858252167629695\n",
      "dissented: -0.05689742389288811\n",
      "sharply: -0.05592480131281088\n",
      "Retail Sales_RollingMean: -0.04343186647790213\n",
      "correction: 0.0401070040895268\n",
      "liquidation: -0.033794508842418025\n",
      "Negative Frequency: 0.03138261591104559\n",
      "negative: -0.03045065965991951\n",
      "Retail Sales: -0.02815513359434406\n",
      "Durable Goods Orders: -0.026828365446539996\n",
      "LEI_RollingMean: -0.013222303770652248\n",
      "persist: 0.004840430167197406\n",
      "downward: -0.0035941830018749203\n",
      "deterioration: -0.0011907120913957573\n"
     ]
    }
   ],
   "source": [
    "# Subset numerical columns\n",
    "numerical_columns = second_dataset.select_dtypes(include='number')\n",
    "\n",
    "# Remove 'Difference', 'Increase', and 'Decrease' from numerical columns\n",
    "columns_to_exclude = ['Difference', 'Increase', 'Decrease']\n",
    "numerical_columns_subset = numerical_columns.drop(columns_to_exclude, axis=1)\n",
    "\n",
    "# Fill missing values with the mean in numerical columns\n",
    "numerical_columns_subset.fillna(numerical_columns_subset.mean(), inplace=True)\n",
    "\n",
    "# Fill missing values with the mean in 'Difference'\n",
    "difference_mean = second_dataset['Difference'].mean()\n",
    "second_dataset['Difference'].fillna(difference_mean, inplace=True)\n",
    "\n",
    "# Define the number of features to select\n",
    "k = 114\n",
    "\n",
    "# Perform univariate selection using f_regression\n",
    "selector = SelectKBest(score_func=f_regression, k=k)\n",
    "selected_features = selector.fit_transform(numerical_columns_subset, second_dataset['Difference'])\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_features_names = numerical_columns_subset.columns[selected_indices]\n",
    "\n",
    "# Get the feature scores\n",
    "feature_scores = selector.scores_[selected_indices]\n",
    "\n",
    "# Standardize the feature scores\n",
    "scaler = StandardScaler()\n",
    "standardized_scores = scaler.fit_transform(feature_scores.reshape(-1, 1))\n",
    "\n",
    "# Combine selected feature names and their standardized scores\n",
    "selected_features_with_scores = list(zip(selected_features_names, standardized_scores))\n",
    "\n",
    "# Sort the selected features by the absolute value of standardized scores in descending order\n",
    "selected_features_with_scores.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Print the selected features and their standardized relevance scores in descending order\n",
    "print(\"Selected Features and Absolute Standardized Relevance Scores (Descending Order):\")\n",
    "for feature, score in selected_features_with_scores:\n",
    "    print(f\"{feature}: {score[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1bb36fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features with importance greater than the absolute value of 0.15\n",
    "selected_features_greater_than_0_15 = [feature for feature, score in selected_features_with_scores if abs(score[0]) > 0.15]\n",
    "\n",
    "# Include the 'Difference' variable in the selected features\n",
    "selected_features_greater_than_0_15.append('Difference')\n",
    "\n",
    "# Create a subset of 'second_dataset' with the specified variables\n",
    "subset_second_dataset = second_dataset[selected_features_greater_than_0_15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "431975b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 192\n",
      "Validation set size: 24\n",
      "Test set size: 24\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation, and test sets\n",
    "train, valid = train_test_split(subset_second_dataset, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Split the combined set into validation and test sets\n",
    "valid, test = train_test_split(valid, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Verify the sizes of each set\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Validation set size:\", len(valid))\n",
    "print(\"Test set size:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bed45fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_valid = valid.drop(columns=['Difference'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_test = test.drop(columns=['Difference'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "670dec1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Featurees Used: 68\n",
      "Random Seed Used: 1649622067\n"
     ]
    }
   ],
   "source": [
    "# List to store random seeds used in each iteration\n",
    "random_seeds = []\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Function to calculate accuracy based on a threshold\n",
    "def calculate_accuracy(y_true, y_pred, threshold):\n",
    "    num_samples = len(y_true)\n",
    "    correct_predictions = sum(abs(y_true - y_pred) <= threshold)\n",
    "    return correct_predictions / num_samples\n",
    "\n",
    "best_accuracy = -1.0\n",
    "optimal_cv = None\n",
    "best_y_train_pred = None\n",
    "best_y_valid_pred = None\n",
    "threshold = 0.1  # Define your desired threshold here\n",
    "\n",
    "for cv in range(2, 11):  # Try cross-validation folds from 2 to 10\n",
    "    grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=cv, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model with optimal hyperparameters\n",
    "    subset_second_rf_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predict y_train_pred on the training set\n",
    "    y_train_pred = subset_second_rf_model.predict(X_train)\n",
    "\n",
    "    # Predict y_valid_pred on the validation set\n",
    "    y_valid_pred = subset_second_rf_model.predict(X_valid)\n",
    "\n",
    "    # Round the predicted values to the nearest possible value\n",
    "    y_train_pred = [round_to_nearest(val, possible_values) for val in y_train_pred]\n",
    "    y_valid_pred = [round_to_nearest(val, possible_values) for val in y_valid_pred]\n",
    "\n",
    "    # Calculate accuracy for training and validation sets after rounding\n",
    "    accuracy_train = calculate_accuracy(y_train, y_train_pred, threshold)\n",
    "    accuracy_valid = calculate_accuracy(y_valid, y_valid_pred, threshold)\n",
    "    \n",
    "    # Check if the accuracy after rounding is higher than the best accuracy so far\n",
    "    if accuracy_valid > best_accuracy:\n",
    "        best_accuracy = accuracy_valid\n",
    "        optimal_cv = cv\n",
    "        best_y_train_pred = y_train_pred\n",
    "        best_y_valid_pred = y_valid_pred\n",
    "\n",
    "    # Store the random seed used in this iteration\n",
    "    random_seeds.append(np.random.get_state()[1][0])\n",
    "\n",
    "# Use the optimal number of folds in GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=optimal_cv, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with optimal hyperparameters\n",
    "subset_second_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Get the feature importances from the best model\n",
    "feature_importances = subset_second_rf_model.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "# Print the number of features used in the last iteration\n",
    "print(\"Number of Featurees Used:\", num_features_used)\n",
    "\n",
    "# Extract the random seed from the last element of the random_seeds list\n",
    "random_seed_used = random_seeds[-1]\n",
    "\n",
    "# Print the random seed used in the last iteration\n",
    "print(\"Random Seed Used:\", random_seed_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c1ee78d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------------+\n",
      "| Metric       |   Training Set |   Validation Set |\n",
      "+==============+================+==================+\n",
      "| MSE          |     0.00585938 |        0.0286458 |\n",
      "+--------------+----------------+------------------+\n",
      "| RMSE         |     0.0765466  |        0.169251  |\n",
      "+--------------+----------------+------------------+\n",
      "| R^2          |     0.853652   |        0.559265  |\n",
      "+--------------+----------------+------------------+\n",
      "| Adjusted R^2 |     0.768988   |        1.21568   |\n",
      "+--------------+----------------+------------------+\n",
      "| Accuracy     |     0.90625    |        0.666667  |\n",
      "+--------------+----------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(y_true, y_pred, threshold=None):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(y_true)\n",
    "    \n",
    "    if threshold is not None:\n",
    "        for true_val, pred_val in zip(y_true, y_pred):\n",
    "            if abs(true_val - pred_val) <= threshold:\n",
    "                correct_predictions += 1\n",
    "    else:\n",
    "        for true_val, pred_val in zip(y_true, y_pred):\n",
    "            if true_val == pred_val:\n",
    "                correct_predictions += 1\n",
    "            \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, best_y_train_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, best_y_valid_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, best_y_train_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, best_y_valid_pred)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]\n",
    "p_train = X_train.shape[1]\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]\n",
    "p_valid = X_valid.shape[1]\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "\n",
    "# Calculate accuracy for training and validation sets with the threshold\n",
    "accuracy_train = calculate_accuracy(y_train, best_y_train_pred, threshold)\n",
    "accuracy_valid = calculate_accuracy(y_valid, best_y_valid_pred, threshold)\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = [\n",
    "    [\"MSE\", mse_train, mse_valid],\n",
    "    [\"RMSE\", rmse_train, rmse_valid],\n",
    "    [\"R^2\", r2_train, r2_valid],\n",
    "    [\"Adjusted R^2\", adj_r2_train, adj_r2_valid],\n",
    "    [\"Accuracy\", accuracy_train, accuracy_valid],\n",
    "]\n",
    "\n",
    "# Prepare the headers for the table\n",
    "headers = [\"Metric\", \"Training Set\", \"Validation Set\"]\n",
    "\n",
    "# Display the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "39f80ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Short-Term Treasury Diff  LAG_RollingMean       LAG  Nonfarm Payroll  \\\n",
      "0                     -0.02         0.384181  0.469678         110570.0   \n",
      "1                      0.02        -0.094047 -0.028876         111060.0   \n",
      "2                      0.06         0.180436  0.100506         111209.0   \n",
      "3                     -0.06         0.581703  1.673479         111455.0   \n",
      "4                      0.00         1.171385  1.740168         111989.0   \n",
      "\n",
      "   Long-Term Treasury Bond Rate  Level  Date  Difference  \\\n",
      "0                          6.04   3.75     0         0.0   \n",
      "1                          5.81   3.25    50        -0.5   \n",
      "2                          5.68   3.25    91         0.0   \n",
      "3                          5.36   3.25   126         0.0   \n",
      "4                          5.72   3.25   182         0.0   \n",
      "\n",
      "   Proportion Negative Words_x_Short-Term_Treasury_Diff  \\\n",
      "0                                          -0.000525      \n",
      "1                                           0.000588      \n",
      "2                                           0.001951      \n",
      "3                                          -0.001608      \n",
      "4                                           0.000000      \n",
      "\n",
      "   Proportion Negative Words_x_LAG_RollingMean  ...  \\\n",
      "0                                     0.010082  ...   \n",
      "1                                    -0.002763  ...   \n",
      "2                                     0.005866  ...   \n",
      "3                                     0.015590  ...   \n",
      "4                                     0.031777  ...   \n",
      "\n",
      "   tightening_x_Short-Term_Treasury_Diff  tightening_x_LAG_RollingMean  \\\n",
      "0                                   0.10                     -1.920904   \n",
      "1                                  -0.06                      0.282142   \n",
      "2                                  -0.18                     -0.541308   \n",
      "3                                  -0.00                      0.000000   \n",
      "4                                  -0.00                     -2.342769   \n",
      "\n",
      "   tightening_x_LAG  tightening_x_Nonfarm_Payroll  tightening_x_Level  \\\n",
      "0         -2.348390                     -552850.0              -18.75   \n",
      "1          0.086629                     -333180.0               -9.75   \n",
      "2         -0.301519                     -333627.0               -9.75   \n",
      "3          0.000000                           0.0                0.00   \n",
      "4         -3.480337                     -223978.0               -6.50   \n",
      "\n",
      "   CEI_RollingMean_x_Short-Term_Treasury_Diff  \\\n",
      "0                                    0.004824   \n",
      "1                                   -0.004875   \n",
      "2                                   -0.003942   \n",
      "3                                   -0.007653   \n",
      "4                                    0.000000   \n",
      "\n",
      "   CEI_RollingMean_x_LAG_RollingMean  CEI_RollingMean_x_LAG  \\\n",
      "0                          -0.092666              -0.113288   \n",
      "1                           0.022922               0.007038   \n",
      "2                          -0.011855              -0.006603   \n",
      "3                           0.074198               0.213456   \n",
      "4                           0.365598               0.543119   \n",
      "\n",
      "   CEI_RollingMean_x_Nonfarm_Payroll  CEI_RollingMean_x_Level  \n",
      "0                      -26669.964119                -0.904516  \n",
      "1                      -27068.704893                -0.792124  \n",
      "2                       -7306.430674                -0.213525  \n",
      "3                       14216.342308                 0.414545  \n",
      "4                       34952.596084                 1.014349  \n",
      "\n",
      "[5 rows x 323 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a third subset incorporating designated variables from subset_second_dataset\n",
    "third_subset = subset_second_dataset[['Short-Term Treasury Diff', 'LAG_RollingMean', 'LAG', 'Nonfarm Payroll', 'Long-Term Treasury Bond Rate', 'Level', 'Date', 'Difference']].copy()\n",
    "\n",
    "# List of vectorized terms (previously sentiment columns)\n",
    "vectorized_terms = [col for col in subset_second_dataset.columns if col not in ['Short-Term Treasury Diff', 'LAG_RollingMean', 'LAG', 'Nonfarm Payroll', 'Long-Term Treasury Bond Rate', 'Level', 'Date', 'Difference']]\n",
    "\n",
    "# Create a list to store DataFrames of new columns\n",
    "new_columns_dfs = []\n",
    "\n",
    "# Introduce new variables by computing the product of vectorized terms with specified columns\n",
    "for term in vectorized_terms:\n",
    "    for column in ['Short-Term Treasury Diff', 'LAG_RollingMean', 'LAG', 'Nonfarm Payroll', 'Level']:\n",
    "        new_column_name = f'{term}_x_{column.replace(\" \", \"_\")}'\n",
    "        if term in subset_second_dataset.columns:\n",
    "            new_column = subset_second_dataset[term] * subset_second_dataset[column]\n",
    "            new_columns_dfs.append(pd.DataFrame({new_column_name: new_column}))\n",
    "\n",
    "# Concatenate the new columns DataFrames along the columns axis\n",
    "new_columns_df = pd.concat(new_columns_dfs, axis=1)\n",
    "\n",
    "# Combine the new columns DataFrame with the existing 'third_subset'\n",
    "third_subset = pd.concat([third_subset, new_columns_df], axis=1)\n",
    "\n",
    "# Append the 'Date' variable from subset_second_dataset at this stage\n",
    "third_subset['Date'] = subset_second_dataset['Date']\n",
    "\n",
    "# Display initial records of the third subset\n",
    "print(third_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e679662e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 192\n",
      "Validation set size: 24\n",
      "Test set size: 24\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation, and test sets\n",
    "train, valid = train_test_split(third_subset, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Split the combined set into validation and test sets\n",
    "valid, test = train_test_split(valid, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Verify the sizes of each set\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Validation set size:\", len(valid))\n",
    "print(\"Test set size:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d6f9d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_valid = valid.drop(columns=['Difference'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_test = test.drop(columns=['Difference'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79acb4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store random seeds used in each iteration\n",
    "random_seeds = []\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Function to calculate accuracy based on a threshold\n",
    "def calculate_accuracy(y_true, y_pred, threshold):\n",
    "    num_samples = len(y_true)\n",
    "    correct_predictions = sum(abs(y_true - y_pred) <= threshold)\n",
    "    return correct_predictions / num_samples\n",
    "\n",
    "best_accuracy = -1.0\n",
    "optimal_cv = None\n",
    "best_y_train_pred = None\n",
    "best_y_valid_pred = None\n",
    "threshold = 0.1  # Define your desired threshold here\n",
    "\n",
    "for cv in range(2, 11):  # Try cross-validation folds from 2 to 10\n",
    "    grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=cv, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model with optimal hyperparameters\n",
    "    subset_third_rf_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predict y_train_pred on the training set\n",
    "    y_train_pred = subset_third_rf_model.predict(X_train)\n",
    "\n",
    "    # Predict y_valid_pred on the validation set\n",
    "    y_valid_pred = subset_third_rf_model.predict(X_valid)\n",
    "\n",
    "    # Round the predicted values to the nearest possible value\n",
    "    y_train_pred = [round_to_nearest(val, possible_values) for val in y_train_pred]\n",
    "    y_valid_pred = [round_to_nearest(val, possible_values) for val in y_valid_pred]\n",
    "\n",
    "    # Calculate accuracy for training and validation sets after rounding\n",
    "    accuracy_train = calculate_accuracy(y_train, y_train_pred, threshold)\n",
    "    accuracy_valid = calculate_accuracy(y_valid, y_valid_pred, threshold)\n",
    "    \n",
    "    # Check if the accuracy after rounding is higher than the best accuracy so far\n",
    "    if accuracy_valid > best_accuracy:\n",
    "        best_accuracy = accuracy_valid\n",
    "        optimal_cv = cv\n",
    "        best_y_train_pred = y_train_pred\n",
    "        best_y_valid_pred = y_valid_pred\n",
    "\n",
    "    # Store the random seed used in this iteration\n",
    "    random_seeds.append(np.random.get_state()[1][0])\n",
    "\n",
    "# Use the optimal number of folds in GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=optimal_cv, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with optimal hyperparameters\n",
    "subset_third_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Get the feature importances from the best model\n",
    "feature_importances = subset_third_rf_model.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "# Print the number of features used in the last iteration\n",
    "print(\"Number of Features Used:\", num_features_used)\n",
    "\n",
    "# Extract the random seed from the last element of the random_seeds list\n",
    "random_seed_used = random_seeds[-1]\n",
    "\n",
    "# Print the random seed used in the last iteration\n",
    "print(\"Random Seed Used:\", random_seed_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f110bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred, threshold=None):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(y_true)\n",
    "    \n",
    "    if threshold is not None:\n",
    "        for true_val, pred_val in zip(y_true, y_pred):\n",
    "            if abs(true_val - pred_val) <= threshold:\n",
    "                correct_predictions += 1\n",
    "    else:\n",
    "        for true_val, pred_val in zip(y_true, y_pred):\n",
    "            if true_val == pred_val:\n",
    "                correct_predictions += 1\n",
    "            \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, best_y_train_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, best_y_valid_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, best_y_train_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, best_y_valid_pred)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]\n",
    "p_train = X_train.shape[1]\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]\n",
    "p_valid = X_valid.shape[1]\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "\n",
    "# Calculate accuracy for training and validation sets with the threshold\n",
    "accuracy_train = calculate_accuracy(y_train, best_y_train_pred, threshold)\n",
    "accuracy_valid = calculate_accuracy(y_valid, best_y_valid_pred, threshold)\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = [\n",
    "    [\"MSE\", mse_train, mse_valid],\n",
    "    [\"RMSE\", rmse_train, rmse_valid],\n",
    "    [\"R^2\", r2_train, r2_valid],\n",
    "    [\"Adjusted R^2\", adj_r2_train, adj_r2_valid],\n",
    "    [\"Accuracy\", accuracy_train, accuracy_valid],\n",
    "]\n",
    "\n",
    "# Prepare the headers for the table\n",
    "headers = [\"Metric\", \"Training Set\", \"Validation Set\"]\n",
    "\n",
    "# Display the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9f1ab1a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subset_third_rf_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get the feature importances from the best model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m feature_importances \u001b[38;5;241m=\u001b[39m \u001b[43msubset_third_rf_model\u001b[49m\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Get the selected feature indices\u001b[39;00m\n\u001b[0;32m      5\u001b[0m selected_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(feature_importances \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'subset_third_rf_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the feature importances from the best model\n",
    "feature_importances = subset_third_rf_model.feature_importances_\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_indices = np.where(feature_importances > 0)[0]\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_features_names = X_train.columns[selected_indices]\n",
    "\n",
    "# Print the selected feature names\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features_names:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d396fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features with importance greater than the absolute value of 0.2\n",
    "selected_features_greater_than_0_2 = [feature for feature, score in selected_features_with_scores if abs(score[0]) > 0.2]\n",
    "\n",
    "# Include the 'Difference' variable in the selected features\n",
    "selected_features_greater_than_0_2.append('Difference')\n",
    "\n",
    "# Create a subset of 'second_dataset' with the specified variables\n",
    "subset_second_dataset = second_dataset[selected_features_greater_than_0_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training, validation, and test sets\n",
    "train, valid = train_test_split(subset_second_dataset, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Split the combined set into validation and test sets\n",
    "valid, test = train_test_split(valid, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Verify the sizes of each set\n",
    "print(\"Training set size:\", len(train))\n",
    "print(\"Validation set size:\", len(valid))\n",
    "print(\"Test set size:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference'])\n",
    "y_train = train['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_valid = valid.drop(columns=['Difference'])\n",
    "y_valid = valid['Difference']  # Use the 'Difference' variable as the target\n",
    "\n",
    "X_test = test.drop(columns=['Difference'])\n",
    "y_test = test['Difference']  # Use the 'Difference' variable as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee5dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store random seeds used in each iteration\n",
    "random_seeds = []\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Function to calculate accuracy based on a threshold\n",
    "def calculate_accuracy(y_true, y_pred, threshold):\n",
    "    num_samples = len(y_true)\n",
    "    correct_predictions = sum(abs(y_true - y_pred) <= threshold)\n",
    "    return correct_predictions / num_samples\n",
    "\n",
    "best_accuracy = -1.0\n",
    "optimal_cv = None\n",
    "best_y_train_pred = None\n",
    "best_y_test_pred = None  # Changed variable name from best_y_valid_pred\n",
    "threshold = 0.1  # Define your desired threshold here\n",
    "\n",
    "for cv in range(2, 11):  # Try cross-validation folds from 2 to 10\n",
    "    grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=cv, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model with optimal hyperparameters\n",
    "    subset_third_rf_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predict y_train_pred on the training set\n",
    "    y_train_pred = subset_third_rf_model.predict(X_train)\n",
    "\n",
    "    # Predict y_test_pred on the test set  # Changed variable name from y_valid_pred\n",
    "    y_test_pred = subset_third_rf_model.predict(X_test)  # Use 'test' set instead of 'valid'\n",
    "\n",
    "    # Round the predicted values to the nearest possible value\n",
    "    y_train_pred = [round_to_nearest(val, possible_values) for val in y_train_pred]\n",
    "    y_test_pred = [round_to_nearest(val, possible_values) for val in y_test_pred]\n",
    "\n",
    "    # Calculate accuracy for training and test sets after rounding\n",
    "    accuracy_train = calculate_accuracy(y_train, y_train_pred, threshold)\n",
    "    accuracy_test = calculate_accuracy(y_test, y_test_pred, threshold)  # Use 'test' set instead of 'valid'\n",
    "    \n",
    "    # Check if the accuracy after rounding is higher than the best accuracy so far\n",
    "    if accuracy_test > best_accuracy:  # Change from accuracy_valid to accuracy_test\n",
    "        best_accuracy = accuracy_test  # Change from accuracy_valid to accuracy_test\n",
    "        optimal_cv = cv\n",
    "        best_y_train_pred = y_train_pred\n",
    "        best_y_test_pred = y_test_pred  # Changed variable name from best_y_valid_pred\n",
    "\n",
    "    # Store the random seed used in this iteration\n",
    "    random_seeds.append(np.random.get_state()[1][0])\n",
    "\n",
    "# Use the optimal number of folds in GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=optimal_cv, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with optimal hyperparameters\n",
    "subset_third_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Get the feature importances from the best model\n",
    "feature_importances = subset_third_rf_model.feature_importances_\n",
    "\n",
    "# Get the number of features used (non-zero feature importances)\n",
    "num_features_used = np.sum(feature_importances > 0)\n",
    "\n",
    "# Print the number of features used in the last iteration\n",
    "print(\"Number of Features Used:\", num_features_used)\n",
    "\n",
    "# Extract the random seed from the last element of the random_seeds list\n",
    "random_seed_used = random_seeds[-1]\n",
    "\n",
    "# Print the random seed used in the last iteration\n",
    "print(\"Random Seed Used:\", random_seed_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399509d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred, threshold=None):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(y_true)\n",
    "    \n",
    "    if threshold is not None:\n",
    "        for true_val, pred_val in zip(y_true, y_pred):\n",
    "            if abs(true_val - pred_val) <= threshold:\n",
    "                correct_predictions += 1\n",
    "    else:\n",
    "        for true_val, pred_val in zip(y_true, y_pred):\n",
    "            if true_val == pred_val:\n",
    "                correct_predictions += 1\n",
    "            \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, best_y_train_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, best_y_valid_pred)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for test set\n",
    "mse_test = mean_squared_error(y_test, best_y_test_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for test set\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, best_y_train_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, best_y_valid_pred)\n",
    "\n",
    "# Calculate R-squared (R^2) for test set\n",
    "r2_test = r2_score(y_test, best_y_test_pred)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]\n",
    "p_train = X_train.shape[1]\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]\n",
    "p_valid = X_valid.shape[1]\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "\n",
    "# Calculate adjusted R-squared for test set\n",
    "n_test = X_test.shape[0]\n",
    "p_test = X_test.shape[1]\n",
    "adj_r2_test = 1 - ((1 - r2_test) * (n_test - 1) / (n_test - p_test - 1))\n",
    "\n",
    "# Calculate accuracy for training, validation, and test sets with the threshold\n",
    "accuracy_train = calculate_accuracy(y_train, best_y_train_pred, threshold)\n",
    "accuracy_valid = calculate_accuracy(y_valid, best_y_valid_pred, threshold)\n",
    "accuracy_test = calculate_accuracy(y_test, best_y_test_pred, threshold)\n",
    "\n",
    "# Prepare the data for the table\n",
    "data = [\n",
    "    [\"MSE\", mse_train, mse_valid, mse_test],\n",
    "    [\"RMSE\", rmse_train, rmse_valid, rmse_test],\n",
    "    [\"R^2\", r2_train, r2_valid, r2_test],\n",
    "    [\"Adjusted R^2\", adj_r2_train, adj_r2_valid, adj_r2_test],\n",
    "    [\"Accuracy\", accuracy_train, accuracy_valid, accuracy_test],\n",
    "]\n",
    "\n",
    "# Prepare the headers for the table\n",
    "headers = [\"Metric\", \"Training Set\", \"Validation Set\", \"Test Set\"]\n",
    "\n",
    "# Display the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc9a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best_rf_model to the specified location\n",
    "model_filename = r\"C:\\Users\\abact\\BC-Project\\models\\best_random_forest_model_data.joblib\"\n",
    "joblib.dump(subset_third_rf_model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f9b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances from the best model\n",
    "feature_importances = subset_third_rf_model.feature_importances_\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_indices = np.where(feature_importances > 0)[0]\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_features_names = X_train.columns[selected_indices]\n",
    "\n",
    "# Print the selected feature names\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features_names:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "75e55f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save X_train as a CSV file\n",
    "X_train.to_csv(r\"C:\\Users\\abact\\BC-Project\\data\\processed\\X_train.csv\", index=False)\n",
    "\n",
    "# Save u_train as a CSV file\n",
    "y_train.to_csv(r\"C:\\Users\\abact\\BC-Project\\data\\processed\\y_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7353b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save X_valid as a CSV file\n",
    "X_valid.to_csv(r\"C:\\Users\\abact\\BC-Project\\data\\processed\\X_valid.csv\", index=False)\n",
    "\n",
    "# Save X_test as a CSV file\n",
    "X_test.to_csv(r\"C:\\Users\\abact\\BC-Project\\data\\processed\\X_test.csv\", index=False)\n",
    "\n",
    "# Save y_valid as a CSV file\n",
    "y_valid.to_csv(r\"C:\\Users\\abact\\BC-Project\\data\\processed\\y_valid.csv\", index=False)\n",
    "\n",
    "# Save y_test as a CSV file\n",
    "y_test.to_csv(r\"C:\\Users\\abact\\BC-Project\\data\\processed\\y_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
