{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00fafee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mord in c:\\users\\abact\\appdata\\roaming\\python\\python310\\site-packages (0.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xgboost in c:\\users\\abact\\appdata\\roaming\\python\\python310\\site-packages (1.7.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\abact\\appdata\\roaming\\python\\python310\\site-packages (from xgboost) (1.22.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\abact\\appdata\\roaming\\python\\python310\\site-packages (from xgboost) (1.7.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting shap\n",
      "  Downloading shap-0.42.1-cp310-cp310-win_amd64.whl (462 kB)\n",
      "     -------------------------------------- 462.3/462.3 kB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (22.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\abact\\appdata\\roaming\\python\\python310\\site-packages (from shap) (1.22.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\abact\\appdata\\roaming\\python\\python310\\site-packages (from shap) (1.7.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (4.64.1)\n",
      "Collecting slicer==0.0.7\n",
      "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numba in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (0.56.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (1.2.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (1.5.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in c:\\programdata\\anaconda3\\lib\\site-packages (from numba->shap) (0.39.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from numba->shap) (65.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->shap) (2022.7)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abact\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas->shap) (1.12.0)\n",
      "Installing collected packages: slicer, shap\n",
      "Successfully installed shap-0.42.1 slicer-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install mord\n",
    "!pip install xgboost\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3f914cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from tpot import TPOTClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mord import OrdinalRidge\n",
    "import xgboost as xgb\n",
    "from keras.layers import Input\n",
    "from keras_tuner import RandomSearch\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "53fe5796",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://github.com/abactat/BC-Project/blob/main/data/processed/train_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:713\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    710\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[1;32m--> 713\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    722\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:363\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[0;32m    362\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[0;32m    364\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    366\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:265\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\urllib\\request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "url = (r'https://github.com/abactat/BC-Project/blob/main/data/processed/train_data.csv')\n",
    "train = pd.read_csv(url,index_col=0,parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "534b5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'C:\\Users\\abact\\BC-Project\\data\\train_data.csv')\n",
    "valid = pd.read_csv(r'C:\\Users\\abact\\BC-Project\\data\\val_data.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\abact\\BC-Project\\data\\test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d950f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert variables to numeric in the train dataset\n",
    "variables_to_convert = train.columns.drop('Date')\n",
    "train[variables_to_convert] = train[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert variables to numeric in the valid dataset\n",
    "valid[variables_to_convert] = valid[variables_to_convert].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea184cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Direction'] = np.select([\n",
    "    train['Difference'] > 0,\n",
    "    train['Difference'] < 0,\n",
    "    train['Difference'] == 0\n",
    "], [\n",
    "    1,  # Increase\n",
    "    -1,  # Decrease\n",
    "    0   # Hold\n",
    "], default=-1)\n",
    "\n",
    "valid['Direction'] = np.select([\n",
    "    valid['Difference'] > 0,\n",
    "    valid['Difference'] < 0,\n",
    "    valid['Difference'] == 0\n",
    "], [\n",
    "    1,  # Increase\n",
    "    -1,  # Decrease\n",
    "    0   # Hold\n",
    "], default=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "047ff245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Increase', 'Decrease', 'Level', 'CPI', 'Unemployment Rate',\n",
      "       'Retail Sales', 'Durable Goods Orders', 'Short-Term Treasury Bond Rate',\n",
      "       'Long-Term Treasury Bond Rate', 'Bank Reserves', 'LEI', 'CEI', 'LAG',\n",
      "       'Treasury Deposits', 'Average Hourly Earnings', 'Net Sentiment Score',\n",
      "       'Positive Frequency', 'Negative Frequency', 'Word Count',\n",
      "       'Standardized Sentiment Score', 'Short-Term Treasury Diff',\n",
      "       'LEI_RollingMean', 'CEI_RollingMean', 'LAG_RollingMean',\n",
      "       'Retail Sales_RollingMean', 'Durable Goods Orders_RollingMean',\n",
      "       'almost', 'anticipated', 'anticipating', 'apparent', 'apparently',\n",
      "       'appeared', 'appearing', 'assumed', 'assumption', 'attain',\n",
      "       'attractive', 'attractiveness', 'believed', 'better', 'bolstered',\n",
      "       'bolstering', 'cautious', 'cautiously', 'could', 'depend', 'easier',\n",
      "       'encouragement', 'favorable', 'gaining', 'good', 'improved',\n",
      "       'improvement', 'leading', 'might', 'nearly', 'opportunity',\n",
      "       'optimistic', 'positive', 'possibility', 'possibly', 'presumed',\n",
      "       'profitability', 'progress', 'progressed', 'prosperity', 'rebound',\n",
      "       'rebounded', 'rebounding', 'regain', 'risk', 'roughly', 'satisfactory',\n",
      "       'smooth', 'sometime', 'stabilization', 'stabilized', 'stable',\n",
      "       'strengthening', 'stronger', 'strongest', 'succeeded', 'sudden',\n",
      "       'suggested', 'suggesting', 'surpassing', 'tremendous', 'uncertain',\n",
      "       'uncertainty', 'unclear', 'unexpected', 'unknown', 'upturn', 'vary',\n",
      "       'volatile', 'volatility', 'Difference', 'Date', 'Direction'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44534f2c",
   "metadata": {},
   "source": [
    "###Directional Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d994a764",
   "metadata": {},
   "source": [
    "###Ordinal Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c5c6e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Mean Squared Error (MSE): 0.5\n",
      "Validation Root Mean Squared Error (RMSE): 0.7071067811865476\n",
      "Validation R^2: -0.22033898305084754\n",
      "Validation Adjusted R^2: 1.3953210790164716\n",
      "Training Mean Squared Error (MSE): 0.17801047120418848\n",
      "Training Root Mean Squared Error (RMSE): 0.4219128715791786\n",
      "Training R^2: 0.39903757171941523\n",
      "Training Adjusted R^2: -0.18940480597199083\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease', 'Direction', 'Date'])\n",
    "y_train = train['Direction']  # Use the 'Direction' variable as the target\n",
    "\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease', 'Direction', 'Date'])\n",
    "y_valid = valid['Direction']  # Use the 'Direction' variable as the target\n",
    "\n",
    "# Center and scale the independent variables\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Initialize and train the ordinal logistic regression model\n",
    "ordinal_model = OrdinalRidge()\n",
    "ordinal_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_valid_pred = ordinal_model.predict(X_valid_scaled)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = ordinal_model.predict(X_train_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, y_valid_pred)\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse_valid)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "print(\"Validation Root Mean Squared Error (RMSE):\", rmse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, y_valid_pred)\n",
    "print(\"Validation R^2:\", r2_valid)\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = len(y_valid)  # Number of samples in validation set\n",
    "p_valid = X_valid.shape[1]  # Number of features in validation set\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "print(\"Validation Adjusted R^2:\", adj_r2_valid)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "print(\"Training Mean Squared Error (MSE):\", mse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "print(\"Training Root Mean Squared Error (RMSE):\", rmse_train)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "print(\"Training R^2:\", r2_train)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = len(y_train)  # Number of samples in training set\n",
    "p_train = X_train.shape[1]  # Number of features in training set\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "print(\"Training Adjusted R^2:\", adj_r2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c0244ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8219895287958116\n",
      "Validation Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the ordinal logistic regression model\n",
    "ordinal_model = OrdinalRidge()\n",
    "ordinal_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_valid_pred = ordinal_model.predict(X_valid_scaled)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = ordinal_model.predict(X_train_scaled)\n",
    "\n",
    "# Convert the predicted values to ordinal categories\n",
    "y_train_pred_category = np.round(y_train_pred).astype(int)\n",
    "y_valid_pred_category = np.round(y_valid_pred).astype(int)\n",
    "\n",
    "# Calculate accuracy for training and validation sets\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred_category)\n",
    "accuracy_valid = accuracy_score(y_valid, y_valid_pred_category)\n",
    "\n",
    "print(\"Training Accuracy:\", accuracy_train)\n",
    "print(\"Validation Accuracy:\", accuracy_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1137cd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      "[ 0.02402709 -0.08662731 -0.15515958  0.14026437 -0.02640148 -0.03135705\n",
      " -0.15349031 -0.01867492 -0.16180556 -0.02017475 -0.01504331 -0.05897942\n",
      " -0.15792497  0.01118825  0.00359184 -0.00099917 -0.08670814  0.01118825\n",
      "  0.31060554  0.03835019  0.02974458  0.0336391   0.30179174 -0.2238576\n",
      " -0.02915485 -0.01829946  0.12587379  0.05849727 -0.02207385  0.09471137\n",
      " -0.1053766  -0.05668885 -0.05741903  0.02104695  0.0189186  -0.01731716\n",
      " -0.01223244  0.11684601 -0.0643666  -0.04351069 -0.06576789 -0.00441159\n",
      "  0.20326871 -0.1015785   0.08127378 -0.07471455  0.00067962 -0.04500762\n",
      "  0.26533477 -0.13220994  0.10739167 -0.00710719 -0.17400346 -0.09744961\n",
      " -0.06866535  0.06749356  0.00143774 -0.07296422 -0.02882997 -0.03363\n",
      " -0.07990339 -0.06529572  0.02111175 -0.1220375  -0.05129406  0.02012124\n",
      " -0.0021426  -0.08774782 -0.03045944  0.04898673 -0.03176268 -0.01020718\n",
      "  0.00399995 -0.05599647 -0.10103282 -0.13480984 -0.15984486 -0.03711265\n",
      "  0.05295604 -0.06143211 -0.07283566 -0.08469576  0.07568411  0.\n",
      "  0.         -0.01257812  0.04300333 -0.01596146 -0.04296638 -0.02640513\n",
      " -0.01476638 -0.0201807   0.04386084  0.07677427]\n",
      "Intercepts:\n",
      "0.04712041884816773\n"
     ]
    }
   ],
   "source": [
    "# Access the coefficients and intercepts\n",
    "coefficients = ordinal_model.coef_\n",
    "intercepts = ordinal_model.intercept_\n",
    "\n",
    "# Display coefficients and intercepts\n",
    "print(\"Coefficients:\")\n",
    "print(coefficients)\n",
    "\n",
    "print(\"Intercepts:\")\n",
    "print(intercepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99238f5",
   "metadata": {},
   "source": [
    "###XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84b39e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mean Squared Error (MSE): 3.2998337238190296e-07\n",
      "Validation Mean Squared Error (MSE): 0.23903423250877073\n",
      "Training Root Mean Squared Error (RMSE): 0.0005744417919875807\n",
      "Validation Root Mean Squared Error (RMSE): 0.48891127263417716\n",
      "Training R^2: 0.9999988859778449\n",
      "Validation R^2: 0.41659441557181376\n",
      "Training Adjusted R^2: 0.9999977951644847\n",
      "Validation Adjusted R^2: 1.1889905414344828\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease', 'Direction', 'Date'])\n",
    "y_train = train['Direction']  # Use the 'Direction' variable as the target\n",
    "\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease', 'Direction', 'Date'])\n",
    "y_valid = valid['Direction']  # Use the 'Direction' variable as the target\n",
    "\n",
    "# Initialize and train the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_valid_pred = xgb_model.predict(X_valid)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training and validation sets\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_valid = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "print(\"Training Mean Squared Error (MSE):\", mse_train)\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse_valid)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training and validation sets\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "print(\"Training Root Mean Squared Error (RMSE):\", rmse_train)\n",
    "print(\"Validation Root Mean Squared Error (RMSE):\", rmse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training and validation sets\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_valid = r2_score(y_valid, y_valid_pred)\n",
    "\n",
    "print(\"Training R^2:\", r2_train)\n",
    "print(\"Validation R^2:\", r2_valid)\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_train = len(y_train)  # Number of samples in training set\n",
    "n_valid = len(y_valid)  # Number of samples in validation set\n",
    "p = X_train.shape[1]  # Number of features in the data\n",
    "\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p - 1))\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p - 1))\n",
    "\n",
    "print(\"Training Adjusted R^2:\", adj_r2_train)\n",
    "print(\"Validation Adjusted R^2:\", adj_r2_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7c72404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "Short-Term Treasury Diff: 0.2838\n",
      "Word Count: 0.1056\n",
      "Short-Term Treasury Bond Rate: 0.1025\n",
      "depend: 0.0745\n",
      "better: 0.0509\n",
      "CEI_RollingMean: 0.0351\n",
      "assumption: 0.0346\n",
      "uncertainty: 0.0330\n",
      "LEI: 0.0317\n",
      "Treasury Deposits: 0.0280\n",
      "encouragement: 0.0276\n",
      "CEI: 0.0253\n",
      "Positive Frequency: 0.0219\n",
      "Level: 0.0216\n",
      "Bank Reserves: 0.0198\n",
      "LAG_RollingMean: 0.0191\n",
      "Unemployment Rate: 0.0174\n",
      "anticipating: 0.0100\n",
      "Average Hourly Earnings: 0.0077\n",
      "Net Sentiment Score: 0.0075\n",
      "Negative Frequency: 0.0066\n",
      "anticipated: 0.0065\n",
      "risk: 0.0057\n",
      "possibility: 0.0043\n",
      "progress: 0.0037\n",
      "appeared: 0.0022\n",
      "suggesting: 0.0018\n",
      "Durable Goods Orders: 0.0013\n",
      "CPI: 0.0013\n",
      "could: 0.0012\n",
      "favorable: 0.0008\n",
      "apparent: 0.0007\n",
      "Retail Sales: 0.0006\n",
      "Durable Goods Orders_RollingMean: 0.0006\n",
      "Long-Term Treasury Bond Rate: 0.0006\n",
      "unclear: 0.0006\n",
      "stabilized: 0.0005\n",
      "believed: 0.0005\n",
      "LEI_RollingMean: 0.0005\n",
      "improvement: 0.0004\n",
      "stronger: 0.0004\n",
      "bolstered: 0.0003\n",
      "good: 0.0002\n",
      "volatile: 0.0002\n",
      "uncertain: 0.0001\n",
      "possibly: 0.0001\n",
      "unexpected: 0.0001\n",
      "positive: 0.0001\n",
      "opportunity: 0.0000\n",
      "satisfactory: 0.0000\n",
      "cautious: 0.0000\n",
      "nearly: 0.0000\n",
      "apparently: 0.0000\n",
      "rebounded: 0.0000\n",
      "stable: 0.0000\n",
      "stabilization: 0.0000\n",
      "roughly: 0.0000\n",
      "optimistic: 0.0000\n",
      "progressed: 0.0000\n",
      "might: 0.0000\n",
      "prosperity: 0.0000\n",
      "rebound: 0.0000\n",
      "rebounding: 0.0000\n",
      "strengthening: 0.0000\n",
      "improved: 0.0000\n",
      "upturn: 0.0000\n",
      "leading: 0.0000\n",
      "almost: 0.0000\n",
      "suggested: 0.0000\n",
      "easier: 0.0000\n",
      "gaining: 0.0000\n",
      "LAG: 0.0000\n",
      "Standardized Sentiment Score: 0.0000\n",
      "Retail Sales_RollingMean: 0.0000\n",
      "appearing: 0.0000\n",
      "assumed: 0.0000\n",
      "attain: 0.0000\n",
      "attractive: 0.0000\n",
      "attractiveness: 0.0000\n",
      "bolstering: 0.0000\n",
      "cautiously: 0.0000\n",
      "presumed: 0.0000\n",
      "profitability: 0.0000\n",
      "regain: 0.0000\n",
      "smooth: 0.0000\n",
      "sometime: 0.0000\n",
      "strongest: 0.0000\n",
      "succeeded: 0.0000\n",
      "sudden: 0.0000\n",
      "surpassing: 0.0000\n",
      "tremendous: 0.0000\n",
      "unknown: 0.0000\n",
      "vary: 0.0000\n",
      "volatility: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "\n",
    "# Create a dictionary to map feature names to their importance scores\n",
    "feature_importance_dict = {feature_name: importance_score for feature_name, importance_score in zip(X_train.columns, feature_importance)}\n",
    "\n",
    "# Sort the feature importance dictionary in descending order based on importance scores\n",
    "sorted_feature_importance = dict(sorted(feature_importance_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Print the feature importance scores in descending order\n",
    "print(\"Feature Importance:\")\n",
    "for feature_name, importance_score in sorted_feature_importance.items():\n",
    "    print(f\"{feature_name}: {importance_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c8fbf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n",
      "Validation Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_valid_pred = xgb_model.predict(X_valid)\n",
    "\n",
    "# Convert the predicted values to ordinal categories\n",
    "y_train_pred_category = np.round(y_train_pred).astype(int)\n",
    "y_valid_pred_category = np.round(y_valid_pred).astype(int)\n",
    "\n",
    "# Calculate accuracy for training and validation sets\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred_category)\n",
    "accuracy_valid = accuracy_score(y_valid, y_valid_pred_category)\n",
    "\n",
    "print(\"Training Accuracy:\", accuracy_train)\n",
    "print(\"Validation Accuracy:\", accuracy_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6523315",
   "metadata": {},
   "source": [
    "###RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e59ff4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 [==============================] - 8s 350ms/step - loss: 0.9299 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.5711 - accuracy: 1.0000 - val_loss: 0.3338 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.2061 - accuracy: 1.0000 - val_loss: 0.0650 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.5739e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 8.9700e-04 - accuracy: 1.0000 - val_loss: 8.1923e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 7.8234e-04 - accuracy: 1.0000 - val_loss: 7.3299e-04 - val_accuracy: 1.0000\n",
      "6/6 [==============================] - 2s 8ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Training Mean Squared Error (MSE): 0.0\n",
      "Validation Mean Squared Error (MSE): 0.0\n",
      "Training Root Mean Squared Error (RMSE): 0.0\n",
      "Validation Root Mean Squared Error (RMSE): 0.0\n",
      "Training R^2: 1.0\n",
      "Validation R^2: 1.0\n",
      "Training Adjusted R^2: 1.0\n",
      "Validation Adjusted R^2: 1.0\n",
      "Training Accuracy: 1.0\n",
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "X_train = train.drop(columns=['Direction', 'Increase', 'Decrease', 'Difference', 'Date'])\n",
    "y_train = train['Direction']\n",
    "\n",
    "X_valid = valid.drop(columns=['Direction', 'Increase', 'Decrease', 'Difference', 'Date'])\n",
    "y_valid = valid['Direction']\n",
    "\n",
    "# Encode the ordinal 'Direction' variable to numerical values (e.g., -1, 0, 1)\n",
    "y_train_encoded = pd.Series(np.where(y_train == 'Decrease', -1, np.where(y_train == 'Hold', 0, 1)))\n",
    "y_valid_encoded = pd.Series(np.where(y_valid == 'Decrease', -1, np.where(y_valid == 'Hold', 0, 1)))\n",
    "\n",
    "# Center and scale the independent variables\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Build the RNN model with additional modifications to obtain feature importances\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Int('lstm_units', min_value=32, max_value=128, step=32),\n",
    "                   input_shape=(sequence_length, 1),\n",
    "                   return_sequences=True))\n",
    "    model.add(LSTM(units=hp.Int('lstm_units', min_value=16, max_value=64, step=16)))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4])),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Prepare the input sequences for the RNN model (assuming sequence_length is defined)\n",
    "def prepare_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length + 1):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(data[i+sequence_length-1])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "sequence_length = 10  # Define the length of the input sequence\n",
    "X_train_seq, y_train_seq = prepare_sequences(y_train_encoded, sequence_length)\n",
    "X_valid_seq, y_valid_seq = prepare_sequences(y_valid_encoded, sequence_length)\n",
    "\n",
    "# Build the RNN model with additional modifications to obtain feature importances\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(sequence_length, 1), return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32, validation_data=(X_valid_seq, y_valid_seq))\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_train_pred = model.predict(X_train_seq).argmax(axis=1)\n",
    "y_valid_pred = model.predict(X_valid_seq).argmax(axis=1)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training and validation sets\n",
    "mse_train = mean_squared_error(y_train_seq, y_train_pred)\n",
    "mse_valid = mean_squared_error(y_valid_seq, y_valid_pred)\n",
    "print(\"Training Mean Squared Error (MSE):\", mse_train)\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse_valid)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training and validation sets\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "print(\"Training Root Mean Squared Error (RMSE):\", rmse_train)\n",
    "print(\"Validation Root Mean Squared Error (RMSE):\", rmse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training and validation sets\n",
    "r2_train = r2_score(y_train_seq, y_train_pred)\n",
    "r2_valid = r2_score(y_valid_seq, y_valid_pred)\n",
    "print(\"Training R^2:\", r2_train)\n",
    "print(\"Validation R^2:\", r2_valid)\n",
    "\n",
    "# Calculate the number of samples and features\n",
    "n_train, p_train = X_train_seq.shape[0], X_train_seq.shape[1]\n",
    "n_valid, p_valid = X_valid_seq.shape[0], X_valid_seq.shape[1]\n",
    "\n",
    "# Calculate adjusted R-squared for training and validation sets\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "print(\"Training Adjusted R^2:\", adj_r2_train)\n",
    "print(\"Validation Adjusted R^2:\", adj_r2_valid)\n",
    "\n",
    "# Show training and validation set accuracy\n",
    "accuracy_train = (y_train_seq == y_train_pred).mean()\n",
    "accuracy_valid = (y_valid_seq == y_valid_pred).mean()\n",
    "print(\"Training Accuracy:\", accuracy_train)\n",
    "print(\"Validation Accuracy:\", accuracy_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5a82421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 10s]\n",
      "val_accuracy: 1.0\n",
      "\n",
      "Best val_accuracy So Far: 1.0\n",
      "Total elapsed time: 00h 00m 55s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 10, 96)            37632     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 96)                74112     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 291       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 112,035\n",
      "Trainable params: 112,035\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 7s 256ms/step - loss: 1.0472 - accuracy: 1.0000 - val_loss: 1.0139 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.9924 - accuracy: 1.0000 - val_loss: 0.9590 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.9367 - accuracy: 1.0000 - val_loss: 0.9017 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.8779 - accuracy: 1.0000 - val_loss: 0.8399 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.8133 - accuracy: 1.0000 - val_loss: 0.7708 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.7407 - accuracy: 1.0000 - val_loss: 0.6922 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.6578 - accuracy: 1.0000 - val_loss: 0.6024 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.5634 - accuracy: 1.0000 - val_loss: 0.5010 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.4581 - accuracy: 1.0000 - val_loss: 0.3909 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.3467 - accuracy: 1.0000 - val_loss: 0.2797 - val_accuracy: 1.0000\n",
      "6/6 [==============================] - 1s 8ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "Training Mean Squared Error (MSE): 0.0\n",
      "Validation Mean Squared Error (MSE): 0.0\n",
      "Training Root Mean Squared Error (RMSE): 0.0\n",
      "Validation Root Mean Squared Error (RMSE): 0.0\n",
      "Training R^2: 1.0\n",
      "Validation R^2: 1.0\n",
      "Training Adjusted R^2: 1.0\n",
      "Validation Adjusted R^2: 1.0\n",
      "Training Accuracy: 1.0\n",
      "Validation Accuracy: 1.0\n",
      "Best Hyperparameters:\n",
      "{'lstm_units': 96, 'learning_rate': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "# Build the RNN model with additional modifications to obtain feature importances\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Int('lstm_units', min_value=32, max_value=128, step=32),\n",
    "                   input_shape=(sequence_length, 1),\n",
    "                   return_sequences=True))\n",
    "    model.add(LSTM(units=hp.Int('lstm_units', min_value=16, max_value=64, step=16)))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4])),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,  # Number of hyperparameter combinations to try\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',  # Directory to store the results\n",
    "    project_name='my_project'\n",
    ")\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(X_train_seq, y_train_seq, epochs=10, batch_size=32, validation_data=(X_valid_seq, y_valid_seq))\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "# Print the summary of the best model\n",
    "best_model.summary()\n",
    "\n",
    "# Train the best model\n",
    "best_model.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32, validation_data=(X_valid_seq, y_valid_seq))\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "y_train_pred = best_model.predict(X_train_seq).argmax(axis=1)\n",
    "y_valid_pred = best_model.predict(X_valid_seq).argmax(axis=1)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training and validation sets\n",
    "mse_train = mean_squared_error(y_train_seq, y_train_pred)\n",
    "mse_valid = mean_squared_error(y_valid_seq, y_valid_pred)\n",
    "print(\"Training Mean Squared Error (MSE):\", mse_train)\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse_valid)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training and validation sets\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "print(\"Training Root Mean Squared Error (RMSE):\", rmse_train)\n",
    "print(\"Validation Root Mean Squared Error (RMSE):\", rmse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training and validation sets\n",
    "r2_train = r2_score(y_train_seq, y_train_pred)\n",
    "r2_valid = r2_score(y_valid_seq, y_valid_pred)\n",
    "print(\"Training R^2:\", r2_train)\n",
    "print(\"Validation R^2:\", r2_valid)\n",
    "\n",
    "# Calculate the number of samples and features\n",
    "n_train, p_train = X_train_seq.shape[0], X_train_seq.shape[1]\n",
    "n_valid, p_valid = X_valid_seq.shape[0], X_valid_seq.shape[1]\n",
    "\n",
    "# Calculate adjusted R-squared for training and validation sets\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "print(\"Training Adjusted R^2:\", adj_r2_train)\n",
    "print(\"Validation Adjusted R^2:\", adj_r2_valid)\n",
    "\n",
    "# Show training and validation set accuracy\n",
    "accuracy_train = (y_train_seq == y_train_pred).mean()\n",
    "accuracy_valid = (y_valid_seq == y_valid_pred).mean()\n",
    "print(\"Training Accuracy:\", accuracy_train)\n",
    "print(\"Validation Accuracy:\", accuracy_valid)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f71d36bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 10, 64)            16896     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,411\n",
      "Trainable params: 29,411\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the RNN model with additional modifications to obtain feature importances\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(sequence_length, 1)))  # Add an input layer with the desired input shape\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Show the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d0cb9397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 [==============================] - 6s 234ms/step - loss: 0.9051 - accuracy: 1.0000 - val_loss: 0.7161 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.5849 - accuracy: 1.0000 - val_loss: 0.3778 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.2602 - accuracy: 1.0000 - val_loss: 0.1182 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.0744 - accuracy: 1.0000 - val_loss: 0.0318 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "6/6 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Training Mean Squared Error (MSE): 0.0\n",
      "Validation Mean Squared Error (MSE): 0.0\n",
      "Training Root Mean Squared Error (RMSE): 0.0\n",
      "Validation Root Mean Squared Error (RMSE): 0.0\n",
      "Training R^2: 1.0\n",
      "Validation R^2: 1.0\n",
      "Training Adjusted R^2: 1.0\n",
      "Validation Adjusted R^2: 1.0\n",
      "Training Accuracy: 1.0\n",
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Date' column to datetime type\n",
    "train['Date'] = pd.to_datetime(train['Date'])\n",
    "valid['Date'] = pd.to_datetime(valid['Date'])\n",
    "\n",
    "# Define a reference date (you can choose any date you prefer)\n",
    "reference_date = pd.to_datetime('1990-01-01')\n",
    "\n",
    "# Extract numerical representation of 'Date' column\n",
    "train['Date'] = (train['Date'] - reference_date).dt.days\n",
    "valid['Date'] = (valid['Date'] - reference_date).dt.days\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = train.drop(columns=['Direction', 'Increase', 'Decrease', 'Difference'])\n",
    "y_train = train['Direction']\n",
    "\n",
    "X_valid = valid.drop(columns=['Direction', 'Increase', 'Decrease', 'Difference'])\n",
    "y_valid = valid['Direction']\n",
    "\n",
    "# Encode the ordinal 'Direction' variable to numerical values (e.g., -1, 0, 1)\n",
    "y_train_encoded = pd.Series(np.where(y_train == 'Decrease', -1, np.where(y_train == 'Hold', 0, 1)))\n",
    "y_valid_encoded = pd.Series(np.where(y_valid == 'Decrease', -1, np.where(y_valid == 'Hold', 0, 1)))\n",
    "\n",
    "# Center and scale the independent variables\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Build the RNN model with additional modifications to obtain feature importances\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Int('lstm_units', min_value=32, max_value=128, step=32),\n",
    "                   input_shape=(sequence_length, 1),\n",
    "                   return_sequences=True))\n",
    "    model.add(LSTM(units=hp.Int('lstm_units', min_value=16, max_value=64, step=16)))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4])),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Prepare the input sequences for the RNN model (assuming sequence_length is defined)\n",
    "def prepare_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length + 1):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(data[i+sequence_length-1])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "sequence_length = 10  # Define the length of the input sequence\n",
    "X_train_seq, y_train_seq = prepare_sequences(y_train_encoded, sequence_length)\n",
    "X_valid_seq, y_valid_seq = prepare_sequences(y_valid_encoded, sequence_length)\n",
    "\n",
    "# Build the RNN model with additional modifications to obtain feature importances\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(sequence_length, 1), return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32, validation_data=(X_valid_seq, y_valid_seq))\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_train_pred = model.predict(X_train_seq).argmax(axis=1)\n",
    "y_valid_pred = model.predict(X_valid_seq).argmax(axis=1)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training and validation sets\n",
    "mse_train = mean_squared_error(y_train_seq, y_train_pred)\n",
    "mse_valid = mean_squared_error(y_valid_seq, y_valid_pred)\n",
    "print(\"Training Mean Squared Error (MSE):\", mse_train)\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse_valid)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training and validation sets\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "print(\"Training Root Mean Squared Error (RMSE):\", rmse_train)\n",
    "print(\"Validation Root Mean Squared Error (RMSE):\", rmse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training and validation sets\n",
    "r2_train = r2_score(y_train_seq, y_train_pred)\n",
    "r2_valid = r2_score(y_valid_seq, y_valid_pred)\n",
    "print(\"Training R^2:\", r2_train)\n",
    "print(\"Validation R^2:\", r2_valid)\n",
    "\n",
    "# Calculate the number of samples and features\n",
    "n_train, p_train = X_train_seq.shape[0], X_train_seq.shape[1]\n",
    "n_valid, p_valid = X_valid_seq.shape[0], X_valid_seq.shape[1]\n",
    "\n",
    "# Calculate adjusted R-squared for training and validation sets\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "print(\"Training Adjusted R^2:\", adj_r2_train)\n",
    "print(\"Validation Adjusted R^2:\", adj_r2_valid)\n",
    "\n",
    "# Show training and validation set accuracy\n",
    "accuracy_train = (y_train_seq == y_train_pred).mean()\n",
    "accuracy_valid = (y_valid_seq == y_valid_pred).mean()\n",
    "print(\"Training Accuracy:\", accuracy_train)\n",
    "print(\"Validation Accuracy:\", accuracy_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
