{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec425cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mord in c:\\users\\abact\\appdata\\roaming\\python\\python310\\site-packages (0.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xgboost in c:\\users\\abact\\appdata\\roaming\\python\\python310\\site-packages (1.7.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\abact\\appdata\\roaming\\python\\python310\\site-packages (from xgboost) (1.7.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\abact\\appdata\\roaming\\python\\python310\\site-packages (from xgboost) (1.22.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: shap in c:\\users\\abact\\appdata\\roaming\\python\\python310\\site-packages (0.42.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\abact\\appdata\\roaming\\python\\python310\\site-packages (from shap) (1.7.3)\n",
      "Requirement already satisfied: slicer==0.0.7 in c:\\users\\abact\\appdata\\roaming\\python\\python310\\site-packages (from shap) (0.0.7)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (22.0)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (4.64.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (2.0.0)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (1.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\abact\\appdata\\roaming\\python\\python310\\site-packages (from shap) (1.22.1)\n",
      "Requirement already satisfied: numba in c:\\programdata\\anaconda3\\lib\\site-packages (from shap) (0.56.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in c:\\programdata\\anaconda3\\lib\\site-packages (from numba->shap) (0.39.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from numba->shap) (65.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->shap) (2022.7)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abact\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas->shap) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mord\n",
    "!pip install xgboost\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3f914cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from tpot import TPOTClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mord import OrdinalRidge\n",
    "import xgboost as xgb\n",
    "from keras.layers import Input\n",
    "from keras_tuner import RandomSearch\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "import shap\n",
    "from tabulate import tabulate\n",
    "import requests\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "534b5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The URL of the raw dataset on GitHub\n",
    "url = \"https://raw.githubusercontent.com/abactat/BC-Project/main/data/processed/train_data.csv?token=GHSAT0AAAAAACC4ZCNLK5WDAXMHGAA2JI24ZGGSK4A\"\n",
    "\n",
    "# Send an HTTP GET request to fetch the content of the raw dataset\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200 means success)\n",
    "if response.status_code == 200:\n",
    "    # Read the content as a pandas DataFrame and assign it to the 'train' variable\n",
    "    train = pd.read_csv(StringIO(response.text))\n",
    "    print(\"Dataset downloaded and loaded into 'train' successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download the dataset. Status code: {response.status_code}\")\n",
    "    \n",
    "# The URL of the raw dataset on GitHub\n",
    "url = \"https://raw.githubusercontent.com/abactat/BC-Project/main/data/processed/val_data.csv?token=GHSAT0AAAAAACC4ZCNK6U7OS5YW72PK3GBSZGGSH2A\"\n",
    "\n",
    "# Send an HTTP GET request to fetch the content of the raw dataset\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200 means success)\n",
    "if response.status_code == 200:\n",
    "    # Read the content as a pandas DataFrame and assign it to the 'valid' variable\n",
    "    valid = pd.read_csv(StringIO(response.text))\n",
    "    print(\"Dataset downloaded and loaded into 'valid' successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download the dataset. Status code: {response.status_code}\")\n",
    "    \n",
    "# The URL of the raw dataset on GitHub\n",
    "url = \"https://raw.githubusercontent.com/abactat/BC-Project/main/data/processed/test_data.csv?token=GHSAT0AAAAAACC4ZCNL45CUOECJPHCIM43GZGGSIGQ\"\n",
    "\n",
    "# Send an HTTP GET request to fetch the content of the raw dataset\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200 means success)\n",
    "if response.status_code == 200:\n",
    "    # Read the content as a pandas DataFrame and assign it to the 'train' variable\n",
    "    test = pd.read_csv(StringIO(response.text))\n",
    "    print(\"Dataset downloaded and loaded into 'test' successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download the dataset. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d950f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert variables to numeric in the train dataset\n",
    "variables_to_convert = train.columns.drop('Date')\n",
    "train[variables_to_convert] = train[variables_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert variables to numeric in the valid dataset\n",
    "valid[variables_to_convert] = valid[variables_to_convert].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea184cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Direction'] = np.select([\n",
    "    train['Difference'] > 0,\n",
    "    train['Difference'] < 0,\n",
    "    train['Difference'] == 0\n",
    "], [\n",
    "    1,  # Increase\n",
    "    -1,  # Decrease\n",
    "    0   # Hold\n",
    "], default=-1)\n",
    "\n",
    "valid['Direction'] = np.select([\n",
    "    valid['Difference'] > 0,\n",
    "    valid['Difference'] < 0,\n",
    "    valid['Difference'] == 0\n",
    "], [\n",
    "    1,  # Increase\n",
    "    -1,  # Decrease\n",
    "    0   # Hold\n",
    "], default=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "047ff245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Increase', 'Decrease', 'Level', 'CPI', 'Unemployment Rate',\n",
      "       'Retail Sales', 'Durable Goods Orders', 'Short-Term Treasury Bond Rate',\n",
      "       'Long-Term Treasury Bond Rate', 'Bank Reserves', 'LEI', 'CEI', 'LAG',\n",
      "       'Treasury Deposits', 'Average Hourly Earnings', 'Net Sentiment Score',\n",
      "       'Positive Frequency', 'Negative Frequency', 'Word Count',\n",
      "       'Standardized Sentiment Score', 'Short-Term Treasury Diff',\n",
      "       'LEI_RollingMean', 'CEI_RollingMean', 'LAG_RollingMean',\n",
      "       'Retail Sales_RollingMean', 'Durable Goods Orders_RollingMean',\n",
      "       'almost', 'anticipated', 'anticipating', 'apparent', 'apparently',\n",
      "       'appeared', 'appearing', 'assumed', 'assumption', 'attain',\n",
      "       'attractive', 'attractiveness', 'believed', 'better', 'bolstered',\n",
      "       'bolstering', 'cautious', 'cautiously', 'could', 'depend', 'easier',\n",
      "       'encouragement', 'favorable', 'gaining', 'good', 'improved',\n",
      "       'improvement', 'leading', 'might', 'nearly', 'opportunity',\n",
      "       'optimistic', 'positive', 'possibility', 'possibly', 'presumed',\n",
      "       'profitability', 'progress', 'progressed', 'prosperity', 'rebound',\n",
      "       'rebounded', 'rebounding', 'regain', 'risk', 'roughly', 'satisfactory',\n",
      "       'smooth', 'sometime', 'stabilization', 'stabilized', 'stable',\n",
      "       'strengthening', 'stronger', 'strongest', 'succeeded', 'sudden',\n",
      "       'suggested', 'suggesting', 'surpassing', 'tremendous', 'uncertain',\n",
      "       'uncertainty', 'unclear', 'unexpected', 'unknown', 'upturn', 'vary',\n",
      "       'volatile', 'volatility', 'Difference', 'Date', 'Direction'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b105da",
   "metadata": {},
   "source": [
    "###Directional Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d994a764",
   "metadata": {},
   "source": [
    "###Ordinal Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c5c6e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease', 'Direction', 'Date'])\n",
    "y_train = train['Direction']  # Use the 'Direction' variable as the target\n",
    "\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease', 'Direction', 'Date'])\n",
    "y_valid = valid['Direction']  # Use the 'Direction' variable as the target\n",
    "\n",
    "# Center and scale the independent variables\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Initialize and train the ordinal logistic regression model\n",
    "ordinal_model = OrdinalRidge()\n",
    "ordinal_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_valid_pred = ordinal_model.predict(X_valid_scaled)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = ordinal_model.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4120d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mean Squared Error (MSE): 0.17801047120418848\n",
      "Validation Mean Squared Error (MSE): 0.5\n",
      "Training Root Mean Squared Error (RMSE): 0.4219128715791786\n",
      "Validation Root Mean Squared Error (RMSE): 0.7071067811865476\n",
      "Training R^2: 0.39903757171941523\n",
      "Validation R^2: -0.22033898305084754\n",
      "Training Adjusted R^2: -0.18940480597199083\n",
      "Validation Adjusted R^2: 1.3953210790164716\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "print(\"Training Mean Squared Error (MSE):\", mse_train)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, y_valid_pred)\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse_valid)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "print(\"Training Root Mean Squared Error (RMSE):\", rmse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "print(\"Validation Root Mean Squared Error (RMSE):\", rmse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "print(\"Training R^2:\", r2_train)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, y_valid_pred)\n",
    "print(\"Validation R^2:\", r2_valid)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]  # Number of samples in training set\n",
    "p_train = X_train.shape[1]  # Number of features in training set\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "print(\"Training Adjusted R^2:\", adj_r2_train)\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]  # Number of samples in validation set\n",
    "p_valid = X_valid.shape[1]  # Number of features in validation set\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "print(\"Validation Adjusted R^2:\", adj_r2_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7d1fc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8219895287958116\n",
      "Validation Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the ordinal logistic regression model\n",
    "ordinal_model = OrdinalRidge()\n",
    "ordinal_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_valid_pred = ordinal_model.predict(X_valid_scaled)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = ordinal_model.predict(X_train_scaled)\n",
    "\n",
    "# Convert the predicted values to ordinal categories\n",
    "y_train_pred_category = np.round(y_train_pred).astype(int)\n",
    "y_valid_pred_category = np.round(y_valid_pred).astype(int)\n",
    "\n",
    "# Calculate accuracy for training and validation sets\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred_category)\n",
    "accuracy_valid = accuracy_score(y_valid, y_valid_pred_category)\n",
    "\n",
    "print(\"Training Accuracy:\", accuracy_train)\n",
    "print(\"Validation Accuracy:\", accuracy_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1137cd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      "[ 0.02402709 -0.08662731 -0.15515958  0.14026437 -0.02640148 -0.03135705\n",
      " -0.15349031 -0.01867492 -0.16180556 -0.02017475 -0.01504331 -0.05897942\n",
      " -0.15792497  0.01118825  0.00359184 -0.00099917 -0.08670814  0.01118825\n",
      "  0.31060554  0.03835019  0.02974458  0.0336391   0.30179174 -0.2238576\n",
      " -0.02915485 -0.01829946  0.12587379  0.05849727 -0.02207385  0.09471137\n",
      " -0.1053766  -0.05668885 -0.05741903  0.02104695  0.0189186  -0.01731716\n",
      " -0.01223244  0.11684601 -0.0643666  -0.04351069 -0.06576789 -0.00441159\n",
      "  0.20326871 -0.1015785   0.08127378 -0.07471455  0.00067962 -0.04500762\n",
      "  0.26533477 -0.13220994  0.10739167 -0.00710719 -0.17400346 -0.09744961\n",
      " -0.06866535  0.06749356  0.00143774 -0.07296422 -0.02882997 -0.03363\n",
      " -0.07990339 -0.06529572  0.02111175 -0.1220375  -0.05129406  0.02012124\n",
      " -0.0021426  -0.08774782 -0.03045944  0.04898673 -0.03176268 -0.01020718\n",
      "  0.00399995 -0.05599647 -0.10103282 -0.13480984 -0.15984486 -0.03711265\n",
      "  0.05295604 -0.06143211 -0.07283566 -0.08469576  0.07568411  0.\n",
      "  0.         -0.01257812  0.04300333 -0.01596146 -0.04296638 -0.02640513\n",
      " -0.01476638 -0.0201807   0.04386084  0.07677427]\n",
      "Intercepts:\n",
      "0.04712041884816773\n"
     ]
    }
   ],
   "source": [
    "# Access the coefficients and intercepts\n",
    "coefficients = ordinal_model.coef_\n",
    "intercepts = ordinal_model.intercept_\n",
    "\n",
    "# Display coefficients and intercepts\n",
    "print(\"Coefficients:\")\n",
    "print(coefficients)\n",
    "\n",
    "print(\"Intercepts:\")\n",
    "print(intercepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e623cb",
   "metadata": {},
   "source": [
    "###XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "241121e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the model\n",
    "X_train = train.drop(columns=['Difference', 'Increase', 'Decrease', 'Direction', 'Date'])\n",
    "y_train = train['Direction']  # Use the 'Direction' variable as the target\n",
    "\n",
    "X_valid = valid.drop(columns=['Difference', 'Increase', 'Decrease', 'Direction', 'Date'])\n",
    "y_valid = valid['Direction']  # Use the 'Direction' variable as the target\n",
    "\n",
    "# Initialize and train the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_valid_pred = xgb_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e306453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mean Squared Error (MSE): 3.2998337238190296e-07\n",
      "Validation Mean Squared Error (MSE): 0.23903423250877073\n",
      "Training Root Mean Squared Error (RMSE): 0.0005744417919875807\n",
      "Validation Root Mean Squared Error (RMSE): 0.48891127263417716\n",
      "Training R^2: 0.9999988859778449\n",
      "Validation R^2: 0.41659441557181376\n",
      "Training Adjusted R^2: 0.9999977951644847\n",
      "Validation Adjusted R^2: 1.1889905414344828\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean Squared Error (MSE) for training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "print(\"Training Mean Squared Error (MSE):\", mse_train)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for validation set\n",
    "mse_valid = mean_squared_error(y_valid, y_valid_pred)\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse_valid)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training set\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "print(\"Training Root Mean Squared Error (RMSE):\", rmse_train)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for validation set\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "print(\"Validation Root Mean Squared Error (RMSE):\", rmse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training set\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "print(\"Training R^2:\", r2_train)\n",
    "\n",
    "# Calculate R-squared (R^2) for validation set\n",
    "r2_valid = r2_score(y_valid, y_valid_pred)\n",
    "print(\"Validation R^2:\", r2_valid)\n",
    "\n",
    "# Calculate adjusted R-squared for training set\n",
    "n_train = X_train.shape[0]  # Number of samples in training set\n",
    "p_train = X_train.shape[1]  # Number of features in training set\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "print(\"Training Adjusted R^2:\", adj_r2_train)\n",
    "\n",
    "# Calculate adjusted R-squared for validation set\n",
    "n_valid = X_valid.shape[0]  # Number of samples in validation set\n",
    "p_valid = X_valid.shape[1]  # Number of features in validation set\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "print(\"Validation Adjusted R^2:\", adj_r2_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4f3b766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "Short-Term Treasury Diff: 0.2838\n",
      "Word Count: 0.1056\n",
      "Short-Term Treasury Bond Rate: 0.1025\n",
      "depend: 0.0745\n",
      "better: 0.0509\n",
      "CEI_RollingMean: 0.0351\n",
      "assumption: 0.0346\n",
      "uncertainty: 0.0330\n",
      "LEI: 0.0317\n",
      "Treasury Deposits: 0.0280\n",
      "encouragement: 0.0276\n",
      "CEI: 0.0253\n",
      "Positive Frequency: 0.0219\n",
      "Level: 0.0216\n",
      "Bank Reserves: 0.0198\n",
      "LAG_RollingMean: 0.0191\n",
      "Unemployment Rate: 0.0174\n",
      "anticipating: 0.0100\n",
      "Average Hourly Earnings: 0.0077\n",
      "Net Sentiment Score: 0.0075\n",
      "Negative Frequency: 0.0066\n",
      "anticipated: 0.0065\n",
      "risk: 0.0057\n",
      "possibility: 0.0043\n",
      "progress: 0.0037\n",
      "appeared: 0.0022\n",
      "suggesting: 0.0018\n",
      "Durable Goods Orders: 0.0013\n",
      "CPI: 0.0013\n",
      "could: 0.0012\n",
      "favorable: 0.0008\n",
      "apparent: 0.0007\n",
      "Retail Sales: 0.0006\n",
      "Durable Goods Orders_RollingMean: 0.0006\n",
      "Long-Term Treasury Bond Rate: 0.0006\n",
      "unclear: 0.0006\n",
      "stabilized: 0.0005\n",
      "believed: 0.0005\n",
      "LEI_RollingMean: 0.0005\n",
      "improvement: 0.0004\n",
      "stronger: 0.0004\n",
      "bolstered: 0.0003\n",
      "good: 0.0002\n",
      "volatile: 0.0002\n",
      "uncertain: 0.0001\n",
      "possibly: 0.0001\n",
      "unexpected: 0.0001\n",
      "positive: 0.0001\n",
      "opportunity: 0.0000\n",
      "satisfactory: 0.0000\n",
      "cautious: 0.0000\n",
      "nearly: 0.0000\n",
      "apparently: 0.0000\n",
      "rebounded: 0.0000\n",
      "stable: 0.0000\n",
      "stabilization: 0.0000\n",
      "roughly: 0.0000\n",
      "optimistic: 0.0000\n",
      "progressed: 0.0000\n",
      "might: 0.0000\n",
      "prosperity: 0.0000\n",
      "rebound: 0.0000\n",
      "rebounding: 0.0000\n",
      "strengthening: 0.0000\n",
      "improved: 0.0000\n",
      "upturn: 0.0000\n",
      "leading: 0.0000\n",
      "almost: 0.0000\n",
      "suggested: 0.0000\n",
      "easier: 0.0000\n",
      "gaining: 0.0000\n",
      "LAG: 0.0000\n",
      "Standardized Sentiment Score: 0.0000\n",
      "Retail Sales_RollingMean: 0.0000\n",
      "appearing: 0.0000\n",
      "assumed: 0.0000\n",
      "attain: 0.0000\n",
      "attractive: 0.0000\n",
      "attractiveness: 0.0000\n",
      "bolstering: 0.0000\n",
      "cautiously: 0.0000\n",
      "presumed: 0.0000\n",
      "profitability: 0.0000\n",
      "regain: 0.0000\n",
      "smooth: 0.0000\n",
      "sometime: 0.0000\n",
      "strongest: 0.0000\n",
      "succeeded: 0.0000\n",
      "sudden: 0.0000\n",
      "surpassing: 0.0000\n",
      "tremendous: 0.0000\n",
      "unknown: 0.0000\n",
      "vary: 0.0000\n",
      "volatility: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "\n",
    "# Create a dictionary to map feature names to their importance scores\n",
    "feature_importance_dict = {feature_name: importance_score for feature_name, importance_score in zip(X_train.columns, feature_importance)}\n",
    "\n",
    "# Sort the feature importance dictionary in descending order based on importance scores\n",
    "sorted_feature_importance = dict(sorted(feature_importance_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Print the feature importance scores in descending order\n",
    "print(\"Feature Importance:\")\n",
    "for feature_name, importance_score in sorted_feature_importance.items():\n",
    "    print(f\"{feature_name}: {importance_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e649b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n",
      "Validation Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_valid_pred = xgb_model.predict(X_valid)\n",
    "\n",
    "# Convert the predicted values to ordinal categories\n",
    "y_train_pred_category = np.round(y_train_pred).astype(int)\n",
    "y_valid_pred_category = np.round(y_valid_pred).astype(int)\n",
    "\n",
    "# Calculate accuracy for training and validation sets\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred_category)\n",
    "accuracy_valid = accuracy_score(y_valid, y_valid_pred_category)\n",
    "\n",
    "print(\"Training Accuracy:\", accuracy_train)\n",
    "print(\"Validation Accuracy:\", accuracy_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84615628",
   "metadata": {},
   "source": [
    "###RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "810b20f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 [==============================] - 6s 245ms/step - loss: 0.9943 - accuracy: 0.6484 - val_loss: 0.7901 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.6399 - accuracy: 1.0000 - val_loss: 0.3995 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2650 - accuracy: 1.0000 - val_loss: 0.1087 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.0663 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "6/6 [==============================] - 1s 7ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Training Mean Squared Error (MSE): 0.0\n",
      "Validation Mean Squared Error (MSE): 0.0\n",
      "Training Root Mean Squared Error (RMSE): 0.0\n",
      "Validation Root Mean Squared Error (RMSE): 0.0\n",
      "Training R^2: 1.0\n",
      "Validation R^2: 1.0\n",
      "Training Adjusted R^2: 1.0\n",
      "Validation Adjusted R^2: 1.0\n",
      "Training Accuracy: 1.0\n",
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "X_train = train.drop(columns=['Direction', 'Increase', 'Decrease', 'Difference', 'Date'])\n",
    "y_train = train['Direction']\n",
    "\n",
    "X_valid = valid.drop(columns=['Direction', 'Increase', 'Decrease', 'Difference', 'Date'])\n",
    "y_valid = valid['Direction']\n",
    "\n",
    "# Encode the ordinal 'Direction' variable to numerical values (e.g., -1, 0, 1)\n",
    "y_train_encoded = pd.Series(np.where(y_train == 'Decrease', -1, np.where(y_train == 'Hold', 0, 1)))\n",
    "y_valid_encoded = pd.Series(np.where(y_valid == 'Decrease', -1, np.where(y_valid == 'Hold', 0, 1)))\n",
    "\n",
    "# Center and scale the independent variables\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Build the RNN model with additional modifications to obtain feature importances\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Int('lstm_units', min_value=32, max_value=128, step=32),\n",
    "                   input_shape=(sequence_length, 1),\n",
    "                   return_sequences=True))\n",
    "    model.add(LSTM(units=hp.Int('lstm_units', min_value=16, max_value=64, step=16)))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4])),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Prepare the input sequences for the RNN model (assuming sequence_length is defined)\n",
    "def prepare_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length + 1):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(data[i+sequence_length-1])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "sequence_length = 10  # Define the length of the input sequence\n",
    "X_train_seq, y_train_seq = prepare_sequences(y_train_encoded, sequence_length)\n",
    "X_valid_seq, y_valid_seq = prepare_sequences(y_valid_encoded, sequence_length)\n",
    "\n",
    "# Build the RNN model with additional modifications to obtain feature importances\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(sequence_length, 1), return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32, validation_data=(X_valid_seq, y_valid_seq))\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_train_pred = model.predict(X_train_seq).argmax(axis=1)\n",
    "y_valid_pred = model.predict(X_valid_seq).argmax(axis=1)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training and validation sets\n",
    "mse_train = mean_squared_error(y_train_seq, y_train_pred)\n",
    "mse_valid = mean_squared_error(y_valid_seq, y_valid_pred)\n",
    "print(\"Training Mean Squared Error (MSE):\", mse_train)\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse_valid)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training and validation sets\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "print(\"Training Root Mean Squared Error (RMSE):\", rmse_train)\n",
    "print(\"Validation Root Mean Squared Error (RMSE):\", rmse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training and validation sets\n",
    "r2_train = r2_score(y_train_seq, y_train_pred)\n",
    "r2_valid = r2_score(y_valid_seq, y_valid_pred)\n",
    "print(\"Training R^2:\", r2_train)\n",
    "print(\"Validation R^2:\", r2_valid)\n",
    "\n",
    "# Calculate the number of samples and features\n",
    "n_train, p_train = X_train_seq.shape[0], X_train_seq.shape[1]\n",
    "n_valid, p_valid = X_valid_seq.shape[0], X_valid_seq.shape[1]\n",
    "\n",
    "# Calculate adjusted R-squared for training and validation sets\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "print(\"Training Adjusted R^2:\", adj_r2_train)\n",
    "print(\"Validation Adjusted R^2:\", adj_r2_valid)\n",
    "\n",
    "# Show training and validation set accuracy\n",
    "accuracy_train = (y_train_seq == y_train_pred).mean()\n",
    "accuracy_valid = (y_valid_seq == y_valid_pred).mean()\n",
    "print(\"Training Accuracy:\", accuracy_train)\n",
    "print(\"Validation Accuracy:\", accuracy_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b58f33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from my_dir\\my_project\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 10, 128)           66560     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,531\n",
      "Trainable params: 198,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 5s 204ms/step - loss: 0.9635 - accuracy: 1.0000 - val_loss: 0.9212 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.8926 - accuracy: 1.0000 - val_loss: 0.8475 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.8162 - accuracy: 1.0000 - val_loss: 0.7660 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.7305 - accuracy: 1.0000 - val_loss: 0.6731 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.6321 - accuracy: 1.0000 - val_loss: 0.5658 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.5189 - accuracy: 1.0000 - val_loss: 0.4441 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3932 - accuracy: 1.0000 - val_loss: 0.3144 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.2650 - accuracy: 1.0000 - val_loss: 0.1928 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.1535 - accuracy: 1.0000 - val_loss: 0.1006 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.0765 - accuracy: 1.0000 - val_loss: 0.0468 - val_accuracy: 1.0000\n",
      "6/6 [==============================] - 1s 8ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Training Mean Squared Error (MSE): 0.0\n",
      "Validation Mean Squared Error (MSE): 0.0\n",
      "Training Root Mean Squared Error (RMSE): 0.0\n",
      "Validation Root Mean Squared Error (RMSE): 0.0\n",
      "Training R^2: 1.0\n",
      "Validation R^2: 1.0\n",
      "Training Adjusted R^2: 1.0\n",
      "Validation Adjusted R^2: 1.0\n",
      "Training Accuracy: 1.0\n",
      "Validation Accuracy: 1.0\n",
      "Best Hyperparameters:\n",
      "{'lstm_units': 128, 'learning_rate': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "# Build the RNN model with additional modifications to obtain feature importances\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Int('lstm_units', min_value=32, max_value=128, step=32),\n",
    "                   input_shape=(sequence_length, 1),\n",
    "                   return_sequences=True))\n",
    "    model.add(LSTM(units=hp.Int('lstm_units', min_value=16, max_value=64, step=16)))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4])),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,  # Number of hyperparameter combinations to try\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',  # Directory to store the results\n",
    "    project_name='my_project'\n",
    ")\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(X_train_seq, y_train_seq, epochs=10, batch_size=32, validation_data=(X_valid_seq, y_valid_seq))\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "# Print the summary of the best model\n",
    "best_model.summary()\n",
    "\n",
    "# Train the best model\n",
    "best_model.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32, validation_data=(X_valid_seq, y_valid_seq))\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "y_train_pred = best_model.predict(X_train_seq).argmax(axis=1)\n",
    "y_valid_pred = best_model.predict(X_valid_seq).argmax(axis=1)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training and validation sets\n",
    "mse_train = mean_squared_error(y_train_seq, y_train_pred)\n",
    "mse_valid = mean_squared_error(y_valid_seq, y_valid_pred)\n",
    "print(\"Training Mean Squared Error (MSE):\", mse_train)\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse_valid)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training and validation sets\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "print(\"Training Root Mean Squared Error (RMSE):\", rmse_train)\n",
    "print(\"Validation Root Mean Squared Error (RMSE):\", rmse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training and validation sets\n",
    "r2_train = r2_score(y_train_seq, y_train_pred)\n",
    "r2_valid = r2_score(y_valid_seq, y_valid_pred)\n",
    "print(\"Training R^2:\", r2_train)\n",
    "print(\"Validation R^2:\", r2_valid)\n",
    "\n",
    "# Calculate the number of samples and features\n",
    "n_train, p_train = X_train_seq.shape[0], X_train_seq.shape[1]\n",
    "n_valid, p_valid = X_valid_seq.shape[0], X_valid_seq.shape[1]\n",
    "\n",
    "# Calculate adjusted R-squared for training and validation sets\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "print(\"Training Adjusted R^2:\", adj_r2_train)\n",
    "print(\"Validation Adjusted R^2:\", adj_r2_valid)\n",
    "\n",
    "# Show training and validation set accuracy\n",
    "accuracy_train = (y_train_seq == y_train_pred).mean()\n",
    "accuracy_valid = (y_valid_seq == y_valid_pred).mean()\n",
    "print(\"Training Accuracy:\", accuracy_train)\n",
    "print(\"Validation Accuracy:\", accuracy_valid)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hyperparameters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78232ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 10, 64)            16896     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,411\n",
      "Trainable params: 29,411\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the RNN model with additional modifications to obtain feature importances\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(sequence_length, 1)))  # Add an input layer with the desired input shape\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Show the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "175dd3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 [==============================] - 5s 215ms/step - loss: 0.9420 - accuracy: 0.8242 - val_loss: 0.7131 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.5649 - accuracy: 1.0000 - val_loss: 0.3379 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.2149 - accuracy: 1.0000 - val_loss: 0.0727 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.0380 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 9.4774e-04 - accuracy: 1.0000 - val_loss: 7.2798e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 6.4860e-04 - accuracy: 1.0000 - val_loss: 5.5334e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 5.1389e-04 - accuracy: 1.0000 - val_loss: 4.6409e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 4.4133e-04 - accuracy: 1.0000 - val_loss: 4.1143e-04 - val_accuracy: 1.0000\n",
      "6/6 [==============================] - 1s 6ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Training Mean Squared Error (MSE): 0.0\n",
      "Validation Mean Squared Error (MSE): 0.0\n",
      "Training Root Mean Squared Error (RMSE): 0.0\n",
      "Validation Root Mean Squared Error (RMSE): 0.0\n",
      "Training R^2: 1.0\n",
      "Validation R^2: 1.0\n",
      "Training Adjusted R^2: 1.0\n",
      "Validation Adjusted R^2: 1.0\n",
      "Training Accuracy: 1.0\n",
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Date' column to datetime type\n",
    "train['Date'] = pd.to_datetime(train['Date'])\n",
    "valid['Date'] = pd.to_datetime(valid['Date'])\n",
    "\n",
    "# Define a reference date (you can choose any date you prefer)\n",
    "reference_date = pd.to_datetime('1990-01-01')\n",
    "\n",
    "# Extract numerical representation of 'Date' column\n",
    "train['Date'] = (train['Date'] - reference_date).dt.days\n",
    "valid['Date'] = (valid['Date'] - reference_date).dt.days\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = train.drop(columns=['Direction', 'Increase', 'Decrease', 'Difference'])\n",
    "y_train = train['Direction']\n",
    "\n",
    "X_valid = valid.drop(columns=['Direction', 'Increase', 'Decrease', 'Difference'])\n",
    "y_valid = valid['Direction']\n",
    "\n",
    "# Encode the ordinal 'Direction' variable to numerical values (e.g., -1, 0, 1)\n",
    "y_train_encoded = pd.Series(np.where(y_train == 'Decrease', -1, np.where(y_train == 'Hold', 0, 1)))\n",
    "y_valid_encoded = pd.Series(np.where(y_valid == 'Decrease', -1, np.where(y_valid == 'Hold', 0, 1)))\n",
    "\n",
    "# Center and scale the independent variables\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Build the RNN model with additional modifications to obtain feature importances\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Int('lstm_units', min_value=32, max_value=128, step=32),\n",
    "                   input_shape=(sequence_length, 1),\n",
    "                   return_sequences=True))\n",
    "    model.add(LSTM(units=hp.Int('lstm_units', min_value=16, max_value=64, step=16)))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4])),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Prepare the input sequences for the RNN model (assuming sequence_length is defined)\n",
    "def prepare_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length + 1):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        targets.append(data[i+sequence_length-1])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "sequence_length = 10  # Define the length of the input sequence\n",
    "X_train_seq, y_train_seq = prepare_sequences(y_train_encoded, sequence_length)\n",
    "X_valid_seq, y_valid_seq = prepare_sequences(y_valid_encoded, sequence_length)\n",
    "\n",
    "# Build the RNN model with additional modifications to obtain feature importances\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(sequence_length, 1), return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32, validation_data=(X_valid_seq, y_valid_seq))\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_train_pred = model.predict(X_train_seq).argmax(axis=1)\n",
    "y_valid_pred = model.predict(X_valid_seq).argmax(axis=1)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) for training and validation sets\n",
    "mse_train = mean_squared_error(y_train_seq, y_train_pred)\n",
    "mse_valid = mean_squared_error(y_valid_seq, y_valid_pred)\n",
    "print(\"Training Mean Squared Error (MSE):\", mse_train)\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse_valid)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for training and validation sets\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "rmse_valid = np.sqrt(mse_valid)\n",
    "print(\"Training Root Mean Squared Error (RMSE):\", rmse_train)\n",
    "print(\"Validation Root Mean Squared Error (RMSE):\", rmse_valid)\n",
    "\n",
    "# Calculate R-squared (R^2) for training and validation sets\n",
    "r2_train = r2_score(y_train_seq, y_train_pred)\n",
    "r2_valid = r2_score(y_valid_seq, y_valid_pred)\n",
    "print(\"Training R^2:\", r2_train)\n",
    "print(\"Validation R^2:\", r2_valid)\n",
    "\n",
    "# Calculate the number of samples and features\n",
    "n_train, p_train = X_train_seq.shape[0], X_train_seq.shape[1]\n",
    "n_valid, p_valid = X_valid_seq.shape[0], X_valid_seq.shape[1]\n",
    "\n",
    "# Calculate adjusted R-squared for training and validation sets\n",
    "adj_r2_train = 1 - ((1 - r2_train) * (n_train - 1) / (n_train - p_train - 1))\n",
    "adj_r2_valid = 1 - ((1 - r2_valid) * (n_valid - 1) / (n_valid - p_valid - 1))\n",
    "print(\"Training Adjusted R^2:\", adj_r2_train)\n",
    "print(\"Validation Adjusted R^2:\", adj_r2_valid)\n",
    "\n",
    "# Show training and validation set accuracy\n",
    "accuracy_train = (y_train_seq == y_train_pred).mean()\n",
    "accuracy_valid = (y_valid_seq == y_valid_pred).mean()\n",
    "print(\"Training Accuracy:\", accuracy_train)\n",
    "print(\"Validation Accuracy:\", accuracy_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
